"""æ•°æ®åº“æ“ä½œå™¨

æä¾›æ•°æ®åº“çš„åŸºæœ¬æ“ä½œåŠŸèƒ½ï¼ŒåŒ…æ‹¬æ•°æ®æ’å…¥ã€æ›´æ–°ã€æŸ¥è¯¢ç­‰ã€‚
"""

import logging
import pandas as pd
from functools import lru_cache
from typing import List, Dict, Any, Optional, Union

from .table_creator import SchemaTableCreator
from .connection import get_conn
from .interfaces import IDBOperator
from .types import TableName

logger = logging.getLogger(__name__)


class DBOperator(SchemaTableCreator, IDBOperator):
    """æ•°æ®åº“æ“ä½œå™¨

    ç»§æ‰¿è‡ªSchemaTableCreatorï¼Œæä¾›æ•°æ®åº“çš„åŸºæœ¬æ“ä½œåŠŸèƒ½ã€‚
    """

    def __init__(self, schema_file_path: str = None, conn=get_conn):
        """åˆå§‹åŒ–æ•°æ®åº“æ“ä½œå™¨

        Args:
            schema_file_path: Schemaæ–‡ä»¶è·¯å¾„
            conn: æ•°æ®åº“è¿æ¥å‡½æ•°
        """
        super().__init__(schema_file_path, conn)

    @classmethod
    def create_default(cls) -> "DBOperator":
        """åˆ›å»ºé»˜è®¤çš„æ•°æ®åº“æ“ä½œå™¨å®ä¾‹

        ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»º DBOperator å®ä¾‹ï¼ŒåŒ…æ‹¬ï¼š
        - é»˜è®¤çš„ schema æ–‡ä»¶è·¯å¾„ï¼ˆä»é…ç½®ä¸­è·å–ï¼‰
        - é»˜è®¤çš„æ•°æ®åº“è¿æ¥å‡½æ•°

        Returns:
            DBOperator: é»˜è®¤é…ç½®çš„æ•°æ®åº“æ“ä½œå™¨å®ä¾‹
        """
        return cls()

    def upsert(
        self,
        table_name: str,
        data: Union[pd.DataFrame, Dict[str, Any], List[Dict[str, Any]]],
    ) -> bool:
        """å‘è¡¨ä¸­æ’å…¥æˆ–æ›´æ–°æ•°æ®

        Args:
            table_name: è¡¨å
            data: è¦æ’å…¥çš„æ•°æ®ï¼Œå¯ä»¥æ˜¯DataFrameã€å­—å…¸æˆ–å­—å…¸åˆ—è¡¨

        Returns:
            æ“ä½œæ˜¯å¦æˆåŠŸ
        """
        # æ•°æ®éªŒè¯
        if data is None:
            logger.debug(f"æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ upsert æ“ä½œ: {table_name}")
            return True

        if isinstance(data, pd.DataFrame) and data.empty:
            logger.debug(f"DataFrame ä¸ºç©ºï¼Œè·³è¿‡ upsert æ“ä½œ: {table_name}")
            return True

        if isinstance(data, (list, dict)) and not data:
            logger.debug(f"æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ upsert æ“ä½œ: {table_name}")
            return True

        # ç¡®ä¿è¡¨å­˜åœ¨
        if not self.table_exists(table_name):
            logger.info(f"è¡¨ '{table_name}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
            if not self.create_table(table_name):
                logger.error(f"åˆ›å»ºè¡¨ '{table_name}' å¤±è´¥")
                return False

        # è·å–è¡¨é…ç½®
        if not self._table_exists_in_schema(table_name):
            logger.error(f"è¡¨ '{table_name}' åœ¨ schema ä¸­ä¸å­˜åœ¨")
            return False

        table_config = self._get_table_config(table_name)
        primary_key = getattr(
            table_config, "primary_key", table_config.get("primary_key", [])
        )

        if not primary_key:
            raise ValueError(f"è¡¨ '{table_name}' æœªå®šä¹‰ä¸»é”®ï¼Œæ— æ³•æ‰§è¡Œ upsert æ“ä½œ")

        # è½¬æ¢æ•°æ®æ ¼å¼
        if isinstance(data, dict):
            df = pd.DataFrame([data])
        elif isinstance(data, list):
            df = pd.DataFrame(data)
        else:
            df = data.copy()

        if df.empty:
            logger.debug(f"å¤„ç†åçš„æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ upsert æ“ä½œ: {table_name}")
            return True

        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        missing_pk_cols = [col for col in primary_key if col not in df.columns]
        if missing_pk_cols:
            raise ValueError(
                f"æ•°æ®ä¸­ç¼ºå°‘ä¸»é”®åˆ— {missing_pk_cols}ï¼Œæ— æ³•æ‰§è¡Œ upsert æ“ä½œ"
            )

        # æ£€æŸ¥DataFrameæ˜¯å¦åŒ…å«è¡¨çš„æ‰€æœ‰å¿…éœ€åˆ—
        columns = getattr(table_config, "columns", table_config.get("columns", []))
        table_columns = self._extract_column_names(columns)
        missing_cols = [col for col in table_columns if col not in df.columns]
        if missing_cols:
            raise ValueError(f"DataFrame ç¼ºå°‘ä»¥ä¸‹åˆ—: {missing_cols}")

        try:
            # æ‰§è¡Œ upsert æ“ä½œ
            if callable(self.conn):
                with self.conn() as conn:
                    self._perform_upsert(conn, table_name, df, primary_key)
            else:
                self._perform_upsert(self.conn, table_name, df, primary_key)

            logger.debug(f"ğŸ“¥ æˆåŠŸå‘è¡¨ '{table_name}' upsert {len(df)} æ¡è®°å½•")
            return True

        except Exception as e:
            logger.error(f"âŒ upsert æ“ä½œå¤±è´¥ - è¡¨: {table_name}, é”™è¯¯: {e}")
            raise

    def _perform_upsert(
        self, conn, table_name: str, df: pd.DataFrame, primary_key: List[str]
    ) -> None:
        """æ‰§è¡Œå®é™…çš„ upsert æ“ä½œ

        Args:
            conn: æ•°æ®åº“è¿æ¥
            table_name: è¡¨å
            df: æ•°æ®DataFrame
            primary_key: ä¸»é”®åˆ—è¡¨
        """
        # æ„å»º upsert SQL
        columns = df.columns.tolist()
        placeholders = ", ".join(["?" for _ in columns])
        column_names = ", ".join(columns)

        # æ„å»º ON CONFLICT å­å¥
        " AND ".join([f"excluded.{col} = {table_name}.{col}" for col in primary_key])
        update_columns = [col for col in columns if col not in primary_key]

        if update_columns:
            update_clause = ", ".join(
                [f"{col} = excluded.{col}" for col in update_columns]
            )
            sql = f"""
                INSERT INTO {table_name} ({column_names})
                VALUES ({placeholders})
                ON CONFLICT ({", ".join(primary_key)})
                DO UPDATE SET {update_clause}
            """
        else:
            # å¦‚æœæ²¡æœ‰éä¸»é”®åˆ—ï¼Œåˆ™å¿½ç•¥å†²çª
            sql = f"""
                INSERT INTO {table_name} ({column_names})
                VALUES ({placeholders})
                ON CONFLICT ({", ".join(primary_key)})
                DO NOTHING
            """

        # æ‰¹é‡æ’å…¥æ•°æ®
        self._upsert_batch_records(conn, sql, df)

    def _upsert_batch_records(self, conn, sql: str, df: pd.DataFrame) -> None:
        """æ‰§è¡Œæ‰¹é‡upsertè®°å½•

        Args:
            conn: æ•°æ®åº“è¿æ¥
            sql: SQLè¯­å¥
            df: æ•°æ®DataFrame
        """
        data_tuples = [tuple(row) for row in df.values]
        conn.executemany(sql, data_tuples)
        conn.commit()

    def get_max_date(self, table_key: str, ts_codes: Optional[List[str]] = None) -> Dict[str, str]:
        """æ ¹æ® schema ä¸­å®šä¹‰çš„ date_colï¼ŒæŸ¥è¯¢æŒ‡å®šè¡¨ä¸­ä¸€ä¸ªæˆ–å¤šä¸ªè‚¡ç¥¨çš„æœ€æ–°æ—¥æœŸ

        Args:
            table_key: è¡¨åœ¨schemaé…ç½®ä¸­çš„é”®å (e.g., 'stock_daily')
            ts_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨ã€‚å¦‚æœä¸º None æˆ–ç©ºï¼Œåˆ™æŸ¥è¯¢å…¨è¡¨çš„æœ€æ–°æ—¥æœŸã€‚

        Returns:
            ä¸€ä¸ªå­—å…¸ï¼Œkeyä¸ºè‚¡ç¥¨ä»£ç ï¼Œvalueä¸ºå¯¹åº”çš„æœ€æ–°æ—¥æœŸ (YYYYMMDDæ ¼å¼å­—ç¬¦ä¸²)ã€‚
            å¦‚æœæŸ¥è¯¢å…¨è¡¨ï¼Œåˆ™keyä¸ºç‰¹æ®Šå€¼ '__all__'ã€‚
        """
        if not self._table_exists_in_schema(table_key):
            raise ValueError(f"è¡¨é…ç½® '{table_key}' ä¸å­˜åœ¨äº schema ä¸­")

        table_config = self._get_table_config(table_key)
        table_name = table_config.get("table_name")

        if "date_col" not in table_config or not table_config["date_col"]:
            logger.debug(f"è¡¨ '{table_name}' æœªå®šä¹‰ date_col å­—æ®µï¼Œæ— æ³•æŸ¥è¯¢æœ€å¤§æ—¥æœŸ")
            return {}

        date_col = table_config["date_col"]
        
        params = []
        if ts_codes:
            # æŸ¥è¯¢æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨çš„æœ€æ–°æ—¥æœŸ
            placeholders = ", ".join(["?" for _ in ts_codes])
            sql = f"SELECT ts_code, MAX({date_col}) as max_date FROM {table_name} WHERE ts_code IN ({placeholders}) GROUP BY ts_code"
            params.extend(ts_codes)
        else:
            # æŸ¥è¯¢å…¨è¡¨çš„æœ€æ–°æ—¥æœŸ
            sql = f"SELECT MAX({date_col}) as max_date FROM {table_name}"

        try:
            if callable(self.conn):
                with self.conn() as conn:
                    results = conn.execute(sql, params).fetchall()
            else:
                results = self.conn.execute(sql, params).fetchall()

            max_dates = {}
            if ts_codes:
                for row in results:
                    max_dates[row[0]] = str(row[1])
            elif results and results[0][0] is not None:
                max_dates['__all__'] = str(results[0][0])
            
            return max_dates

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢è¡¨ '{table_name}' æœ€å¤§æ—¥æœŸå¤±è´¥: {e}")
            raise

    @lru_cache(maxsize=1)
    def get_all_symbols(self) -> List[str]:
        """è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 

        Returns:
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        table_name = TableName.STOCK_BASIC.value

        # æ„å»ºæŸ¥è¯¢SQLï¼Œæ·»åŠ è¿‡æ»¤æ¡ä»¶
        sql = f"SELECT DISTINCT ts_code FROM {table_name} WHERE ts_code IS NOT NULL AND ts_code != ''"

        try:
            if callable(self.conn):
                with self.conn() as conn:
                    result = conn.execute(sql).fetchall()
            else:
                result = self.conn.execute(sql).fetchall()

            # æå– ts_code åˆ—è¡¨ï¼Œè¿‡æ»¤ç©ºå€¼
            ts_codes = [row[0] for row in result if row[0] is not None and row[0] != ""]
            logger.debug(f"ä»è¡¨ '{table_name}' æŸ¥è¯¢åˆ° {len(ts_codes)} ä¸ªè‚¡ç¥¨ä»£ç ")
            return ts_codes

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢è¡¨ '{table_name}' çš„ ts_code å¤±è´¥: {e}")
            raise
