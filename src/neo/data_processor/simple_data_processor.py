"""ç®€å•æ•°æ®å¤„ç†å™¨å®ç°

ä¸“æ³¨æ•°æ®æ¸…æ´—ã€è½¬æ¢å’ŒéªŒè¯çš„æ•°æ®å¤„ç†å±‚ã€‚
"""

import logging
from typing import Optional, Dict
import pandas as pd

from ..configs import get_config
from .interfaces import IDataProcessor, IDataBuffer
from ..database.operator import DBOperator
from ..database.interfaces import ISchemaLoader
from ..database.schema_loader import SchemaLoader

logger = logging.getLogger(__name__)



class SimpleDataProcessor(IDataProcessor):
    """ç®€åŒ–çš„æ•°æ®å¤„ç†å™¨å®ç°

    ä¸“æ³¨äºæ•°æ®æ¸…æ´—ã€è½¬æ¢å’ŒéªŒè¯ã€‚
    """

    @classmethod
    def create_default(
        cls,
        enable_batch: bool = True,
        db_operator: Optional[DBOperator] = None,
        schema_loader: Optional[ISchemaLoader] = None,
        data_buffer: Optional[IDataBuffer] = None,
    ) -> "SimpleDataProcessor":
        """åˆ›å»ºé»˜è®¤é…ç½®çš„æ•°æ®å¤„ç†å™¨

        Args:
            enable_batch: æ˜¯å¦å¯ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼
            db_operator: æ•°æ®åº“æ“ä½œå™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            schema_loader: Schema åŠ è½½å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            data_buffer: æ•°æ®ç¼“å†²å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®

        Returns:
            é…ç½®å¥½çš„æ•°æ®å¤„ç†å™¨å®ä¾‹
        """
        return cls(
            db_operator=db_operator or DBOperator.create_default(),
            enable_batch=enable_batch,
            schema_loader=schema_loader or SchemaLoader(),
            data_buffer=data_buffer,
        )

    def __init__(
        self,
        db_operator: Optional[DBOperator] = None,
        enable_batch: bool = True,
        schema_loader: Optional[ISchemaLoader] = None,
        data_buffer: Optional[IDataBuffer] = None,
    ):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            db_operator: æ•°æ®åº“æ“ä½œå™¨ï¼Œç”¨äºä¿å­˜æ•°æ®
            enable_batch: æ˜¯å¦å¯ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼
            schema_loader: Schema åŠ è½½å™¨ï¼Œç”¨äºè·å–è¡¨åæ˜ å°„
            data_buffer: æ•°æ®ç¼“å†²å™¨ï¼Œç”¨äºæ‰¹é‡å¤„ç†
        """
        self.config = get_config()
        self.db_operator = db_operator or DBOperator()
        self.enable_batch = enable_batch
        self.schema_loader = schema_loader or SchemaLoader()

        # æ‰¹é‡å¤„ç†é…ç½®
        self.batch_size = self.config.data_processor.batch_size
        self.flush_interval_seconds = self.config.data_processor.flush_interval_seconds

        # æ•°æ®ç¼“å†²å™¨ï¼šè´Ÿè´£ç¼“å†²åŒºç®¡ç†å’Œå¼‚æ­¥åˆ·æ–°
        from .data_buffer import get_sync_data_buffer
        self.data_buffer = data_buffer or get_sync_data_buffer(self.flush_interval_seconds)
        
        # æ³¨å†Œå·²çŸ¥çš„æ•°æ®ç±»å‹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        self._register_known_types()

    def _register_known_types(self) -> None:
        """æ³¨å†Œå·²çŸ¥çš„æ•°æ®ç±»å‹åˆ°ç¼“å†²å™¨
        
        è¿™é‡Œå¯ä»¥é¢„å…ˆæ³¨å†Œä¸€äº›å·²çŸ¥çš„æ•°æ®ç±»å‹ï¼Œé¿å…è¿è¡Œæ—¶æ³¨å†Œã€‚
        ç›®å‰ä¸ºç©ºå®ç°ï¼Œæ•°æ®ç±»å‹ä¼šåœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åŠ¨æ€æ³¨å†Œã€‚
        """
        pass

    def _get_table_name(self, task_type) -> Optional[str]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–å¯¹åº”çš„è¡¨å

        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æšä¸¾ï¼‰

        Returns:
            å¯¹åº”çš„è¡¨åï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å› None
        """
        try:
            # å¦‚æœæ˜¯æšä¸¾ç±»å‹ï¼Œä½¿ç”¨å…¶ name å±æ€§
            type_name = task_type.name if hasattr(task_type, "name") else str(task_type)
            schema = self.schema_loader.load_schema(type_name)
            return schema.table_name
        except KeyError:
            type_name = task_type.name if hasattr(task_type, "name") else str(task_type)
            logger.warning(f"æœªæ‰¾åˆ°ä»»åŠ¡ç±»å‹ '{type_name}' å¯¹åº”çš„è¡¨é…ç½®")
            return None

    def process(self, task_type: str, data: pd.DataFrame) -> bool:
        """å¤„ç†ä»»åŠ¡ç»“æœ

        Args:
            task_type: ä»»åŠ¡ç±»å‹å­—ç¬¦ä¸²
            data: è¦å¤„ç†çš„æ•°æ®

        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        logger.debug(f"å¤„ç†ä»»åŠ¡: {task_type}")

        try:
            # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
            if data is None or data.empty:
                logger.warning("æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                return False

            logger.debug(f"æ•°æ®ç»´åº¦: {len(data)} è¡Œ x {len(data.columns)} åˆ—")

            # æ ¹æ®æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
            if self.enable_batch:
                # æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šä½¿ç”¨æ•°æ®ç¼“å†²å™¨
                # ç¡®ä¿æ•°æ®ç±»å‹å·²æ³¨å†Œ
                self.data_buffer.register_type(task_type, self._save_data_callback, self.batch_size)
                
                # æ·»åŠ æ•°æ®åˆ°ç¼“å†²å™¨
                self.data_buffer.add(task_type, data)
                logger.debug(f"æ•°æ®å·²æ·»åŠ åˆ°ç¼“å†²åŒº: {task_type}, rows: {len(data)}")
                success = True
            else:
                # å•æ¡å¤„ç†æ¨¡å¼ï¼šç›´æ¥ä¿å­˜
                success = self._save_data(data, task_type)

            if success:
                if not self.enable_batch:
                    logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(data)} è¡Œæ•°æ®")
            else:
                logger.warning(f"æ•°æ®å¤„ç†å¤±è´¥: {task_type}")

            return success

        except Exception as e:
            print(f"ğŸ’¥ å¤„ç†å¼‚å¸¸: {task_type} - {str(e)}")
            logger.error(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
            return False

    def _save_data_callback(self, data_type: str, data: pd.DataFrame) -> bool:
        """æ•°æ®ä¿å­˜å›è°ƒå‡½æ•°
        
        ä¾›æ•°æ®ç¼“å†²å™¨è°ƒç”¨çš„å›è°ƒå‡½æ•°ï¼Œç”¨äºä¿å­˜åˆå¹¶åçš„æ•°æ®ã€‚
        
        Args:
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            data: è¦ä¿å­˜çš„åˆå¹¶æ•°æ®
            
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        return self._save_data(data, data_type)

    def _save_data(self, data: pd.DataFrame, task_type: str) -> bool:
        """æ•°æ®ä¿å­˜

        å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ã€‚

        Args:
            data: è½¬æ¢åçš„æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å° task_type çš„ç±»å‹å’Œå€¼
            logger.debug(f"task_type ç±»å‹: {type(task_type)}, å€¼: {task_type}")

            # æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è·å–è¡¨å
            table_name = self._get_table_name(task_type)
            if not table_name:
                logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")
                return False

            # ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
            self.db_operator.upsert(table_name, data)
            logger.info(f"æ•°æ®ä¿å­˜æˆåŠŸ: {table_name}, {len(data)} rows")

            return True

        except Exception as e:
            logger.error(f"æ•°æ®ä¿å­˜å¤±è´¥: {e}")
            return False

    def flush_all(self, force: bool = True) -> bool:
        """åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒºæ•°æ®åˆ°æ•°æ®åº“

        Args:
            force: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ•°æ®ï¼ˆå¿½ç•¥æ‰¹é‡å¤§å°é™åˆ¶ï¼‰

        Returns:
            bool: æ‰€æœ‰åˆ·æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            self.data_buffer.flush()
            return True
        except Exception as e:
            logger.error(f"åˆ·æ–°ç¼“å†²åŒºå¤±è´¥: {e}")
            return False

    def get_buffer_status(self) -> Dict[str, int]:
        """è·å–ç¼“å†²åŒºçŠ¶æ€
        
        Returns:
            å„æ•°æ®ç±»å‹çš„ç¼“å†²åŒºå¤§å°
        """
        if hasattr(self.data_buffer, 'get_buffer_sizes'):
            return self.data_buffer.get_buffer_sizes()
        return {}
        
    def shutdown(self) -> None:
        """å…³é—­æ•°æ®å¤„ç†å™¨ï¼Œæ¸…ç†èµ„æº
        
        ç¡®ä¿æ‰€æœ‰ç¼“å†²çš„æ•°æ®éƒ½è¢«åˆ·æ–°åˆ°æ•°æ®åº“ï¼Œå¹¶åœæ­¢åå°çº¿ç¨‹ã€‚
        """
        if hasattr(self, 'data_buffer'):
            self.data_buffer.shutdown()


class AsyncSimpleDataProcessor:
    """å¼‚æ­¥æ•°æ®å¤„ç†å™¨å®ç°

    åŸºäºAsyncCallbackQueueBufferå®ç°çš„å¼‚æ­¥æ•°æ®å¤„ç†å™¨ï¼Œ
    ä¸“æ³¨äºæ•°æ®æ¸…æ´—ã€è½¬æ¢å’ŒéªŒè¯ã€‚
    """

    @classmethod
    def create_default(
        cls,
        enable_batch: bool = True,
        db_operator: Optional[DBOperator] = None,
        schema_loader: Optional[ISchemaLoader] = None,
        data_buffer: Optional[IDataBuffer] = None,
    ) -> "AsyncSimpleDataProcessor":
        """åˆ›å»ºé»˜è®¤é…ç½®çš„å¼‚æ­¥æ•°æ®å¤„ç†å™¨

        Args:
            enable_batch: æ˜¯å¦å¯ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼
            db_operator: æ•°æ®åº“æ“ä½œå™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            schema_loader: Schema åŠ è½½å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            data_buffer: æ•°æ®ç¼“å†²å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å¼‚æ­¥é»˜è®¤é…ç½®

        Returns:
            é…ç½®å¥½çš„å¼‚æ­¥æ•°æ®å¤„ç†å™¨å®ä¾‹
        """
        return cls(
            db_operator=db_operator or DBOperator.create_default(),
            enable_batch=enable_batch,
            schema_loader=schema_loader or SchemaLoader(),
            data_buffer=data_buffer,
        )

    def __init__(
        self,
        db_operator: Optional[DBOperator] = None,
        enable_batch: bool = True,
        schema_loader: Optional[ISchemaLoader] = None,
        data_buffer: Optional[IDataBuffer] = None,
    ):
        """åˆå§‹åŒ–å¼‚æ­¥æ•°æ®å¤„ç†å™¨

        Args:
            db_operator: æ•°æ®åº“æ“ä½œå™¨ï¼Œç”¨äºä¿å­˜æ•°æ®
            enable_batch: æ˜¯å¦å¯ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼
            schema_loader: Schema åŠ è½½å™¨ï¼Œç”¨äºè·å–è¡¨åæ˜ å°„
            data_buffer: æ•°æ®ç¼“å†²å™¨ï¼Œç”¨äºæ‰¹é‡å¤„ç†
        """
        self.config = get_config()
        self.db_operator = db_operator or DBOperator()
        self.enable_batch = enable_batch
        self.schema_loader = schema_loader or SchemaLoader()

        # æ‰¹é‡å¤„ç†é…ç½®
        self.batch_size = self.config.data_processor.batch_size
        self.flush_interval_seconds = self.config.data_processor.flush_interval_seconds

        # å¼‚æ­¥æ•°æ®ç¼“å†²å™¨ï¼šè´Ÿè´£ç¼“å†²åŒºç®¡ç†å’Œå¼‚æ­¥åˆ·æ–°
        if data_buffer is None:
            # å¦‚æœæ²¡æœ‰æä¾› data_bufferï¼Œæˆ‘ä»¬éœ€è¦å¼‚æ­¥åˆå§‹åŒ–å®ƒ
            self.data_buffer = None
            self._data_buffer_initialized = False
        else:
            self.data_buffer = data_buffer
            self._data_buffer_initialized = True
        
        # æ³¨å†Œå·²çŸ¥çš„æ•°æ®ç±»å‹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        self._register_known_types()

    def _register_known_types(self) -> None:
        """æ³¨å†Œå·²çŸ¥çš„æ•°æ®ç±»å‹åˆ°ç¼“å†²å™¨
        
        è¿™é‡Œå¯ä»¥é¢„å…ˆæ³¨å†Œä¸€äº›å·²çŸ¥çš„æ•°æ®ç±»å‹ï¼Œé¿å…è¿è¡Œæ—¶æ³¨å†Œã€‚
        ç›®å‰ä¸ºç©ºå®ç°ï¼Œæ•°æ®ç±»å‹ä¼šåœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åŠ¨æ€æ³¨å†Œã€‚
        """
        pass

    async def _ensure_data_buffer_initialized(self):
        """ç¡®ä¿æ•°æ®ç¼“å†²å™¨å·²åˆå§‹åŒ–"""
        if not self._data_buffer_initialized:
            from .data_buffer import get_async_data_buffer
            self.data_buffer = await get_async_data_buffer(self.flush_interval_seconds)
            if self.data_buffer is None:
                raise RuntimeError("Failed to initialize async data buffer")
            self._data_buffer_initialized = True

    def _get_table_name(self, task_type) -> Optional[str]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–å¯¹åº”çš„è¡¨å

        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æšä¸¾ï¼‰

        Returns:
            å¯¹åº”çš„è¡¨åï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å› None
        """
        try:
            # å¦‚æœæ˜¯æšä¸¾ç±»å‹ï¼Œä½¿ç”¨å…¶ name å±æ€§
            type_name = task_type.name if hasattr(task_type, "name") else str(task_type)
            schema = self.schema_loader.load_schema(type_name)
            return schema.table_name
        except KeyError:
            type_name = task_type.name if hasattr(task_type, "name") else str(task_type)
            logger.warning(f"æœªæ‰¾åˆ°ä»»åŠ¡ç±»å‹ '{type_name}' å¯¹åº”çš„è¡¨é…ç½®")
            return None

    async def process(self, task_type: str, data: pd.DataFrame) -> bool:
        """å¼‚æ­¥å¤„ç†ä»»åŠ¡ç»“æœ

        Args:
            task_type: ä»»åŠ¡ç±»å‹å­—ç¬¦ä¸²
            data: è¦å¤„ç†çš„æ•°æ®

        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        logger.debug(f"å¼‚æ­¥å¤„ç†ä»»åŠ¡: {task_type}")

        try:
            # ç¡®ä¿æ•°æ®ç¼“å†²å™¨å·²åˆå§‹åŒ–
            await self._ensure_data_buffer_initialized()
            
            # å†æ¬¡æ£€æŸ¥æ•°æ®ç¼“å†²å™¨æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
            if self.data_buffer is None:
                raise RuntimeError("Data buffer is still None after initialization")
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
            if data is None or data.empty:
                logger.warning("æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                return False

            logger.debug(f"æ•°æ®ç»´åº¦: {len(data)} è¡Œ x {len(data.columns)} åˆ—")

            # æ ¹æ®æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
            if self.enable_batch:
                # æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šä½¿ç”¨å¼‚æ­¥æ•°æ®ç¼“å†²å™¨
                # ç¡®ä¿æ•°æ®ç±»å‹å·²æ³¨å†Œ
                self.data_buffer.register_type(task_type, self._save_data_callback, self.batch_size)
                
                # æ·»åŠ æ•°æ®åˆ°ç¼“å†²å™¨
                await self.data_buffer.add(task_type, data)
                logger.debug(f"æ•°æ®å·²æ·»åŠ åˆ°å¼‚æ­¥ç¼“å†²åŒº: {task_type}, rows: {len(data)}")
                success = True
            else:
                # å•æ¡å¤„ç†æ¨¡å¼ï¼šç›´æ¥ä¿å­˜
                success = self._save_data(data, task_type)

            if success:
                if not self.enable_batch:
                    logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(data)} è¡Œæ•°æ®")
            else:
                logger.warning(f"å¼‚æ­¥æ•°æ®å¤„ç†å¤±è´¥: {task_type}")

            return success

        except Exception as e:
            print(f"ğŸ’¥ å¼‚æ­¥å¤„ç†å¼‚å¸¸: {task_type} - {str(e)}")
            logger.error(f"å¼‚æ­¥å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
            return False

    async def _save_data_callback(self, data_type: str, data: pd.DataFrame) -> bool:
        """å¼‚æ­¥æ•°æ®ä¿å­˜å›è°ƒå‡½æ•°
        
        ä¾›å¼‚æ­¥æ•°æ®ç¼“å†²å™¨è°ƒç”¨çš„å›è°ƒå‡½æ•°ï¼Œç”¨äºä¿å­˜åˆå¹¶åçš„æ•°æ®ã€‚
        
        Args:
            data_type: æ•°æ®ç±»å‹æ ‡è¯†
            data: è¦ä¿å­˜çš„åˆå¹¶æ•°æ®
            
        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        return self._save_data(data, data_type)

    def _save_data(self, data: pd.DataFrame, task_type: str) -> bool:
        """æ•°æ®ä¿å­˜

        å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ã€‚

        Args:
            data: è½¬æ¢åçš„æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å° task_type çš„ç±»å‹å’Œå€¼
            logger.debug(f"task_type ç±»å‹: {type(task_type)}, å€¼: {task_type}")

            # æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è·å–è¡¨å
            table_name = self._get_table_name(task_type)
            if not table_name:
                logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")
                return False

            # ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
            self.db_operator.upsert(table_name, data)
            logger.info(f"å¼‚æ­¥æ•°æ®ä¿å­˜æˆåŠŸ: {table_name}, {len(data)} rows")

            return True

        except Exception as e:
            logger.error(f"å¼‚æ­¥æ•°æ®ä¿å­˜å¤±è´¥: {e}")
            return False

    async def flush_all(self, force: bool = True) -> bool:
        """å¼‚æ­¥åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒºæ•°æ®åˆ°æ•°æ®åº“

        Args:
            force: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ•°æ®ï¼ˆå¿½ç•¥æ‰¹é‡å¤§å°é™åˆ¶ï¼‰

        Returns:
            bool: æ‰€æœ‰åˆ·æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            await self.data_buffer.flush()
            return True
        except Exception as e:
            logger.error(f"å¼‚æ­¥åˆ·æ–°ç¼“å†²åŒºå¤±è´¥: {e}")
            return False

    def get_buffer_status(self) -> Dict[str, int]:
        """è·å–ç¼“å†²åŒºçŠ¶æ€
        
        Returns:
            å„æ•°æ®ç±»å‹çš„ç¼“å†²åŒºå¤§å°
        """
        if hasattr(self.data_buffer, 'get_buffer_sizes'):
            return self.data_buffer.get_buffer_sizes()
        return {}
        
    async def shutdown(self) -> None:
        """å¼‚æ­¥å…³é—­æ•°æ®å¤„ç†å™¨ï¼Œæ¸…ç†èµ„æº
        
        ç¡®ä¿æ‰€æœ‰ç¼“å†²çš„æ•°æ®éƒ½è¢«åˆ·æ–°åˆ°æ•°æ®åº“ï¼Œå¹¶åœæ­¢åå°çº¿ç¨‹ã€‚
        """
        if hasattr(self, 'data_buffer') and self.data_buffer is not None and self._data_buffer_initialized:
            await self.data_buffer.shutdown()
