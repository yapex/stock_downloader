"""ç®€å•æ•°æ®å¤„ç†å™¨å®ç°

ä¸“æ³¨æ•°æ®æ¸…æ´—ã€è½¬æ¢å’ŒéªŒè¯çš„æ•°æ®å¤„ç†å±‚ã€‚
"""

import logging
from typing import Optional, Dict, Any, List
import pandas as pd
import time
from datetime import timedelta
import threading
from collections import defaultdict

from ..config import get_config
from .interfaces import IDataProcessor
from .types import TaskResult
from ..database.operator import DBOperator

logger = logging.getLogger(__name__)


class SimpleDataProcessor(IDataProcessor):
    """ç®€åŒ–çš„æ•°æ®å¤„ç†å™¨å®ç°

    ä¸“æ³¨äºæ•°æ®æ¸…æ´—ã€è½¬æ¢å’ŒéªŒè¯ã€‚
    """

    def __init__(self, db_operator: Optional[DBOperator] = None, enable_batch: bool = True):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            db_operator: æ•°æ®åº“æ“ä½œå™¨ï¼Œç”¨äºä¿å­˜æ•°æ®
            enable_batch: æ˜¯å¦å¯ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼
        """
        self.config = get_config()
        self.db_operator = db_operator or DBOperator()
        self.enable_batch = enable_batch

        # æ‰¹é‡å¤„ç†é…ç½®
        self.batch_size = self.config.data_processor.batch_size
        self.flush_interval_seconds = self.config.data_processor.flush_interval_seconds
        
        # æ‰¹é‡å¤„ç†ç¼“å†²åŒºï¼šæŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„å­˜å‚¨å¾…å¤„ç†æ•°æ®
        self.batch_buffers: Dict[str, List[pd.DataFrame]] = defaultdict(list)
        self.buffer_lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨é”
        self.last_flush_time = time.time()

        # ç»Ÿè®¡ä¿¡æ¯è·Ÿè¸ª
        self.stats = {
            "total_processed": 0,
            "successful_processed": 0,
            "failed_processed": 0,
            "total_rows_processed": 0,
            "start_time": time.time(),
            "last_stats_output": time.time(),
            "task_type_stats": {},  # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
            "batch_flushes": 0,  # æ‰¹é‡åˆ·æ–°æ¬¡æ•°
            "buffered_items": 0,  # å½“å‰ç¼“å†²åŒºé¡¹ç›®æ•°
        }

        # ç»Ÿè®¡è¾“å‡ºé—´éš”ï¼ˆç§’ï¼‰
        self.stats_output_interval = 30

    def process(self, task_result: TaskResult) -> bool:
        """å¤„ç†ä»»åŠ¡ç»“æœ

        Args:
            task_result: ä»»åŠ¡æ‰§è¡Œç»“æœ

        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        task_name = (
            f"{task_result.config.symbol}_{task_result.config.task_type.name}"
            if task_result.config.symbol
            else task_result.config.task_type.name
        )

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats["total_processed"] += 1
        task_type_name = task_result.config.task_type.name
        if task_type_name not in self.stats["task_type_stats"]:
            self.stats["task_type_stats"][task_type_name] = {
                "count": 0,
                "success": 0,
                "rows": 0,
            }
        self.stats["task_type_stats"][task_type_name]["count"] += 1

        print(f"ğŸ“Š å¼€å§‹å¤„ç†: {task_name}")
        logger.info(
            f"å¼€å§‹å¤„ç†TaskResult: {task_result.config.task_type.value}, symbol: {task_result.config.symbol}"
        )

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._maybe_output_stats()

        try:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æˆåŠŸ
            if not task_result.success:
                print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œè·³è¿‡å¤„ç†: {task_name} - {task_result.error}")
                logger.warning(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œè·³è¿‡å¤„ç†: {task_result.error}")
                # æ›´æ–°å¤±è´¥ç»Ÿè®¡
                self.stats["failed_processed"] += 1
                return False

            # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
            if task_result.data is None or task_result.data.empty:
                print(f"âš ï¸  æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†: {task_name}")
                logger.warning("æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                return False

            print(
                f"ğŸ“ˆ æ•°æ®è¡Œæ•°: {len(task_result.data)} è¡Œï¼Œåˆ—æ•°: {len(task_result.data.columns)} åˆ—"
            )

            # æ•°æ®æ¸…æ´—å’ŒéªŒè¯
            print(f"ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—: {task_name}")
            cleaned_data = self._clean_data(
                task_result.data, task_result.config.task_type.value
            )
            if cleaned_data is None:
                print(f"âŒ æ•°æ®æ¸…æ´—å¤±è´¥: {task_name}")
                logger.warning("æ•°æ®æ¸…æ´—å¤±è´¥")
                return False

            print(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ: {task_name}ï¼Œæ¸…æ´—å {len(cleaned_data)} è¡Œ")

            # æ•°æ®è½¬æ¢
            print(f"ğŸ”„ å¼€å§‹æ•°æ®è½¬æ¢: {task_name}")
            transformed_data = self._transform_data(
                cleaned_data, task_result.config.task_type.value
            )
            if transformed_data is None:
                print(f"âŒ æ•°æ®è½¬æ¢å¤±è´¥: {task_name}")
                logger.warning("æ•°æ®è½¬æ¢å¤±è´¥")
                return False

            print(f"âœ… æ•°æ®è½¬æ¢å®Œæˆ: {task_name}")

            # æ ¹æ®æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
            if self.enable_batch:
                # æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šæ·»åŠ åˆ°ç¼“å†²åŒº
                success = self._add_to_buffer(transformed_data, task_result.config.task_type.value.api_method)
                if success:
                    print(f"ğŸ“¦ æ•°æ®å·²æ·»åŠ åˆ°ç¼“å†²åŒº: {task_name}ï¼Œ{len(transformed_data)} è¡Œæ•°æ®")
                    logger.info(
                        f"æ•°æ®å·²æ·»åŠ åˆ°ç¼“å†²åŒº: {task_result.config.task_type.value}, symbol: {task_result.config.symbol}, rows: {len(transformed_data)}"
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å†²åŒº
                    task_type_key = task_result.config.task_type.value.api_method
                    if self._should_flush(task_type_key):
                        flush_success = self._flush_buffer(task_type_key)
                        if not flush_success:
                            success = False
                    else:
                        # æ£€æŸ¥å®šæ—¶åˆ·æ–°
                        self._check_and_flush_all_buffers()
            else:
                # å•æ¡å¤„ç†æ¨¡å¼ï¼šç›´æ¥ä¿å­˜
                print(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ•°æ®: {task_name}")
                success = self._save_data(
                    transformed_data, task_result.config.task_type.value.api_method
                )

            if success:
                if not self.enable_batch:
                    print(
                        f"ğŸ‰ æ•°æ®å¤„ç†å®Œæˆ: {task_name}ï¼ŒæˆåŠŸä¿å­˜ {len(transformed_data)} è¡Œæ•°æ®"
                    )
                    # åœ¨æ‰¹é‡æ¨¡å¼ä¸‹ï¼Œè¡Œæ•°ç»Ÿè®¡åœ¨åˆ·æ–°æ—¶æ›´æ–°
                    self.stats["total_rows_processed"] += len(transformed_data)
                    
                # æ›´æ–°æˆåŠŸç»Ÿè®¡
                self.stats["successful_processed"] += 1
                self.stats["task_type_stats"][task_type_name]["success"] += 1
                if not self.enable_batch:
                    self.stats["task_type_stats"][task_type_name]["rows"] += len(transformed_data)
            else:
                print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {task_name}")
                logger.warning(
                    f"æ•°æ®å¤„ç†å¤±è´¥: {task_result.config.task_type}, symbol: {task_result.config.symbol}"
                )
                self.stats["failed_processed"] += 1

            return success

        except Exception as e:
            print(f"ğŸ’¥ å¤„ç†å¼‚å¸¸: {task_name} - {str(e)}")
            logger.error(f"å¤„ç†TaskResultæ—¶å‡ºé”™: {e}")
            self.stats["failed_processed"] += 1
            return False

    def _clean_data(self, data: pd.DataFrame, task_type: str) -> Optional[pd.DataFrame]:
        """æ•°æ®æ¸…æ´—

        Args:
            data: åŸå§‹æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            æ¸…æ´—åçš„æ•°æ®ï¼Œå¦‚æœæ¸…æ´—å¤±è´¥è¿”å›None
        """
        try:
            cleaned_data = data.copy()

            # æ ¹æ®ä»»åŠ¡ç±»å‹è¿›è¡Œç‰¹å®šæ¸…æ´—
            if task_type == "stock_basic":
                # è‚¡ç¥¨åŸºç¡€ä¿¡æ¯æ¸…æ´—
                required_columns = ["ts_code", "symbol", "name"]
                if not all(col in cleaned_data.columns for col in required_columns):
                    logger.warning(f"è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ç¼ºå°‘å¿…è¦å­—æ®µ: {required_columns}")
                    return None
                # åªç§»é™¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œ
                cleaned_data = cleaned_data.dropna(subset=required_columns)
            elif task_type in ["daily", "weekly", "monthly"]:
                # è¡Œæƒ…æ•°æ®æ¸…æ´—
                required_columns = [
                    "ts_code",
                    "trade_date",
                    "open",
                    "high",
                    "low",
                    "close",
                ]
                if not all(col in cleaned_data.columns for col in required_columns):
                    logger.warning(f"è¡Œæƒ…æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {required_columns}")
                    return None

                # åªç§»é™¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œ
                cleaned_data = cleaned_data.dropna(subset=required_columns)

                # ç¡®ä¿ä»·æ ¼æ•°æ®ä¸ºæ­£æ•°
                price_columns = ["open", "high", "low", "close"]
                for col in price_columns:
                    if col in cleaned_data.columns:
                        cleaned_data = cleaned_data[cleaned_data[col] > 0]
            elif task_type in ["income", "balancesheet", "cashflow"]:
                # è´¢åŠ¡æ•°æ®æ¸…æ´— - åªæ£€æŸ¥å…³é”®å­—æ®µ
                required_columns = ["ts_code", "end_date"]
                if not all(col in cleaned_data.columns for col in required_columns):
                    logger.warning(f"è´¢åŠ¡æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {required_columns}")
                    return None
                # åªç§»é™¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œ
                cleaned_data = cleaned_data.dropna(subset=required_columns)
            else:
                # å…¶ä»–ç±»å‹æ•°æ®ï¼Œä¿æŒåŸæ ·ï¼Œä¸è¿›è¡Œä¸¥æ ¼çš„ç©ºå€¼æ¸…æ´—
                pass

            logger.debug(f"æ•°æ®æ¸…æ´—å®Œæˆ: {len(data)} -> {len(cleaned_data)} rows")
            return cleaned_data

        except Exception as e:
            logger.error(f"æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            return None

    def _transform_data(
        self, data: pd.DataFrame, task_type: str
    ) -> Optional[pd.DataFrame]:
        """æ•°æ®è½¬æ¢

        Args:
            data: æ¸…æ´—åçš„æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            è½¬æ¢åçš„æ•°æ®ï¼Œå¦‚æœè½¬æ¢å¤±è´¥è¿”å›None
        """
        try:
            transformed_data = data.copy()

            # æ ¹æ®ä»»åŠ¡ç±»å‹è¿›è¡Œç‰¹å®šè½¬æ¢
            if task_type in ["daily", "weekly", "monthly"]:
                # è¡Œæƒ…æ•°æ®è½¬æ¢
                if "trade_date" in transformed_data.columns:
                    # ç¡®ä¿äº¤æ˜“æ—¥æœŸæ ¼å¼æ­£ç¡®
                    transformed_data["trade_date"] = pd.to_datetime(
                        transformed_data["trade_date"], format="%Y%m%d"
                    )

                # è®¡ç®—æ¶¨è·Œå¹…ï¼ˆå¦‚æœæœ‰å‰æ”¶ç›˜ä»·ï¼‰
                if "pre_close" in transformed_data.columns:
                    transformed_data["pct_chg"] = (
                        (transformed_data["close"] - transformed_data["pre_close"])
                        / transformed_data["pre_close"]
                        * 100
                    ).round(2)

            logger.debug(f"æ•°æ®è½¬æ¢å®Œæˆ: {len(transformed_data)} rows")
            return transformed_data

        except Exception as e:
            logger.error(f"æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            return None

    def _save_data(self, data: pd.DataFrame, task_type: str) -> bool:
        """æ•°æ®ä¿å­˜

        å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ã€‚

        Args:
            data: è½¬æ¢åçš„æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å° task_type çš„ç±»å‹å’Œå€¼
            logger.debug(f"task_type ç±»å‹: {type(task_type)}, å€¼: {task_type}")

            # æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šè¡¨å
            table_name_mapping = {
                "stock_basic": "stock_basic",
                "daily": "stock_daily",
                "daily_basic": "daily_basic",
                "weekly": "stock_weekly",
                "monthly": "stock_monthly",
                "income": "income_statement",
                "cashflow": "cash_flow",
                "balancesheet": "balance_sheet",
            }

            table_name = table_name_mapping.get(task_type)
            if not table_name:
                logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")
                return False

            # ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
            logger.debug(f"å¼€å§‹ä¿å­˜æ•°æ®: {table_name}, {len(data)} rows")
            self.db_operator.upsert(table_name, data)
            logger.info(f"æ•°æ®ä¿å­˜æˆåŠŸ: {table_name}, {len(data)} rows")

            return True

        except Exception as e:
            logger.error(f"æ•°æ®ä¿å­˜å¤±è´¥: {e}")
            return False

    def _add_to_buffer(self, data: pd.DataFrame, task_type: str) -> bool:
        """å°†æ•°æ®æ·»åŠ åˆ°æ‰¹é‡å¤„ç†ç¼“å†²åŒº
        
        Args:
            data: è¦æ·»åŠ çš„æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹
            
        Returns:
            bool: æ·»åŠ æ˜¯å¦æˆåŠŸ
        """
        try:
            with self.buffer_lock:
                self.batch_buffers[task_type].append(data.copy())
                self.stats["buffered_items"] += len(data)
                
            logger.debug(f"æ•°æ®å·²æ·»åŠ åˆ°ç¼“å†²åŒº: {task_type}, {len(data)} è¡Œ, ç¼“å†²åŒºå¤§å°: {len(self.batch_buffers[task_type])}")
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ•°æ®åˆ°ç¼“å†²åŒºå¤±è´¥: {task_type} - {e}")
            return False

    def _flush_buffer(self, task_type: str, force: bool = False) -> bool:
        """åˆ·æ–°æŒ‡å®šä»»åŠ¡ç±»å‹çš„ç¼“å†²åŒºæ•°æ®åˆ°æ•°æ®åº“
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            force: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥æ‰¹é‡å¤§å°é™åˆ¶ï¼‰
            
        Returns:
            bool: åˆ·æ–°æ˜¯å¦æˆåŠŸ
        """
        with self.buffer_lock:
            if task_type not in self.batch_buffers or not self.batch_buffers[task_type]:
                return True  # æ²¡æœ‰æ•°æ®éœ€è¦åˆ·æ–°
                
            buffer_data = self.batch_buffers[task_type]
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆæŒ‰æ•°æ®è¡Œæ•°è®¡ç®—ï¼‰
            if not force:
                total_rows = sum(len(df) for df in buffer_data)
                if total_rows < self.batch_size:
                    return True  # ä¸éœ€è¦åˆ·æ–°
                
            try:
                # åˆå¹¶æ‰€æœ‰ç¼“å†²åŒºæ•°æ®
                if len(buffer_data) == 1:
                    combined_data = buffer_data[0]
                else:
                    combined_data = pd.concat(buffer_data, ignore_index=True)
                    
                # è·å–è¡¨åæ˜ å°„
                table_name_mapping = {
                    "stock_basic": "stock_basic",
                    "daily": "stock_daily",
                    "daily_basic": "daily_basic",
                    "weekly": "stock_weekly",
                    "monthly": "stock_monthly",
                    "income": "income_statement",
                    "cashflow": "cash_flow",
                    "balancesheet": "balance_sheet",
                }
                
                table_name = table_name_mapping.get(task_type)
                if not table_name:
                    logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")
                    return False
                    
                # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
                logger.info(f"æ‰¹é‡åˆ·æ–°å¼€å§‹: {table_name}, {len(combined_data)} è¡Œæ•°æ® (æ¥è‡ª {len(buffer_data)} ä¸ªä»»åŠ¡)")
                self.db_operator.upsert(table_name, combined_data)
                logger.info(f"æ‰¹é‡åˆ·æ–°æˆåŠŸ: {table_name}, {len(combined_data)} è¡Œæ•°æ®")
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.stats["batch_flushes"] += 1
                self.stats["total_rows_processed"] += len(combined_data)
                
                # è®¡ç®—è¦å‡å°‘çš„ç¼“å†²é¡¹ç›®æ•°ï¼ˆæŒ‰è¡Œæ•°è®¡ç®—ï¼‰
                buffered_rows = sum(len(df) for df in buffer_data)
                
                # æ¸…ç©ºç¼“å†²åŒº
                self.batch_buffers[task_type].clear()
                self.stats["buffered_items"] -= buffered_rows
                
                print(f"ğŸš€ æ‰¹é‡åˆ·æ–°å®Œæˆ: {task_type} -> {table_name}, {len(combined_data)} è¡Œæ•°æ®")
                return True
                
            except Exception as e:
                logger.error(f"æ‰¹é‡åˆ·æ–°å¤±è´¥: {task_type} - {e}")
                print(f"âŒ æ‰¹é‡åˆ·æ–°å¤±è´¥: {task_type} - {str(e)}")
                return False
                
    def _should_flush(self, task_type: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ·æ–°ç¼“å†²åŒº
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥åˆ·æ–°
        """
        with self.buffer_lock:
            # æ£€æŸ¥æ‰¹é‡å¤§å°ï¼ˆæŒ‰æ•°æ®è¡Œæ•°è®¡ç®—ï¼‰
            if task_type in self.batch_buffers and self.batch_buffers[task_type]:
                total_rows = sum(len(df) for df in self.batch_buffers[task_type])
                if total_rows >= self.batch_size:
                    return True
                
            # æ£€æŸ¥æ—¶é—´é—´éš”
            current_time = time.time()
            if current_time - self.last_flush_time >= self.flush_interval_seconds:
                return True
                
            return False
            
    def _check_and_flush_all_buffers(self) -> None:
        """æ£€æŸ¥å¹¶åˆ·æ–°æ‰€æœ‰éœ€è¦åˆ·æ–°çš„ç¼“å†²åŒº"""
        current_time = time.time()
        if current_time - self.last_flush_time >= self.flush_interval_seconds:
            self.flush_all()
            self.last_flush_time = current_time

    def _maybe_output_stats(self) -> None:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
        current_time = time.time()
        if current_time - self.stats["last_stats_output"] >= self.stats_output_interval:
            self._output_stats()
            self.stats["last_stats_output"] = current_time

    def _output_stats(self) -> None:
        """è¾“å‡ºå½“å‰ç»Ÿè®¡ä¿¡æ¯"""
        current_time = time.time()
        elapsed_time = current_time - self.stats["start_time"]

        # è®¡ç®—å¤„ç†é€Ÿç‡
        processing_rate = (
            self.stats["total_processed"] / elapsed_time if elapsed_time > 0 else 0
        )
        success_rate = (
            (self.stats["successful_processed"] / self.stats["total_processed"] * 100)
            if self.stats["total_processed"] > 0
            else 0
        )

        print("\n" + "=" * 60)
        print("ğŸ“ˆ æ•°æ®å¤„ç†ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 60)
        print(f"â±ï¸  è¿è¡Œæ—¶é—´: {timedelta(seconds=int(elapsed_time))}")
        print(f"ğŸ“Š æ€»å¤„ç†ä»»åŠ¡: {self.stats['total_processed']}")
        print(f"âœ… æˆåŠŸå¤„ç†: {self.stats['successful_processed']}")
        print(f"âŒ å¤±è´¥å¤„ç†: {self.stats['failed_processed']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"ğŸš€ å¤„ç†é€Ÿç‡: {processing_rate:.2f} ä»»åŠ¡/ç§’")
        print(f"ğŸ“‹ æ€»å¤„ç†è¡Œæ•°: {self.stats['total_rows_processed']}")
        
        # æ‰¹é‡å¤„ç†ç»Ÿè®¡
        if self.enable_batch:
            print(f"ğŸ”„ æ‰¹é‡åˆ·æ–°æ¬¡æ•°: {self.stats['batch_flushes']}")
            print(f"ğŸ“¦ å½“å‰ç¼“å†²é¡¹ç›®: {self.stats['buffered_items']}")
            
            # æ˜¾ç¤ºå„ç¼“å†²åŒºçŠ¶æ€
            if self.batch_buffers:
                print("\nğŸ“¦ ç¼“å†²åŒºçŠ¶æ€:")
                with self.buffer_lock:
                    for task_type, buffer_data in self.batch_buffers.items():
                        if buffer_data:
                            total_rows = sum(len(df) for df in buffer_data)
                            print(f"  {task_type}: {len(buffer_data)} ä¸ªä»»åŠ¡, {total_rows} è¡Œæ•°æ®")

        # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        if self.stats["task_type_stats"]:
            print("\nğŸ“‹ æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡:")
            for task_type, stats in self.stats["task_type_stats"].items():
                task_success_rate = (
                    (stats["success"] / stats["count"] * 100)
                    if stats["count"] > 0
                    else 0
                )
                print(
                    f"  {task_type}: {stats['count']} ä»»åŠ¡, {stats['success']} æˆåŠŸ ({task_success_rate:.1f}%), {stats['rows']} è¡Œ"
                )

        print("=" * 60 + "\n")

        # åŒæ—¶è®°å½•åˆ°æ—¥å¿—
        logger.info(
            f"ç»Ÿè®¡ä¿¡æ¯ - æ€»ä»»åŠ¡: {self.stats['total_processed']}, æˆåŠŸ: {self.stats['successful_processed']}, å¤±è´¥: {self.stats['failed_processed']}, æˆåŠŸç‡: {success_rate:.1f}%, å¤„ç†é€Ÿç‡: {processing_rate:.2f} ä»»åŠ¡/ç§’, æ€»è¡Œæ•°: {self.stats['total_rows_processed']}, æ‰¹é‡åˆ·æ–°: {self.stats['batch_flushes']}, ç¼“å†²é¡¹ç›®: {self.stats['buffered_items']}"
        )
        
    def flush_all(self, force: bool = True) -> bool:
        """åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒºæ•°æ®åˆ°æ•°æ®åº“
        
        Args:
            force: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ•°æ®ï¼ˆå¿½ç•¥æ‰¹é‡å¤§å°é™åˆ¶ï¼‰
            
        Returns:
            bool: æ‰€æœ‰åˆ·æ–°æ˜¯å¦æˆåŠŸ
        """
        success = True
        flushed_types = []
        
        with self.buffer_lock:
            # è·å–æ‰€æœ‰æœ‰æ•°æ®çš„ä»»åŠ¡ç±»å‹
            task_types_to_flush = [task_type for task_type, buffer_data in self.batch_buffers.items() if buffer_data]
            
        if not task_types_to_flush:
            logger.debug("æ²¡æœ‰ç¼“å†²åŒºæ•°æ®éœ€è¦åˆ·æ–°")
            return True
            
        print(f"ğŸ”„ å¼€å§‹åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº: {len(task_types_to_flush)} ä¸ªä»»åŠ¡ç±»å‹")
        
        for task_type in task_types_to_flush:
            if self._flush_buffer(task_type, force=force):
                flushed_types.append(task_type)
            else:
                success = False
                
        if flushed_types:
            print(f"âœ… æ‰¹é‡åˆ·æ–°å®Œæˆ: {', '.join(flushed_types)}")
            logger.info(f"æ‰¹é‡åˆ·æ–°å®Œæˆ: {', '.join(flushed_types)}")
        
        return success

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯

        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        current_time = time.time()
        elapsed_time = current_time - self.stats["start_time"]
        processing_rate = (
            self.stats["total_processed"] / elapsed_time if elapsed_time > 0 else 0
        )
        success_rate = (
            (self.stats["successful_processed"] / self.stats["total_processed"] * 100)
            if self.stats["total_processed"] > 0
            else 0
        )

        # è·å–ç¼“å†²åŒºçŠ¶æ€
        buffer_status = {}
        if self.enable_batch:
            with self.buffer_lock:
                for task_type, buffer_data in self.batch_buffers.items():
                    if buffer_data:
                        total_rows = sum(len(df) for df in buffer_data)
                        buffer_status[task_type] = {
                            "tasks": len(buffer_data),
                            "rows": total_rows
                        }
        
        return {
            "elapsed_time": elapsed_time,
            "total_processed": self.stats["total_processed"],
            "successful_processed": self.stats["successful_processed"],
            "failed_processed": self.stats["failed_processed"],
            "success_rate": success_rate,
            "processing_rate": processing_rate,
            "total_rows_processed": self.stats["total_rows_processed"],
            "task_type_stats": self.stats["task_type_stats"].copy(),
            "batch_enabled": self.enable_batch,
            "batch_flushes": self.stats["batch_flushes"],
            "buffered_items": self.stats["buffered_items"],
            "buffer_status": buffer_status,
            "batch_size": self.batch_size,
            "flush_interval_seconds": self.flush_interval_seconds,
        }
