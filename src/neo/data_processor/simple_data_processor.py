"""ç®€å•æ•°æ®å¤„ç†å™¨å®ç°

ä¸“æ³¨æ•°æ®æ¸…æ´—ã€è½¬æ¢å’ŒéªŒè¯çš„æ•°æ®å¤„ç†å±‚ã€‚
"""

import logging
from typing import Optional, Dict, Any, List
import pandas as pd
import time
import threading
from collections import defaultdict

from ..configs import get_config
from .interfaces import IDataProcessor
from ..database.operator import DBOperator
from ..database.interfaces import ISchemaLoader
from ..database.schema_loader import SchemaLoader

logger = logging.getLogger(__name__)


class SimpleDataProcessor(IDataProcessor):
    """ç®€åŒ–çš„æ•°æ®å¤„ç†å™¨å®ç°

    ä¸“æ³¨äºæ•°æ®æ¸…æ´—ã€è½¬æ¢å’ŒéªŒè¯ã€‚
    """

    @classmethod
    def create_default(cls) -> "SimpleDataProcessor":
        """åˆ›å»ºä½¿ç”¨é»˜è®¤é…ç½®çš„ SimpleDataProcessor å®ä¾‹

        Returns:
            ä½¿ç”¨é»˜è®¤é…ç½®çš„ SimpleDataProcessor å®ä¾‹
        """
        return cls(
            db_operator=DBOperator.create_default(),
            enable_batch=True,
            schema_loader=SchemaLoader(),
        )

    def __init__(
        self,
        db_operator: Optional[DBOperator] = None,
        enable_batch: bool = True,
        schema_loader: Optional[ISchemaLoader] = None,
    ):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            db_operator: æ•°æ®åº“æ“ä½œå™¨ï¼Œç”¨äºä¿å­˜æ•°æ®
            enable_batch: æ˜¯å¦å¯ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼
            schema_loader: Schema åŠ è½½å™¨ï¼Œç”¨äºè·å–è¡¨åæ˜ å°„
        """
        self.config = get_config()
        self.db_operator = db_operator or DBOperator()
        self.enable_batch = enable_batch
        self.schema_loader = schema_loader or SchemaLoader()

        # æ‰¹é‡å¤„ç†é…ç½®
        self.batch_size = self.config.data_processor.batch_size
        self.flush_interval_seconds = self.config.data_processor.flush_interval_seconds

        # æ‰¹é‡å¤„ç†ç¼“å†²åŒºï¼šæŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„å­˜å‚¨å¾…å¤„ç†æ•°æ®
        self.batch_buffers: Dict[str, List[pd.DataFrame]] = defaultdict(list)
        self.buffer_lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨é”
        self.last_flush_time = time.time()

    def _get_table_name(self, task_type) -> Optional[str]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–å¯¹åº”çš„è¡¨å

        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æšä¸¾ï¼‰

        Returns:
            å¯¹åº”çš„è¡¨åï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å› None
        """
        try:
            # å¦‚æœæ˜¯æšä¸¾ç±»å‹ï¼Œä½¿ç”¨å…¶ name å±æ€§
            type_name = task_type.name if hasattr(task_type, "name") else str(task_type)
            schema = self.schema_loader.load_schema(type_name)
            return schema.table_name
        except KeyError:
            type_name = task_type.name if hasattr(task_type, "name") else str(task_type)
            logger.warning(f"æœªæ‰¾åˆ°ä»»åŠ¡ç±»å‹ '{type_name}' å¯¹åº”çš„è¡¨é…ç½®")
            return None

    def process(self, task_type: str, data: pd.DataFrame) -> bool:
        """å¤„ç†ä»»åŠ¡ç»“æœ

        Args:
            task_type: ä»»åŠ¡ç±»å‹å­—ç¬¦ä¸²
            data: è¦å¤„ç†çš„æ•°æ®

        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        logger.debug(f"å¤„ç†ä»»åŠ¡: {task_type}")

        try:
            # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
            if data is None or data.empty:
                logger.warning("æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                return False

            logger.debug(f"æ•°æ®ç»´åº¦: {len(data)} è¡Œ x {len(data.columns)} åˆ—")

            # æ ¹æ®æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
            if self.enable_batch:
                # æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šæ·»åŠ åˆ°ç¼“å†²åŒº
                success = self._add_to_buffer(data, task_type)
                if success:
                    logger.debug(f"æ•°æ®å·²æ·»åŠ åˆ°ç¼“å†²åŒº: {task_type}, rows: {len(data)}")

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å†²åŒº
                    individual_flushed = False
                    if self._should_flush(task_type):
                        flush_success = self._flush_buffer(task_type)
                        if not flush_success:
                            success = False
                        else:
                            individual_flushed = True
                            # å•ç‹¬åˆ·æ–°æˆåŠŸåï¼Œæ›´æ–°æœ€ååˆ·æ–°æ—¶é—´ï¼Œé¿å…å®šæ—¶åˆ·æ–°ç«‹å³è§¦å‘
                            self.last_flush_time = time.time()

                    # åªæœ‰åœ¨æ²¡æœ‰è¿›è¡Œå•ç‹¬åˆ·æ–°æ—¶æ‰æ£€æŸ¥å®šæ—¶åˆ·æ–°
                    if not individual_flushed:
                        self._check_and_flush_all_buffers()
            else:
                # å•æ¡å¤„ç†æ¨¡å¼ï¼šç›´æ¥ä¿å­˜
                success = self._save_data(data, task_type)

            if success:
                if not self.enable_batch:
                    logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(data)} è¡Œæ•°æ®")
            else:
                logger.warning(f"æ•°æ®å¤„ç†å¤±è´¥: {task_type}")

            return success

        except Exception as e:
            print(f"ğŸ’¥ å¤„ç†å¼‚å¸¸: {task_type} - {str(e)}")
            logger.error(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
            return False



    def _save_data(self, data: pd.DataFrame, task_type: str) -> bool:
        """æ•°æ®ä¿å­˜

        å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ã€‚

        Args:
            data: è½¬æ¢åçš„æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å° task_type çš„ç±»å‹å’Œå€¼
            logger.debug(f"task_type ç±»å‹: {type(task_type)}, å€¼: {task_type}")

            # æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è·å–è¡¨å
            table_name = self._get_table_name(task_type)
            if not table_name:
                logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")
                return False

            # ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
            self.db_operator.upsert(table_name, data)
            logger.info(f"æ•°æ®ä¿å­˜æˆåŠŸ: {table_name}, {len(data)} rows")

            return True

        except Exception as e:
            logger.error(f"æ•°æ®ä¿å­˜å¤±è´¥: {e}")
            return False

    def _add_to_buffer(self, data: pd.DataFrame, task_type) -> bool:
        """å°†æ•°æ®æ·»åŠ åˆ°æ‰¹é‡å¤„ç†ç¼“å†²åŒº

        Args:
            data: è¦æ·»åŠ çš„æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æšä¸¾ï¼‰

        Returns:
            bool: æ·»åŠ æ˜¯å¦æˆåŠŸ
        """
        try:
            # è½¬æ¢ä»»åŠ¡ç±»å‹ä¸ºå­—ç¬¦ä¸²é”®
            type_key = task_type.name if hasattr(task_type, "name") else str(task_type)

            with self.buffer_lock:
                self.batch_buffers[type_key].append(data.copy())

            logger.debug(
                f"æ•°æ®å·²æ·»åŠ åˆ°ç¼“å†²åŒº: {type_key}, {len(data)} è¡Œ, ç¼“å†²åŒºå¤§å°: {len(self.batch_buffers[type_key])}"
            )
            return True

        except Exception as e:
            type_key = task_type.name if hasattr(task_type, "name") else str(task_type)
            logger.error(f"æ·»åŠ æ•°æ®åˆ°ç¼“å†²åŒºå¤±è´¥: {type_key} - {e}")
            return False

    def _flush_buffer(self, task_type: str, force: bool = False) -> bool:
        """åˆ·æ–°æŒ‡å®šä»»åŠ¡ç±»å‹çš„ç¼“å†²åŒºæ•°æ®åˆ°æ•°æ®åº“

        Args:
            task_type: ä»»åŠ¡ç±»å‹
            force: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥æ‰¹é‡å¤§å°é™åˆ¶ï¼‰

        Returns:
            bool: åˆ·æ–°æ˜¯å¦æˆåŠŸ
        """
        with self.buffer_lock:
            if task_type not in self.batch_buffers or not self.batch_buffers[task_type]:
                return True  # æ²¡æœ‰æ•°æ®éœ€è¦åˆ·æ–°ï¼Œé™é»˜è¿”å›

            buffer_data = self.batch_buffers[task_type]

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆæŒ‰æ•°æ®è¡Œæ•°è®¡ç®—ï¼‰
            if not force:
                total_rows = sum(len(df) for df in buffer_data)
                if total_rows < self.batch_size:
                    return True  # ä¸éœ€è¦åˆ·æ–°

            try:
                # åˆå¹¶æ‰€æœ‰ç¼“å†²åŒºæ•°æ®
                if len(buffer_data) == 1:
                    combined_data = buffer_data[0]
                else:
                    combined_data = pd.concat(buffer_data, ignore_index=True)

                # æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è·å–è¡¨å
                table_name = self._get_table_name(task_type)
                if not table_name:
                    logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")
                    return False

                # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
                self.db_operator.upsert(table_name, combined_data)
                logger.info(f"æ‰¹é‡ä¿å­˜æˆåŠŸ: {table_name}, {len(combined_data)} è¡Œæ•°æ®")

                # æ¸…ç©ºç¼“å†²åŒº
                self.batch_buffers[task_type].clear()

                logger.info(f"âœ… æ‰¹é‡ä¿å­˜ {len(combined_data)} è¡Œæ•°æ®åˆ° {table_name}")
                return True

            except Exception as e:
                logger.error(f"æ‰¹é‡åˆ·æ–°å¤±è´¥: {task_type} - {e}")
                return False

    def _should_flush(self, task_type: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ·æ–°ç¼“å†²åŒºï¼ˆä»…åŸºäºæ‰¹é‡å¤§å°ï¼‰

        Args:
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            bool: æ˜¯å¦åº”è¯¥åˆ·æ–°
        """
        with self.buffer_lock:
            # åªæ£€æŸ¥æ‰¹é‡å¤§å°ï¼ˆæŒ‰æ•°æ®è¡Œæ•°è®¡ç®—ï¼‰
            if task_type in self.batch_buffers and self.batch_buffers[task_type]:
                total_rows = sum(len(df) for df in self.batch_buffers[task_type])
                if total_rows >= self.batch_size:
                    return True

            return False

    def _check_and_flush_all_buffers(self) -> None:
        """æ£€æŸ¥å¹¶åˆ·æ–°æ‰€æœ‰éœ€è¦åˆ·æ–°çš„ç¼“å†²åŒº"""
        current_time = time.time()
        if current_time - self.last_flush_time >= self.flush_interval_seconds:
            # åªåˆ·æ–°é‚£äº›æœ‰æ•°æ®ä½†æœªè¾¾åˆ°æ‰¹é‡å¤§å°çš„ç¼“å†²åŒº
            flushed_any = False
            with self.buffer_lock:
                for task_type, buffer_data in self.batch_buffers.items():
                    if buffer_data:  # æœ‰æ•°æ®
                        total_rows = sum(len(df) for df in buffer_data)
                        if total_rows < self.batch_size:  # æœªè¾¾åˆ°æ‰¹é‡å¤§å°
                            if self._flush_buffer(task_type, force=True):
                                flushed_any = True

            if flushed_any:
                self.last_flush_time = current_time



    def flush_all(self, force: bool = True) -> bool:
        """åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒºæ•°æ®åˆ°æ•°æ®åº“

        Args:
            force: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ•°æ®ï¼ˆå¿½ç•¥æ‰¹é‡å¤§å°é™åˆ¶ï¼‰

        Returns:
            bool: æ‰€æœ‰åˆ·æ–°æ˜¯å¦æˆåŠŸ
        """
        success = True
        flushed_types = []

        with self.buffer_lock:
            # è·å–æ‰€æœ‰æœ‰æ•°æ®çš„ä»»åŠ¡ç±»å‹
            task_types_to_flush = [
                task_type
                for task_type, buffer_data in self.batch_buffers.items()
                if buffer_data
            ]

        if not task_types_to_flush:
            logger.debug("æ²¡æœ‰ç¼“å†²åŒºæ•°æ®éœ€è¦åˆ·æ–°")
            return True

        logger.debug(f"å¼€å§‹åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº: {len(task_types_to_flush)} ä¸ªä»»åŠ¡ç±»å‹")

        for task_type in task_types_to_flush:
            # ç›´æ¥è°ƒç”¨ _flush_bufferï¼Œé¿å…åµŒå¥—é”
            if self._flush_buffer(task_type, force=force):
                flushed_types.append(task_type)
            else:
                success = False

        if flushed_types:
            logger.debug(f"æ‰¹é‡åˆ·æ–°å®Œæˆ: {', '.join(flushed_types)}")

        return success
