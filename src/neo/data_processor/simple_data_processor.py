"""ç®€å•æ•°æ®å¤„ç†å™¨å®ç°

ä¸“æ³¨æ•°æ®æ¸…æ´—ã€è½¬æ¢å’ŒéªŒè¯çš„æ•°æ®å¤„ç†å±‚ã€‚
"""

import logging
from typing import Optional, Dict, Any, List
import pandas as pd
import time
from datetime import timedelta
import threading
from collections import defaultdict

from ..config import get_config
from .interfaces import IDataProcessor
from .types import TaskResult
from ..database.operator import DBOperator
from ..database.interfaces import ISchemaLoader
from ..database.schema_loader import SchemaLoader

logger = logging.getLogger(__name__)


class SimpleDataProcessor(IDataProcessor):
    """ç®€åŒ–çš„æ•°æ®å¤„ç†å™¨å®ç°

    ä¸“æ³¨äºæ•°æ®æ¸…æ´—ã€è½¬æ¢å’ŒéªŒè¯ã€‚
    """

    def __init__(
        self,
        db_operator: Optional[DBOperator] = None,
        enable_batch: bool = True,
        schema_loader: Optional[ISchemaLoader] = None,
    ):
        """åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨

        Args:
            db_operator: æ•°æ®åº“æ“ä½œå™¨ï¼Œç”¨äºä¿å­˜æ•°æ®
            enable_batch: æ˜¯å¦å¯ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼
            schema_loader: Schema åŠ è½½å™¨ï¼Œç”¨äºè·å–è¡¨åæ˜ å°„
        """
        self.config = get_config()
        self.db_operator = db_operator or DBOperator()
        self.enable_batch = enable_batch
        self.schema_loader = schema_loader or SchemaLoader()

        # æ‰¹é‡å¤„ç†é…ç½®
        self.batch_size = self.config.data_processor.batch_size
        self.flush_interval_seconds = self.config.data_processor.flush_interval_seconds

        # æ‰¹é‡å¤„ç†ç¼“å†²åŒºï¼šæŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„å­˜å‚¨å¾…å¤„ç†æ•°æ®
        self.batch_buffers: Dict[str, List[pd.DataFrame]] = defaultdict(list)
        self.buffer_lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨é”
        self.last_flush_time = time.time()

        # ç»Ÿè®¡ä¿¡æ¯è·Ÿè¸ª
        self.stats = {
            "total_processed": 0,
            "successful_processed": 0,
            "failed_processed": 0,
            "total_rows_processed": 0,
            "start_time": time.time(),
            "last_stats_output": time.time(),
            "task_type_stats": {},  # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
            "batch_flushes": 0,  # æ‰¹é‡åˆ·æ–°æ¬¡æ•°
            "buffered_items": 0,  # å½“å‰ç¼“å†²åŒºé¡¹ç›®æ•°
        }

        # ç»Ÿè®¡è¾“å‡ºé—´éš”ï¼ˆç§’ï¼‰
        self.stats_output_interval = 30

    def _get_table_name(self, task_type) -> Optional[str]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–å¯¹åº”çš„è¡¨å

        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æšä¸¾ï¼‰

        Returns:
            å¯¹åº”çš„è¡¨åï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å› None
        """
        try:
            # å¦‚æœæ˜¯æšä¸¾ç±»å‹ï¼Œä½¿ç”¨å…¶ name å±æ€§
            type_name = task_type.name if hasattr(task_type, "name") else str(task_type)
            schema = self.schema_loader.load_schema(type_name)
            return schema.table_name
        except KeyError:
            type_name = task_type.name if hasattr(task_type, "name") else str(task_type)
            logger.warning(f"æœªæ‰¾åˆ°ä»»åŠ¡ç±»å‹ '{type_name}' å¯¹åº”çš„è¡¨é…ç½®")
            return None

    def process(self, task_type: str, data: pd.DataFrame) -> bool:
        """å¤„ç†ä»»åŠ¡ç»“æœ

        Args:
            task_type: ä»»åŠ¡ç±»å‹å­—ç¬¦ä¸²
            data: è¦å¤„ç†çš„æ•°æ®

        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        # ç¡®ä¿ä»»åŠ¡ç±»å‹ç»Ÿè®¡ç»“æ„å­˜åœ¨
        if task_type not in self.stats["task_type_stats"]:
            self.stats["task_type_stats"][task_type] = {
                "count": 0,
                "success": 0,
                "rows": 0,
            }

        logger.debug(f"å¤„ç†ä»»åŠ¡: {task_type}")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._maybe_output_stats()

        try:
            # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
            if data is None or data.empty:
                logger.warning("æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¤„ç†")
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼šæ€»å¤„ç†æ•°å’Œå¤±è´¥æ•°
                self.stats["total_processed"] += 1
                self.stats["failed_processed"] += 1
                self.stats["task_type_stats"][task_type]["count"] += 1
                return False

            logger.debug(
                f"æ•°æ®ç»´åº¦: {len(data)} è¡Œ x {len(data.columns)} åˆ—"
            )

            # æ ¹æ®æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
            if self.enable_batch:
                # æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šæ·»åŠ åˆ°ç¼“å†²åŒº
                success = self._add_to_buffer(data, task_type)
                if success:
                    logger.debug(
                        f"æ•°æ®å·²æ·»åŠ åˆ°ç¼“å†²åŒº: {task_type}, rows: {len(data)}"
                    )

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å†²åŒº
                    individual_flushed = False
                    if self._should_flush(task_type):
                        flush_success = self._flush_buffer(task_type)
                        if not flush_success:
                            success = False
                        else:
                            individual_flushed = True
                            # å•ç‹¬åˆ·æ–°æˆåŠŸåï¼Œæ›´æ–°æœ€ååˆ·æ–°æ—¶é—´ï¼Œé¿å…å®šæ—¶åˆ·æ–°ç«‹å³è§¦å‘
                            self.last_flush_time = time.time()

                    # åªæœ‰åœ¨æ²¡æœ‰è¿›è¡Œå•ç‹¬åˆ·æ–°æ—¶æ‰æ£€æŸ¥å®šæ—¶åˆ·æ–°
                    if not individual_flushed:
                        self._check_and_flush_all_buffers()
            else:
                # å•æ¡å¤„ç†æ¨¡å¼ï¼šç›´æ¥ä¿å­˜
                success = self._save_data(data, task_type)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼šæ€»å¤„ç†æ•°å’Œä»»åŠ¡ç±»å‹è®¡æ•°
            self.stats["total_processed"] += 1
            self.stats["task_type_stats"][task_type]["count"] += 1

            if success:
                if not self.enable_batch:
                    print(f"âœ… æˆåŠŸä¿å­˜ {len(data)} è¡Œæ•°æ®")
                    # åœ¨æ‰¹é‡æ¨¡å¼ä¸‹ï¼Œè¡Œæ•°ç»Ÿè®¡åœ¨åˆ·æ–°æ—¶æ›´æ–°
                    self.stats["total_rows_processed"] += len(data)

                # æ›´æ–°æˆåŠŸç»Ÿè®¡
                self.stats["successful_processed"] += 1
                self.stats["task_type_stats"][task_type]["success"] += 1
                if not self.enable_batch:
                    self.stats["task_type_stats"][task_type]["rows"] += len(data)
            else:
                logger.warning(f"æ•°æ®å¤„ç†å¤±è´¥: {task_type}")
                self.stats["failed_processed"] += 1

            return success

        except Exception as e:
            print(f"ğŸ’¥ å¤„ç†å¼‚å¸¸: {task_type} - {str(e)}")
            logger.error(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼šæ€»å¤„ç†æ•°å’Œå¤±è´¥æ•°
            self.stats["total_processed"] += 1
            self.stats["failed_processed"] += 1
            self.stats["task_type_stats"][task_type]["count"] += 1
            return False

    def _clean_data(self, data: pd.DataFrame, task_type: str) -> Optional[pd.DataFrame]:
        """æ•°æ®æ¸…æ´—

        Args:
            data: åŸå§‹æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            æ¸…æ´—åçš„æ•°æ®ï¼Œå¦‚æœæ¸…æ´—å¤±è´¥è¿”å›None
        """
        try:
            cleaned_data = data.copy()

            # æ ¹æ®ä»»åŠ¡ç±»å‹è¿›è¡Œç‰¹å®šæ¸…æ´—
            if task_type == "stock_basic":
                # è‚¡ç¥¨åŸºç¡€ä¿¡æ¯æ¸…æ´—
                required_columns = ["ts_code", "symbol", "name"]
                if not all(col in cleaned_data.columns for col in required_columns):
                    logger.warning(f"è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ç¼ºå°‘å¿…è¦å­—æ®µ: {required_columns}")
                    return None
                # åªç§»é™¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œ
                cleaned_data = cleaned_data.dropna(subset=required_columns)
            elif task_type in ["daily", "weekly", "monthly"]:
                # è¡Œæƒ…æ•°æ®æ¸…æ´—
                required_columns = [
                    "ts_code",
                    "trade_date",
                    "open",
                    "high",
                    "low",
                    "close",
                ]
                if not all(col in cleaned_data.columns for col in required_columns):
                    logger.warning(f"è¡Œæƒ…æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {required_columns}")
                    return None

                # åªç§»é™¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œ
                cleaned_data = cleaned_data.dropna(subset=required_columns)

                # ç¡®ä¿ä»·æ ¼æ•°æ®ä¸ºæ­£æ•°
                price_columns = ["open", "high", "low", "close"]
                for col in price_columns:
                    if col in cleaned_data.columns:
                        cleaned_data = cleaned_data[cleaned_data[col] > 0]
            elif task_type in ["income", "balancesheet", "cashflow"]:
                # è´¢åŠ¡æ•°æ®æ¸…æ´— - åªæ£€æŸ¥å…³é”®å­—æ®µ
                required_columns = ["ts_code", "end_date"]
                if not all(col in cleaned_data.columns for col in required_columns):
                    logger.warning(f"è´¢åŠ¡æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {required_columns}")
                    return None
                # åªç§»é™¤å…³é”®å­—æ®µä¸ºç©ºçš„è¡Œ
                cleaned_data = cleaned_data.dropna(subset=required_columns)
            else:
                # å…¶ä»–ç±»å‹æ•°æ®ï¼Œä¿æŒåŸæ ·ï¼Œä¸è¿›è¡Œä¸¥æ ¼çš„ç©ºå€¼æ¸…æ´—
                pass

            logger.debug(f"æ•°æ®æ¸…æ´—å®Œæˆ: {len(data)} -> {len(cleaned_data)} rows")
            return cleaned_data

        except Exception as e:
            logger.error(f"æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            return None

    def _transform_data(
        self, data: pd.DataFrame, task_type: str
    ) -> Optional[pd.DataFrame]:
        """æ•°æ®è½¬æ¢

        Args:
            data: æ¸…æ´—åçš„æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            è½¬æ¢åçš„æ•°æ®ï¼Œå¦‚æœè½¬æ¢å¤±è´¥è¿”å›None
        """
        try:
            transformed_data = data.copy()

            # æ ¹æ®ä»»åŠ¡ç±»å‹è¿›è¡Œç‰¹å®šè½¬æ¢
            if task_type in ["daily", "weekly", "monthly"]:
                # è¡Œæƒ…æ•°æ®è½¬æ¢
                if "trade_date" in transformed_data.columns:
                    # ç¡®ä¿äº¤æ˜“æ—¥æœŸæ ¼å¼æ­£ç¡®
                    transformed_data["trade_date"] = pd.to_datetime(
                        transformed_data["trade_date"], format="%Y%m%d"
                    )

                # è®¡ç®—æ¶¨è·Œå¹…ï¼ˆå¦‚æœæœ‰å‰æ”¶ç›˜ä»·ï¼‰
                if "pre_close" in transformed_data.columns:
                    transformed_data["pct_chg"] = (
                        (transformed_data["close"] - transformed_data["pre_close"])
                        / transformed_data["pre_close"]
                        * 100
                    ).round(2)

            logger.debug(f"æ•°æ®è½¬æ¢å®Œæˆ: {len(transformed_data)} rows")
            return transformed_data

        except Exception as e:
            logger.error(f"æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            return None

    def _save_data(self, data: pd.DataFrame, task_type: str) -> bool:
        """æ•°æ®ä¿å­˜

        å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ã€‚

        Args:
            data: è½¬æ¢åçš„æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å° task_type çš„ç±»å‹å’Œå€¼
            logger.debug(f"task_type ç±»å‹: {type(task_type)}, å€¼: {task_type}")

            # æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è·å–è¡¨å
            table_name = self._get_table_name(task_type)
            if not table_name:
                logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")
                return False

            # ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
            self.db_operator.upsert(table_name, data)
            logger.info(f"æ•°æ®ä¿å­˜æˆåŠŸ: {table_name}, {len(data)} rows")

            return True

        except Exception as e:
            logger.error(f"æ•°æ®ä¿å­˜å¤±è´¥: {e}")
            return False

    def _add_to_buffer(self, data: pd.DataFrame, task_type) -> bool:
        """å°†æ•°æ®æ·»åŠ åˆ°æ‰¹é‡å¤„ç†ç¼“å†²åŒº

        Args:
            data: è¦æ·»åŠ çš„æ•°æ®
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æšä¸¾ï¼‰

        Returns:
            bool: æ·»åŠ æ˜¯å¦æˆåŠŸ
        """
        try:
            # è½¬æ¢ä»»åŠ¡ç±»å‹ä¸ºå­—ç¬¦ä¸²é”®
            type_key = task_type.name if hasattr(task_type, "name") else str(task_type)

            with self.buffer_lock:
                self.batch_buffers[type_key].append(data.copy())
                self.stats["buffered_items"] += len(data)

            logger.debug(
                f"æ•°æ®å·²æ·»åŠ åˆ°ç¼“å†²åŒº: {type_key}, {len(data)} è¡Œ, ç¼“å†²åŒºå¤§å°: {len(self.batch_buffers[type_key])}"
            )
            return True

        except Exception as e:
            type_key = task_type.name if hasattr(task_type, "name") else str(task_type)
            logger.error(f"æ·»åŠ æ•°æ®åˆ°ç¼“å†²åŒºå¤±è´¥: {type_key} - {e}")
            return False

    def _flush_buffer(self, task_type: str, force: bool = False) -> bool:
        """åˆ·æ–°æŒ‡å®šä»»åŠ¡ç±»å‹çš„ç¼“å†²åŒºæ•°æ®åˆ°æ•°æ®åº“

        Args:
            task_type: ä»»åŠ¡ç±»å‹
            force: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥æ‰¹é‡å¤§å°é™åˆ¶ï¼‰

        Returns:
            bool: åˆ·æ–°æ˜¯å¦æˆåŠŸ
        """
        with self.buffer_lock:
            if task_type not in self.batch_buffers or not self.batch_buffers[task_type]:
                return True  # æ²¡æœ‰æ•°æ®éœ€è¦åˆ·æ–°ï¼Œé™é»˜è¿”å›

            buffer_data = self.batch_buffers[task_type]

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆæŒ‰æ•°æ®è¡Œæ•°è®¡ç®—ï¼‰
            if not force:
                total_rows = sum(len(df) for df in buffer_data)
                if total_rows < self.batch_size:
                    return True  # ä¸éœ€è¦åˆ·æ–°

            try:
                # åˆå¹¶æ‰€æœ‰ç¼“å†²åŒºæ•°æ®
                if len(buffer_data) == 1:
                    combined_data = buffer_data[0]
                else:
                    combined_data = pd.concat(buffer_data, ignore_index=True)

                # æ ¹æ®ä»»åŠ¡ç±»å‹åŠ¨æ€è·å–è¡¨å
                table_name = self._get_table_name(task_type)
                if not table_name:
                    logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task_type}")
                    return False

                # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
                self.db_operator.upsert(table_name, combined_data)
                logger.info(f"æ‰¹é‡åˆ·æ–°æˆåŠŸ: {table_name}, {len(combined_data)} è¡Œæ•°æ®")

                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.stats["batch_flushes"] += 1
                self.stats["total_rows_processed"] += len(combined_data)

                # è®¡ç®—è¦å‡å°‘çš„ç¼“å†²é¡¹ç›®æ•°ï¼ˆæŒ‰è¡Œæ•°è®¡ç®—ï¼‰
                buffered_rows = sum(len(df) for df in buffer_data)

                # æ¸…ç©ºç¼“å†²åŒº
                self.batch_buffers[task_type].clear()
                self.stats["buffered_items"] -= buffered_rows

                print(f"âœ… æ‰¹é‡ä¿å­˜ {len(combined_data)} è¡Œæ•°æ®åˆ° {table_name}")
                return True

            except Exception as e:
                logger.error(f"æ‰¹é‡åˆ·æ–°å¤±è´¥: {task_type} - {e}")
                return False

    def _should_flush(self, task_type: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ·æ–°ç¼“å†²åŒºï¼ˆä»…åŸºäºæ‰¹é‡å¤§å°ï¼‰

        Args:
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            bool: æ˜¯å¦åº”è¯¥åˆ·æ–°
        """
        with self.buffer_lock:
            # åªæ£€æŸ¥æ‰¹é‡å¤§å°ï¼ˆæŒ‰æ•°æ®è¡Œæ•°è®¡ç®—ï¼‰
            if task_type in self.batch_buffers and self.batch_buffers[task_type]:
                total_rows = sum(len(df) for df in self.batch_buffers[task_type])
                if total_rows >= self.batch_size:
                    return True

            return False

    def _check_and_flush_all_buffers(self) -> None:
        """æ£€æŸ¥å¹¶åˆ·æ–°æ‰€æœ‰éœ€è¦åˆ·æ–°çš„ç¼“å†²åŒº"""
        current_time = time.time()
        if current_time - self.last_flush_time >= self.flush_interval_seconds:
            # åªåˆ·æ–°é‚£äº›æœ‰æ•°æ®ä½†æœªè¾¾åˆ°æ‰¹é‡å¤§å°çš„ç¼“å†²åŒº
            flushed_any = False
            with self.buffer_lock:
                for task_type, buffer_data in self.batch_buffers.items():
                    if buffer_data:  # æœ‰æ•°æ®
                        total_rows = sum(len(df) for df in buffer_data)
                        if total_rows < self.batch_size:  # æœªè¾¾åˆ°æ‰¹é‡å¤§å°
                            if self._flush_buffer(task_type, force=True):
                                flushed_any = True

            if flushed_any:
                self.last_flush_time = current_time

    def _maybe_output_stats(self) -> None:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è¾“å‡ºç»Ÿè®¡ä¿¡æ¯"""
        current_time = time.time()
        if current_time - self.stats["last_stats_output"] >= self.stats_output_interval:
            self._output_stats()
            self.stats["last_stats_output"] = current_time

    def _output_stats(self) -> None:
        """è¾“å‡ºå½“å‰ç»Ÿè®¡ä¿¡æ¯"""
        current_time = time.time()
        elapsed_time = current_time - self.stats["start_time"]

        # è®¡ç®—å¤„ç†é€Ÿç‡
        processing_rate = (
            self.stats["total_processed"] / elapsed_time if elapsed_time > 0 else 0
        )
        success_rate = (
            (self.stats["successful_processed"] / self.stats["total_processed"] * 100)
            if self.stats["total_processed"] > 0
            else 0
        )

        print("\n" + "=" * 60)
        print("ğŸ“ˆ æ•°æ®å¤„ç†ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 60)
        print(f"â±ï¸  è¿è¡Œæ—¶é—´: {timedelta(seconds=int(elapsed_time))}")
        print(f"ğŸ“Š æ€»å¤„ç†ä»»åŠ¡: {self.stats['total_processed']}")
        print(f"âœ… æˆåŠŸå¤„ç†: {self.stats['successful_processed']}")
        print(f"âŒ å¤±è´¥å¤„ç†: {self.stats['failed_processed']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"ğŸš€ å¤„ç†é€Ÿç‡: {processing_rate:.2f} ä»»åŠ¡/ç§’")
        print(f"ğŸ“‹ æ€»å¤„ç†è¡Œæ•°: {self.stats['total_rows_processed']}")

        # æ‰¹é‡å¤„ç†ç»Ÿè®¡
        if self.enable_batch:
            print(f"ğŸ”„ æ‰¹é‡åˆ·æ–°æ¬¡æ•°: {self.stats['batch_flushes']}")
            print(f"ğŸ“¦ å½“å‰ç¼“å†²é¡¹ç›®: {self.stats['buffered_items']}")

            # æ˜¾ç¤ºå„ç¼“å†²åŒºçŠ¶æ€
            if self.batch_buffers:
                print("\nğŸ“¦ ç¼“å†²åŒºçŠ¶æ€:")
                with self.buffer_lock:
                    for task_type, buffer_data in self.batch_buffers.items():
                        if buffer_data:
                            total_rows = sum(len(df) for df in buffer_data)
                            print(
                                f"  {task_type}: {len(buffer_data)} ä¸ªä»»åŠ¡, {total_rows} è¡Œæ•°æ®"
                            )

        # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        if self.stats["task_type_stats"]:
            print("\nğŸ“‹ æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡:")
            for task_type, stats in self.stats["task_type_stats"].items():
                task_success_rate = (
                    (stats["success"] / stats["count"] * 100)
                    if stats["count"] > 0
                    else 0
                )
                print(
                    f"  {task_type}: {stats['count']} ä»»åŠ¡, {stats['success']} æˆåŠŸ ({task_success_rate:.1f}%), {stats['rows']} è¡Œ"
                )

        print("=" * 60 + "\n")

        # åŒæ—¶è®°å½•åˆ°æ—¥å¿—
        logger.info(
            f"ç»Ÿè®¡ä¿¡æ¯ - æ€»ä»»åŠ¡: {self.stats['total_processed']}, æˆåŠŸ: {self.stats['successful_processed']}, å¤±è´¥: {self.stats['failed_processed']}, æˆåŠŸç‡: {success_rate:.1f}%, å¤„ç†é€Ÿç‡: {processing_rate:.2f} ä»»åŠ¡/ç§’, æ€»è¡Œæ•°: {self.stats['total_rows_processed']}, æ‰¹é‡åˆ·æ–°: {self.stats['batch_flushes']}, ç¼“å†²é¡¹ç›®: {self.stats['buffered_items']}"
        )

    def flush_all(self, force: bool = True) -> bool:
        """åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒºæ•°æ®åˆ°æ•°æ®åº“

        Args:
            force: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰æ•°æ®ï¼ˆå¿½ç•¥æ‰¹é‡å¤§å°é™åˆ¶ï¼‰

        Returns:
            bool: æ‰€æœ‰åˆ·æ–°æ˜¯å¦æˆåŠŸ
        """
        success = True
        flushed_types = []

        with self.buffer_lock:
            # è·å–æ‰€æœ‰æœ‰æ•°æ®çš„ä»»åŠ¡ç±»å‹
            task_types_to_flush = [
                task_type
                for task_type, buffer_data in self.batch_buffers.items()
                if buffer_data
            ]

        if not task_types_to_flush:
            logger.debug("æ²¡æœ‰ç¼“å†²åŒºæ•°æ®éœ€è¦åˆ·æ–°")
            return True

        logger.debug(f"å¼€å§‹åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº: {len(task_types_to_flush)} ä¸ªä»»åŠ¡ç±»å‹")

        for task_type in task_types_to_flush:
            # ç›´æ¥è°ƒç”¨ _flush_bufferï¼Œé¿å…åµŒå¥—é”
            if self._flush_buffer(task_type, force=force):
                flushed_types.append(task_type)
            else:
                success = False

        if flushed_types:
            logger.debug(f"æ‰¹é‡åˆ·æ–°å®Œæˆ: {', '.join(flushed_types)}")

        return success

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯

        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        current_time = time.time()
        elapsed_time = current_time - self.stats["start_time"]
        processing_rate = (
            self.stats["total_processed"] / elapsed_time if elapsed_time > 0 else 0
        )
        success_rate = (
            (self.stats["successful_processed"] / self.stats["total_processed"] * 100)
            if self.stats["total_processed"] > 0
            else 0
        )

        # è·å–ç¼“å†²åŒºçŠ¶æ€
        buffer_status = {}
        if self.enable_batch:
            with self.buffer_lock:
                for task_type, buffer_data in self.batch_buffers.items():
                    if buffer_data:
                        total_rows = sum(len(df) for df in buffer_data)
                        buffer_status[task_type] = {
                            "tasks": len(buffer_data),
                            "rows": total_rows,
                        }

        return {
            "elapsed_time": elapsed_time,
            "total_processed": self.stats["total_processed"],
            "successful_processed": self.stats["successful_processed"],
            "failed_processed": self.stats["failed_processed"],
            "success_rate": success_rate,
            "processing_rate": processing_rate,
            "total_rows_processed": self.stats["total_rows_processed"],
            "task_type_stats": self.stats["task_type_stats"].copy(),
            "batch_enabled": self.enable_batch,
            "batch_flushes": self.stats["batch_flushes"],
            "buffered_items": self.stats["buffered_items"],
            "buffer_status": buffer_status,
            "batch_size": self.batch_size,
            "flush_interval_seconds": self.flush_interval_seconds,
        }
