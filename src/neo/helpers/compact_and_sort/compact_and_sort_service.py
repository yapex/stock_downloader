"""
æ•°æ®å‹ç¼©å’Œæ’åºæœåŠ¡å®ç°
"""

import os
import shutil
from pathlib import Path
from typing import Dict, Any
import ibis
import duckdb
import pandas as pd

from .interfaces import (
    CompactAndSortService,
    DataDeduplicationStrategy,
    FilenameGenerator,
)
from .deduplication_strategies import HybridDeduplicationStrategy
from .filename_generator import UUIDFilenameGenerator


class CompactAndSortServiceImpl(CompactAndSortService):
    """æ•°æ®å‹ç¼©å’Œæ’åºæœåŠ¡å®ç°"""

    def __init__(
        self,
        data_dir: Path,
        temp_dir: Path,
        deduplication_strategy: DataDeduplicationStrategy = None,
        filename_generator: FilenameGenerator = None,
    ):
        """
        åˆå§‹åŒ–æœåŠ¡

        Args:
            data_dir: æ•°æ®ç›®å½•
            temp_dir: ä¸´æ—¶ç›®å½•
            deduplication_strategy: æ•°æ®å»é‡ç­–ç•¥
            filename_generator: æ–‡ä»¶åç”Ÿæˆå™¨
        """
        self.data_dir = data_dir
        self.temp_dir = temp_dir

        # ä½¿ç”¨é»˜è®¤ç­–ç•¥å¦‚æœæœªæä¾›
        self.deduplication_strategy = (
            deduplication_strategy or HybridDeduplicationStrategy()
        )
        self.filename_generator = filename_generator or UUIDFilenameGenerator()

    def compact_and_sort_table(
        self, table_name: str, table_config: Dict[str, Any]
    ) -> None:
        """
        å¯¹å•ä¸ªè¡¨è¿›è¡Œå‹ç¼©å’Œæ’åº

        Args:
            table_name: è¡¨å
            table_config: è¡¨é…ç½®ä¿¡æ¯
        """
        source_path = self.data_dir / table_name
        target_path = self.temp_dir / table_name

        is_partitioned = "date_col" in table_config

        # 1. å‰ç½®æ ¡éªŒ
        self._validate_source_directory_structure(source_path, is_partitioned)

        # 2. ä½¿ç”¨DuckDBè¿æ¥å¤„ç†æ•°æ®
        with duckdb.connect(database=":memory:") as con:
            # 3. è¯»å–æ•°æ®å¹¶åº”ç”¨å»é‡ç­–ç•¥
            table = self._load_and_deduplicate_data(
                con, table_name, table_config, is_partitioned
            )

            # 4. å‡†å¤‡SQLå¹¶æ‰§è¡Œä¼˜åŒ–
            self._execute_optimization(
                con, table, table_name, table_config, target_path, is_partitioned
            )

            # 5. æ•°æ®ä¸€è‡´æ€§æ ¡éªŒ
            is_valid, needs_confirmation = self._validate_data_consistency(
                con, table_name, source_path, target_path, is_partitioned
            )

            if not is_valid:
                raise ValueError(
                    f"æ•°æ®éªŒè¯å¤±è´¥ï¼Œä¼˜åŒ–ä¸­æ–­ã€‚ä¸´æ—¶æ•°æ®ä¿ç•™åœ¨ {target_path}"
                )

        # 6. æ›¿æ¢æ—§æ•°æ®
        self._replace_old_data(source_path, target_path, needs_confirmation)

    def _validate_source_directory_structure(
        self, source_path: Path, is_partitioned: bool
    ):
        """å‰ç½®æ ¡éªŒï¼šæ£€æŸ¥æºç›®å½•ç»“æ„æ˜¯å¦ç¬¦åˆè§„èŒƒ"""
        print(f"  æ ¡éªŒç›®å½•ç»“æ„: {source_path}")
        if not source_path.exists() or not source_path.is_dir():
            raise FileNotFoundError(f"æºç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•: {source_path}")

        # ç³»ç»Ÿæ–‡ä»¶ç™½åå•
        system_files_whitelist = {
            ".DS_Store",
            "Thumbs.db",
            ".gitignore",
            "desktop.ini",
            ".localized",
            "._.DS_Store",
            "Icon\r",
        }

        ignored_files_count = 0

        if is_partitioned:
            # åˆ†åŒºè¡¨ï¼šç›®å½•ä¸‹åªå…è®¸å­˜åœ¨ year=... çš„å­ç›®å½•
            for item in source_path.iterdir():
                if item.name in system_files_whitelist:
                    ignored_files_count += 1
                    continue

                if item.is_file():
                    raise ValueError(
                        f"æ ¡éªŒå¤±è´¥ï¼åˆ†åŒºè¡¨ {source_path.name} çš„æ ¹ç›®å½•ä¸åº”åŒ…å«ä»»ä½•æ–‡ä»¶ï¼Œå‘ç°: {item.name}"
                    )
                if not item.name.startswith("year="):
                    raise ValueError(
                        f"æ ¡éªŒå¤±è´¥ï¼åˆ†åŒºè¡¨ {source_path.name} çš„å­ç›®å½•å¿…é¡»ä»¥ 'year=' å¼€å¤´ï¼Œå‘ç°: {item.name}"
                    )
        else:
            # éåˆ†åŒºè¡¨ï¼šç›®å½•ä¸‹åªå…è®¸å­˜åœ¨ .parquet æ–‡ä»¶
            for item in source_path.iterdir():
                if item.name in system_files_whitelist:
                    ignored_files_count += 1
                    continue

                if item.is_dir():
                    raise ValueError(
                        f"æ ¡éªŒå¤±è´¥ï¼éåˆ†åŒºè¡¨ {source_path.name} çš„æ ¹ç›®å½•ä¸åº”åŒ…å«ä»»ä½•å­ç›®å½•ï¼Œå‘ç°: {item.name}"
                    )
                if item.suffix != ".parquet":
                    raise ValueError(
                        f"æ ¡éªŒå¤±è´¥ï¼éåˆ†åŒºè¡¨ {source_path.name} çš„æ ¹ç›®å½•åªåº”åŒ…å« .parquet æ–‡ä»¶ï¼Œå‘ç°: {item.name}"
                    )

        if ignored_files_count > 0:
            print(f"  ğŸ“‹ å·²å¿½ç•¥ {ignored_files_count} ä¸ªç³»ç»Ÿæ–‡ä»¶")
        print("  âœ… ç›®å½•ç»“æ„æ ¡éªŒé€šè¿‡ã€‚")

    def _load_and_deduplicate_data(
        self,
        con: duckdb.DuckDBPyConnection,
        table_name: str,
        table_config: Dict[str, Any],
        is_partitioned: bool,
    ) -> ibis.Table:
        """åŠ è½½æ•°æ®å¹¶åº”ç”¨å»é‡ç­–ç•¥"""
        source_path = self.data_dir / table_name
        source_pattern = f"'{source_path}/**/*.parquet'"

        if is_partitioned:
            # åˆ†åŒºè¡¨ä½¿ç”¨ç›®å½•æ¨¡å¼
            df = con.execute(
                f"SELECT * FROM read_parquet({source_pattern}, hive_partitioning=1, union_by_name=true)"
            ).df()
        else:
            # éåˆ†åŒºè¡¨
            df = con.execute(f"SELECT * FROM read_parquet({source_pattern})").df()

        # è½¬æ¢ä¸ºibisè¡¨
        table = ibis.memtable(df)

        # åº”ç”¨å»é‡ç­–ç•¥
        deduplicated_table = self.deduplication_strategy.deduplicate(
            table, table_config
        )

        return deduplicated_table

    def _execute_optimization(
        self,
        con: duckdb.DuckDBPyConnection,
        table: ibis.Table,
        table_name: str,
        table_config: Dict[str, Any],
        target_path: Path,
        is_partitioned: bool,
    ):
        """æ‰§è¡Œä¼˜åŒ–æ“ä½œ"""
        # è·å–æ’åºåˆ—
        sort_columns = self._get_sort_columns(table_config)

        # æ¸…ç†ç›®æ ‡è·¯å¾„
        if target_path.exists():
            if target_path.is_dir():
                shutil.rmtree(target_path)
            else:
                target_path.unlink()

        # ä¸ºéåˆ†åŒºè¡¨åˆ›å»ºç›®å½•
        if not is_partitioned:
            target_path.mkdir(parents=True, exist_ok=True)

        # å°†å»é‡åçš„è¡¨ä¿å­˜ä¸ºä¸´æ—¶parquetæ–‡ä»¶
        temp_table_path = self.temp_dir / "temp_table.parquet"
        table.to_parquet(temp_table_path)

        # æ„å»ºSQLå¹¶æ‰§è¡Œ
        if is_partitioned:
            copy_statement = f"""
            COPY (
                SELECT * FROM read_parquet('{self.temp_dir}/temp_table.parquet')
                ORDER BY {sort_columns}
            )
            TO '{target_path}'
            (FORMAT PARQUET, PARTITION_BY (year), OVERWRITE_OR_IGNORE 1);
            """
        else:
            # éåˆ†åŒºè¡¨ï¼šç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            filename = self.filename_generator.generate_filename(table_name)
            target_file_path = target_path / filename
            copy_statement = f"""
            COPY (
                SELECT * FROM read_parquet('{self.temp_dir}/temp_table.parquet')
                ORDER BY {sort_columns}
            )
            TO '{target_file_path}'
            (FORMAT PARQUET, OVERWRITE_OR_IGNORE 1);
            """

        print("  å‡†å¤‡æ‰§è¡Œä¼˜åŒ– SQL...")
        print(copy_statement)

        con.execute(copy_statement)
        print(f"  âœ… è¡¨ {table_name} å·²æˆåŠŸä¼˜åŒ–åˆ°ä¸´æ—¶ç›®å½•: {target_path}")

    def _get_sort_columns(self, table_config: Dict[str, Any]) -> str:
        """æ ¹æ®è¡¨é…ç½®è·å–æ’åºåˆ—"""
        primary_key = table_config.get("primary_key", [])
        if not primary_key:
            raise ValueError(f"è¡¨ {table_config.get('table_name')} æ²¡æœ‰å®šä¹‰ä¸»é”®")
        return ", ".join(primary_key)

    def _validate_data_consistency(
        self,
        con: duckdb.DuckDBPyConnection,
        table_name: str,
        source_path: Path,
        target_path: Path,
        is_partitioned: bool,
    ) -> tuple[bool, bool]:
        """éªŒè¯æºæ•°æ®å’Œä¼˜åŒ–åæ•°æ®çš„ä¸€è‡´æ€§"""
        print("ã€€å¼€å§‹æ•°æ®ä¸€è‡´æ€§éªŒè¯...")

        try:
            if is_partitioned:
                # åˆ†åŒºè¡¨ä½¿ç”¨ç›®å½•æ¨¡å¼
                source_pattern = f"'{source_path}/**/*.parquet'"
                target_pattern = f"'{target_path}/**/*.parquet'"
                source_count = con.execute(
                    f"SELECT COUNT(*) FROM read_parquet({source_pattern}, hive_partitioning=1)"
                ).fetchone()[0]
                target_count = con.execute(
                    f"SELECT COUNT(*) FROM read_parquet({target_pattern}, hive_partitioning=1)"
                ).fetchone()[0]
            else:
                # éåˆ†åŒºè¡¨å¯èƒ½æ˜¯å•ä¸ªæ–‡ä»¶æˆ–ç›®å½•
                if target_path.is_file():
                    source_pattern = f"'{source_path}/**/*.parquet'"
                    target_pattern = f"'{target_path}'"
                    source_count = con.execute(
                        f"SELECT COUNT(*) FROM read_parquet({source_pattern})"
                    ).fetchone()[0]
                    target_count = con.execute(
                        f"SELECT COUNT(*) FROM read_parquet({target_pattern})"
                    ).fetchone()[0]
                else:
                    source_pattern = f"'{source_path}/**/*.parquet'"
                    target_pattern = f"'{target_path}/**/*.parquet'"
                    source_count = con.execute(
                        f"SELECT COUNT(*) FROM read_parquet({source_pattern})"
                    ).fetchone()[0]
                    target_count = con.execute(
                        f"SELECT COUNT(*) FROM read_parquet('{target_path}/**/*.parquet')"
                    ).fetchone()[0]

            print(f"ã€€ã€€è®°å½•æ•°éªŒè¯: æº={source_count:,}, ä¼˜åŒ–å={target_count:,}")

            if source_count != target_count:
                # æœ‰å»é‡ï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤
                timestamp = pd.Timestamp.now().strftime("%Y%m%d%H%M%S")
                import uuid
                unique_id = str(uuid.uuid4())[:8]
                backup_name = f"{source_path.name}.backup_{timestamp}_{unique_id}"
                
                # ä¸€æ¬¡æ€§ç¡®è®¤ï¼šå»é‡å’Œåˆ é™¤åŸæ•°æ®
                if not self._ask_user_combined_confirmation(
                    table_name, source_count, target_count, backup_name
                ):
                    return False, False  # ç”¨æˆ·æ‹’ç»ï¼Œä¸è¿›è¡Œä»»ä½•æ“ä½œ
                print("ã€€ã€€âœ… è®°å½•æ•°æ ¡éªŒé€šè¿‡ï¼ˆç”¨æˆ·ç¡®è®¤å»é‡ç»“æœï¼‰ã€‚")
                return True, False  # ä¸éœ€è¦é¢å¤–ç¡®è®¤ï¼Œç›´æ¥åˆ é™¤å¤‡ä»½
            else:
                # æ— å»é‡ï¼Œç›´æ¥ç»§ç»­ï¼Œä¸éœ€è¦ç¡®è®¤
                print("ã€€ã€€âœ… è®°å½•æ•°æ ¡éªŒé€šè¿‡ï¼Œæ— é‡å¤æ•°æ®ï¼Œç›´æ¥ç»§ç»­ä¼˜åŒ–ã€‚")
                return True, False  # æ— å»é‡ï¼Œç›´æ¥åˆ é™¤å¤‡ä»½

        except Exception as e:
            print(f"ã€€âŒ éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            if not self._ask_user_confirmation(
                "éªŒè¯å¤±è´¥ï¼Œæ˜¯å¦å¼ºè¡Œç»§ç»­ï¼Ÿ", default=False
            ):
                return False, False
            print("ã€€ã€€âœ… ç”¨æˆ·é€‰æ‹©å¼ºè¡Œç»§ç»­ã€‚")
            return True, False  # å¼ºè¡Œç»§ç»­ï¼Œç›´æ¥åˆ é™¤å¤‡ä»½

    def _ask_user_confirmation(self, message: str, default: bool = False) -> bool:
        """ç®€å•çš„ç”¨æˆ·ç¡®è®¤å‡½æ•°"""
        default_text = "[Y/n]" if default else "[y/N]"
        try:
            response = input(f"âš ï¸ {message} {default_text}: ").strip().lower()
            if not response:
                return default
            return response in ["y", "yes"]
        except (KeyboardInterrupt, EOFError):
            print("\nç”¨æˆ·ä¸­æ–­æ“ä½œã€‚")
            return False

    def _ask_user_combined_confirmation(
        self, table_name: str, source_count: int, target_count: int, backup_name: str
    ) -> bool:
        """åˆå¹¶ç¡®è®¤ï¼šå»é‡å’Œåˆ é™¤å¤‡ä»½çš„ä¸€æ¬¡æ€§ç¡®è®¤"""
        print("\n" + "="*60)
        print(f"  ğŸ“Š æ•°æ®ä¼˜åŒ–ç»“æœ - è¡¨: {table_name}")
        print("="*60)
        print(f"  åŸå§‹è®°å½•æ•°: {source_count:,}")
        print(f"  ä¼˜åŒ–åè®°å½•æ•°: {target_count:,}")
        removed_count = source_count - target_count
        if removed_count > 0:
            print(f"  ğŸ”„ å»é‡ç§»é™¤: {removed_count:,} æ¡è®°å½•")
            removal_percentage = (removed_count / source_count) * 100
            print(f"  ğŸ“Š å»é‡æ¯”ä¾‹: {removal_percentage:.2f}%")
        else:
            print("  âœ… æ— é‡å¤æ•°æ®")
        print("-" * 60)
        print("ã€€ğŸ“¦ å°†åˆ›å»ºå¤‡ä»½: " + backup_name)
        print("ã€€ğŸ—‘ï¸ ä¼˜åŒ–åå°†è‡ªåŠ¨åˆ é™¤å¤‡ä»½")
        print("="*60)
        
        try:
            response = input(
                "âš ï¸ æ˜¯å¦æ¥å—å»é‡ç»“æœå¹¶ç»§ç»­ï¼ˆåŒ…æ‹¬åˆ é™¤åŸæ•°æ®å¤‡ä»½ï¼‰ï¼Ÿ [y/N]: "
            ).strip().lower()
            
            if response in ["y", "yes"]:
                print("ã€€âœ… ç”¨æˆ·ç¡®è®¤ç»§ç»­ä¼˜åŒ–æ“ä½œ")
                return True
            else:
                print("ã€€âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œï¼Œä¿ç•™åŸæ•°æ®")
                return False
        except (KeyboardInterrupt, EOFError):
            print("\nç”¨æˆ·ä¸­æ–­æ“ä½œã€‚")
            return False

    def _replace_old_data(
        self, source_path: Path, target_path: Path, needs_confirmation: bool
    ):
        """æ›¿æ¢æ—§æ•°æ®"""
        # ä½¿ç”¨UUIDç¡®ä¿å¤‡ä»½ç›®å½•åå”¯ä¸€
        timestamp = pd.Timestamp.now().strftime("%Y%m%d%H%M%S")
        import uuid

        unique_id = str(uuid.uuid4())[:8]
        backup_path = source_path.with_suffix(f".backup_{timestamp}_{unique_id}")

        print(f"  æ­£åœ¨ç”¨ä¼˜åŒ–åçš„æ•°æ®æ›¿æ¢æ—§æ•°æ®...å¤‡ä»½è‡³: {backup_path}")
        source_path.rename(backup_path)
        target_path.rename(source_path)
        print("  âœ… æ•°æ®æ›¿æ¢æˆåŠŸã€‚")

        # æ¸…ç†å¤‡ä»½
        self._cleanup_backup(backup_path, needs_confirmation)

    def _cleanup_backup(self, backup_path: Path, needs_confirmation: bool):
        """æ¸…ç†å¤‡ä»½ç›®å½• - å·²ç»é€šè¿‡åˆå¹¶ç¡®è®¤ï¼Œç›´æ¥åˆ é™¤å¤‡ä»½"""
        # ç”±äºå·²ç»é€šè¿‡åˆå¹¶ç¡®è®¤ï¼Œä¸éœ€è¦é¢å¤–ç¡®è®¤ï¼Œç›´æ¥åˆ é™¤å¤‡ä»½
        print(f"ã€€âœ… æ ¹æ®ç”¨æˆ·ç¡®è®¤ï¼Œåˆ é™¤å¤‡ä»½ç›®å½•: {backup_path}")
        self._safe_remove_backup(backup_path)

    def _safe_remove_backup(self, backup_path: Path) -> bool:
        """å®‰å…¨åˆ é™¤å¤‡ä»½ç›®å½•ï¼Œå¤„ç†æƒé™é—®é¢˜"""
        try:
            if backup_path.exists():
                # å…ˆä¿®å¤æƒé™
                os.system(f"chmod -R u+w '{backup_path}'")
                shutil.rmtree(backup_path)
                print(f"  âœ… å¤‡ä»½å·²åˆ é™¤: {backup_path}")
                return True
        except Exception as e:
            print(f"  âŒ åˆ é™¤å¤‡ä»½å¤±è´¥ {backup_path}: {e}")
            return False
        return False
