"""Huey ä»»åŠ¡å®šä¹‰

å®šä¹‰å¸¦ @huey_task è£…é¥°å™¨çš„ä¸‹è½½ä»»åŠ¡å‡½æ•°ã€‚
ä½¿ç”¨ä¸­å¿ƒåŒ–çš„å®¹å™¨å®ä¾‹æ¥è·å–ä¾èµ–æœåŠ¡ã€‚
"""

import asyncio
import logging
from datetime import datetime, time, timedelta

import pandas as pd
from ..configs.huey_config import huey_fast, huey_slow
from ..task_bus.types import TaskType
from ..helpers.utils import get_next_day_str
from ..configs.app_config import get_config

logger = logging.getLogger(__name__)


@huey_slow.task()
def build_and_enqueue_downloads_task(group_name: str, stock_codes: list[str] = None):
    """æ„å»ºå¹¶æ´¾å‘å¢é‡ä¸‹è½½ä»»åŠ¡ (æ…¢é€Ÿé˜Ÿåˆ—)

    è¿™æ˜¯æ™ºèƒ½å¢é‡ä¸‹è½½çš„ç¬¬ä¸€æ­¥ã€‚
    å®ƒä¼šæŸ¥è¯¢æ•°æ®æ¹–ä¸­å·²æœ‰æ•°æ®çš„æœ€æ–°æ—¥æœŸï¼Œè®¡ç®—å‡ºéœ€è¦ä¸‹è½½çš„èµ·å§‹æ—¥æœŸï¼Œ
    ç„¶åå°†å…·ä½“çš„ä¸‹è½½ä»»åŠ¡æ´¾å‘åˆ°å¿«é€Ÿé˜Ÿåˆ—ã€‚

    Args:
        group_name: åœ¨ config.toml ä¸­å®šä¹‰çš„ä»»åŠ¡ç»„å
        stock_codes: å¯é€‰çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚æœæä¾›åˆ™åªå¤„ç†è¿™äº›è‚¡ç¥¨
    """
    from ..app import container
    from collections import defaultdict

    logger.info(f"ğŸ› ï¸ [HUEY_SLOW] å¼€å§‹æ„å»ºå¢é‡ä¸‹è½½ä»»åŠ¡, ä»»åŠ¡ç»„: {group_name}")

    group_handler = container.group_handler()
    db_operator = container.db_queryer()
    config = get_config()

    try:
        # 1. è·å–ä»»åŠ¡ç»„çš„ä»»åŠ¡ç±»å‹åˆ—è¡¨
        task_types = group_handler.get_task_types_for_group(group_name)
        if not task_types:
            logger.warning(f"ä»»åŠ¡ç»„ '{group_name}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ä»»åŠ¡ç±»å‹ï¼Œä»»åŠ¡ç»“æŸã€‚")
            return

        # 2. è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨
        if stock_codes:
            # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†è‚¡ç¥¨ä»£ç ï¼Œåˆ™ä½¿ç”¨æŒ‡å®šçš„ä»£ç 
            symbols = stock_codes
            logger.info(f"ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„è‚¡ç¥¨ä»£ç : {symbols}")
        else:
            # å¦åˆ™ä½¿ç”¨ä»»åŠ¡ç»„é…ç½®çš„è‚¡ç¥¨ä»£ç 
            symbols = group_handler.get_symbols_for_group(group_name)
            logger.info(
                f"ä½¿ç”¨ä»»åŠ¡ç»„ '{group_name}' é…ç½®çš„è‚¡ç¥¨ä»£ç : {len(symbols) if symbols else 0} ä¸ª"
            )

        # 3. åˆå§‹åŒ– ParquetDBQueryer ç”¨äºæŸ¥è¯¢æœ€æ–°æ—¥æœŸ
        from ..database.operator import ParquetDBQueryer
        from ..database.schema_loader import SchemaLoader

        schema_loader = SchemaLoader()
        parquet_op = ParquetDBQueryer(
            schema_loader=schema_loader,
            parquet_base_path=config.storage.parquet_base_path,
        )

        # 4. æ‰¹é‡æŸ¥è¯¢æ¯ä¸ª task_type ä¸‹æ‰€æœ‰ symbols çš„æœ€æ–°æ—¥æœŸ
        max_dates = {}
        for task_type in task_types:
            if symbols:  # æœ‰å…·ä½“è‚¡ç¥¨ä»£ç çš„ä»»åŠ¡ç±»å‹
                logger.debug(
                    f"æ­£åœ¨ä¸º {task_type} æŸ¥è¯¢ {len(symbols)} ä¸ªè‚¡ç¥¨çš„æœ€æ–°æ—¥æœŸ..."
                )
                # get_max_dateè¿”å›çš„æ˜¯ {symbol: date} æ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸º {(symbol, task_type): date} æ ¼å¼
                task_max_dates = parquet_op.get_max_date(task_type, symbols)
                for symbol, date in task_max_dates.items():
                    max_dates[(symbol, task_type)] = date
            else:  # stock_basic ç­‰ä¸éœ€è¦å…·ä½“è‚¡ç¥¨ä»£ç çš„ä»»åŠ¡ç±»å‹
                logger.debug(f"ä»»åŠ¡ç±»å‹ {task_type} ä¸éœ€è¦å…·ä½“è‚¡ç¥¨ä»£ç ")
                max_dates[("", task_type)] = None  # ä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸º symbol

        # 5. å¾ªç¯æ´¾å‘å…·ä½“çš„ä¸‹è½½ä»»åŠ¡
        default_start_date = config.download_tasks.default_start_date
        enqueued_count = 0

        for task_type in task_types:
            if symbols:  # æœ‰å…·ä½“è‚¡ç¥¨ä»£ç çš„ä»»åŠ¡ç±»å‹
                for symbol in symbols:
                    latest_date = max_dates.get((symbol, task_type))

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡ä»Šå¤©çš„æ•°æ®ä¸‹è½½
                    if latest_date:
                        # latest_date æ ¼å¼æ˜¯ YYYYMMDD (å¦‚ 20250826)ï¼Œéœ€è¦è½¬æ¢ä¸º YYYY-MM-DD æ ¼å¼è¿›è¡Œæ¯”è¾ƒ
                        today_str = datetime.now().strftime("%Y%m%d")
                        yesterday_str = (datetime.now() - timedelta(days=1)).strftime(
                            "%Y%m%d"
                        )
                        current_time = datetime.now().time()
                        market_close_time = time(18, 0)  # ä¸‹åˆ6ç‚¹

                        logger.debug(
                            f"è·³è¿‡é€»è¾‘æ£€æŸ¥ {symbol}-{task_type}: latest_date={latest_date}, today={today_str}, yesterday={yesterday_str}, current_time={current_time.strftime('%H:%M')}"
                        )

                        # å¦‚æœæœ€å¤§æ—¥æœŸæ˜¯ä»Šå¤©ä¸”å½“å‰æ—¶é—´æœªåˆ°ä¸‹åˆ6ç‚¹ï¼Œè·³è¿‡è¯¥ä»»åŠ¡
                        if (
                            latest_date == today_str
                            and current_time < market_close_time
                        ):
                            logger.info(
                                f"â­ï¸ è·³è¿‡ {symbol} çš„ {task_type} ä»»åŠ¡ï¼šä»Šæ—¥æ•°æ®å°šæœªæ”¶ç›˜ï¼Œå½“å‰æ—¶é—´ {current_time.strftime('%H:%M')}"
                            )
                            continue

                        # å¦‚æœæœ€å¤§æ—¥æœŸæ˜¯æ˜¨å¤©ä¸”å½“å‰æ—¶é—´æœªåˆ°ä¸‹åˆ6ç‚¹ï¼Œä¹Ÿè·³è¿‡è¯¥ä»»åŠ¡ï¼ˆä»Šæ—¥æ•°æ®å°šæœªäº§ç”Ÿï¼‰
                        if (
                            latest_date == yesterday_str
                            and current_time < market_close_time
                        ):
                            logger.info(
                                f"â­ï¸ è·³è¿‡ {symbol} çš„ {task_type} ä»»åŠ¡ï¼šæœ€æ–°æ•°æ®ä¸ºæ˜¨æ—¥ï¼Œä»Šæ—¥æ•°æ®å°šæœªæ”¶ç›˜ï¼Œå½“å‰æ—¶é—´ {current_time.strftime('%H:%M')}"
                            )
                            continue

                    start_date = (
                        get_next_day_str(latest_date)
                        if latest_date
                        else default_start_date
                    )

                    # æ´¾å‘ä»»åŠ¡åˆ°å¿«é€Ÿé˜Ÿåˆ—
                    download_task(
                        task_type=task_type, symbol=symbol, start_date=start_date
                    )
                    enqueued_count += 1
            else:  # stock_basic ç­‰ä¸éœ€è¦å…·ä½“è‚¡ç¥¨ä»£ç çš„ä»»åŠ¡ç±»å‹
                # æ´¾å‘ä»»åŠ¡åˆ°å¿«é€Ÿé˜Ÿåˆ—
                download_task(
                    task_type=task_type, symbol="", start_date=default_start_date
                )
                enqueued_count += 1

        logger.info(f"âœ… [HUEY_SLOW] æˆåŠŸæ´¾å‘ {enqueued_count} ä¸ªå¢é‡ä¸‹è½½ä»»åŠ¡ã€‚")

    except Exception as e:
        logger.error(f"âŒ [HUEY_SLOW] æ„å»ºä¸‹è½½ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        raise e


def _process_data_sync(task_type: str, data: pd.DataFrame) -> bool:
    """å¼‚æ­¥å¤„ç†æ•°æ®çš„å…¬å…±å‡½æ•°

    Args:
        task_type: ä»»åŠ¡ç±»å‹å­—ç¬¦ä¸²
        data: è¦å¤„ç†çš„æ•°æ®

    Returns:
        bool: å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    from ..app import container

    data_processor = container.data_processor()
    try:
        process_success = data_processor.process(task_type, data)
        logger.debug(f"[HUEY] {task_type} æ•°æ®å¤„ç†å™¨è¿”å›ç»“æœ: {process_success}")
        return process_success
    finally:
        # ç¡®ä¿æ•°æ®å¤„ç†å™¨æ­£ç¡®å…³é—­ï¼Œåˆ·æ–°æ‰€æœ‰ç¼“å†²åŒºæ•°æ®
        data_processor.shutdown()


@huey_fast.task(retries=2, retry_delay=60)
def download_task(task_type: TaskType, symbol: str, **kwargs):
    """ä¸‹è½½è‚¡ç¥¨æ•°æ®çš„ Huey ä»»åŠ¡ (å¿«é€Ÿé˜Ÿåˆ—)

    ä¸‹è½½å®Œæˆåï¼Œç›´æ¥è°ƒç”¨æ…¢é€Ÿé˜Ÿåˆ—çš„æ•°æ®å¤„ç†ä»»åŠ¡ã€‚

    Args:
        task_type: ä»»åŠ¡ç±»å‹æšä¸¾
        symbol: è‚¡ç¥¨ä»£ç 
        **kwargs: é¢å¤–çš„ä¸‹è½½å‚æ•°ï¼Œå¦‚ start_date, end_date
    """
    try:
        logger.debug(f"ğŸš€ [HUEY_FAST] å¼€å§‹æ‰§è¡Œä¸‹è½½ä»»åŠ¡: {symbol} ({task_type})")

        # ä»ä¸­å¿ƒåŒ–çš„ app.py è·å–å…±äº«çš„å®¹å™¨å®ä¾‹
        from ..app import container

        downloader = container.downloader()

        result = downloader.download(task_type, symbol, **kwargs)

        if result is not None and not result.empty:
            logger.debug(f"ğŸš€ [HUEY_FAST] ä¸‹è½½å®Œæˆ: {symbol}, å‡†å¤‡æäº¤åˆ°æ…¢é€Ÿé˜Ÿåˆ—...")
            # æ‰‹åŠ¨è°ƒç”¨æ…¢é€Ÿä»»åŠ¡ï¼Œå¹¶ä¼ é€’æ•°æ®
            process_data_task(
                task_type=task_type,
                symbol=symbol,
                data_frame=result.to_dict("records"),
            )
        else:
            logger.warning(
                f"âš ï¸ [HUEY_FAST] ä¸‹è½½ä»»åŠ¡å®Œæˆ: {symbol}, ä½†è¿”å›ç©ºæ•°æ®ï¼Œä¸æäº¤åç»­ä»»åŠ¡"
            )

    except Exception as e:
        logger.error(f"âŒ [HUEY_FAST] ä¸‹è½½ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {symbol}, é”™è¯¯: {e}")
        raise e


@huey_slow.task()
def process_data_task(task_type: str, symbol: str, data_frame: list) -> bool:
    """æ•°æ®å¤„ç†ä»»åŠ¡ (æ…¢é€Ÿé˜Ÿåˆ—)

    Args:
        task_type: ä»»åŠ¡ç±»å‹å­—ç¬¦ä¸²
        symbol: è‚¡ç¥¨ä»£ç 
        data_frame: DataFrame æ•°æ® (å­—å…¸åˆ—è¡¨å½¢å¼)

    Returns:
        bool: å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    try:
        # åˆ›å»ºå¼‚æ­¥æ•°æ®å¤„ç†å™¨å¹¶è¿è¡Œ
        def process_sync():
            try:
                # å°†å­—å…¸åˆ—è¡¨è½¬æ¢ä¸º DataFrame
                if data_frame and isinstance(data_frame, list) and len(data_frame) > 0:
                    df_data = pd.DataFrame(data_frame)
                    logger.debug(
                        f"ğŸŒ [HUEY_SLOW] å¼€å§‹å¼‚æ­¥ä¿å­˜æ•°æ®: {symbol}_{task_type}, æ•°æ®è¡Œæ•°: {len(df_data)}"
                    )
                    return _process_data_sync(task_type, df_data)
                else:
                    logger.warning(
                        f"âš ï¸ [HUEY_SLOW] æ•°æ®ä¿å­˜å¤±è´¥ï¼Œæ— æœ‰æ•ˆæ•°æ®: {symbol}_{task_type}, æ•°æ®ä¸ºç©ºæˆ–None"
                    )
                    return False
            except Exception as e:
                raise e

        result = process_sync()
        logger.info(f"ğŸ† [HUEY_SLOW] æœ€ç»ˆç»“æœ: {symbol}_{task_type}, æˆåŠŸ: {result}")
        return result

    except Exception as e:
        logger.error(f"âŒ [HUEY_SLOW] æ•°æ®å¤„ç†ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {symbol}, é”™è¯¯: {e}")
        raise e


# ==========================================================
# å…ƒæ•°æ®åŒæ­¥ä»»åŠ¡ (ç»´æŠ¤é˜Ÿåˆ—)
# ==========================================================
import duckdb
from pathlib import Path
from huey import crontab
from ..configs.huey_config import huey_maint
from ..configs import get_config

config = get_config()


def get_sync_metadata_crontab():
    """ä»é…ç½®ä¸­è¯»å– cron è¡¨è¾¾å¼"""
    schedule = config.cron_tasks.sync_metadata_schedule
    minute, hour, day, month, day_of_week = schedule.split()
    return crontab(minute, hour, day, month, day_of_week)


@huey_maint.periodic_task(get_sync_metadata_crontab(), name="sync_metadata")
def sync_metadata():
    """
    å‘¨æœŸæ€§ä»»åŠ¡ï¼šæ‰«æ Parquet æ–‡ä»¶ç›®å½•ï¼Œå¹¶æ›´æ–° DuckDB å…ƒæ•°æ®æ–‡ä»¶ã€‚
    """
    logger.info("ğŸ› ï¸ [HUEY_MAINT] å¼€å§‹æ‰§è¡Œå…ƒæ•°æ®åŒæ­¥ä»»åŠ¡...")

    # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„ï¼Œå¹¶æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
    # neo/tasks/huey_tasks.py -> neo/tasks -> neo -> src -> project_root
    project_root = Path(__file__).resolve().parents[3]
    logger.info(f"è¯Šæ–­: é¡¹ç›®æ ¹ç›®å½•: {project_root}")

    parquet_base_path = project_root / config.storage.parquet_base_path
    metadata_db_path = project_root / config.database.metadata_path
    logger.info(f"è¯Šæ–­: Parquet æ ¹ç›®å½•: {parquet_base_path}")
    logger.info(f"è¯Šæ–­: å…ƒæ•°æ®DBè·¯å¾„: {metadata_db_path}")

    if not parquet_base_path.is_dir():
        logger.warning(f"Parquet æ ¹ç›®å½• {parquet_base_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡åŒæ­¥ã€‚")
        return

    try:
        with duckdb.connect(str(metadata_db_path)) as con:
            logger.info("è¯Šæ–­: æˆåŠŸè¿æ¥åˆ°å…ƒæ•°æ®DBã€‚")
            # æ‰«æ Parquet æ ¹ç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•ï¼Œæ¯ä¸ªå­ç›®å½•ä»£è¡¨ä¸€ä¸ªè¡¨

            found_items = list(parquet_base_path.iterdir())
            if not found_items:
                logger.warning(f"è­¦å‘Š: åœ¨ {parquet_base_path} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¡ç›®ã€‚")
                return

            logger.info(
                f"è¯Šæ–­: åœ¨ {parquet_base_path} ä¸­æ‰¾åˆ°ä»¥ä¸‹æ¡ç›®: {[p.name for p in found_items]}"
            )

            for table_dir in found_items:
                if table_dir.is_dir():
                    table_name = table_dir.name
                    # DuckDB çš„ hive_partitioning ä¼šè‡ªåŠ¨å¤„ç†å­ç›®å½•ï¼Œæˆ‘ä»¬åªéœ€æä¾›æ ¹è·¯å¾„
                    # ä¿®æ­£ï¼šä¸ºå¢å¼ºå…¼å®¹æ€§ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ªæ›´æ˜ç¡®çš„ glob è·¯å¾„
                    table_glob_path = str(table_dir / "**/*.parquet")

                    logger.info(
                        f"æ­£åœ¨ä¸ºè¡¨ {table_name} ä»è·¯å¾„ {table_glob_path} åŒæ­¥å…ƒæ•°æ®..."
                    )

                    sql = f"""
                    CREATE OR REPLACE TABLE {table_name} AS
                    SELECT * FROM read_parquet('{table_glob_path}', hive_partitioning=1, union_by_name=True);
                    """
                    con.execute(sql)
                    logger.info(f"âœ… è¡¨ {table_name} å…ƒæ•°æ®åŒæ­¥å®Œæˆã€‚")
                else:
                    logger.info(f"è¯Šæ–­: è·³è¿‡éç›®å½•æ¡ç›®: {table_dir}")

        logger.info("ğŸ› ï¸ [HUEY_MAINT] å…ƒæ•°æ®åŒæ­¥ä»»åŠ¡æˆåŠŸå®Œæˆã€‚")
    except Exception as e:
        logger.error(f"âŒ [HUEY_MAINT] å…ƒæ•°æ®åŒæ­¥ä»»åŠ¡å¤±è´¥: {e}")
        raise e
