"""Hueyæ€§èƒ½æµ‹è¯•è„šæœ¬"""
import time
import statistics
import json
import os
import threading
from typing import Dict, List, Any
from dataclasses import dataclass, asdict
from huey.consumer import Consumer

from huey_config import HueyPerformanceConfig, default_config, huey
from sync_tasks import SyncTaskManager
from async_tasks import AsyncTaskManager


@dataclass
class TestMetrics:
    """æµ‹è¯•æŒ‡æ ‡"""
    total_time: float
    avg_task_time: float
    min_task_time: float
    max_task_time: float
    throughput: float  # æ¯ç§’å¤„ç†çš„è‚¡ç¥¨æ•°
    success_rate: float
    total_data_points: int
    failed_tasks: int
    buffer_stats: Dict[str, int]


@dataclass
class HueyComparisonResult:
    """Hueyå¯¹æ¯”ç»“æœ"""
    sync_metrics: TestMetrics
    async_metrics: TestMetrics
    performance_improvement: Dict[str, float]
    test_config: Dict[str, Any]
    huey_stats: Dict[str, Any]


class HueyPerformanceTestRunner:
    """Hueyæ€§èƒ½æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config: HueyPerformanceConfig = None):
        self.config = config or default_config
        self.sync_manager = SyncTaskManager()
        self.async_manager = AsyncTaskManager()
        self.consumer = None
        self.consumer_thread = None
    
    def _calculate_metrics(self, results: List[Any], total_time: float, buffer_stats: Dict[str, int]) -> TestMetrics:
        """è®¡ç®—æµ‹è¯•æŒ‡æ ‡"""
        if not results:
            return TestMetrics(
                total_time=total_time,
                avg_task_time=0.0,
                min_task_time=0.0,
                max_task_time=0.0,
                throughput=0.0,
                success_rate=0.0,
                total_data_points=0,
                failed_tasks=0,
                buffer_stats=buffer_stats
            )
        
        # å±•å¹³ç»“æœåˆ—è¡¨ï¼ˆå¤„ç†æ‰¹æ¬¡ä»»åŠ¡ï¼‰
        flat_results = []
        for result in results:
            if isinstance(result, list):
                flat_results.extend(result)
            else:
                flat_results.append(result)
        
        # è§£æå­—ç¬¦ä¸²ç»“æœ: "mode:symbol:data_count:processing_time[:ERROR:error_msg]"
        task_times = []
        successful_tasks = 0
        failed_tasks = 0
        total_data_points = 0
        
        for result_str in flat_results:
            if isinstance(result_str, str):
                parts = result_str.split(':')
                if len(parts) >= 4:
                    try:
                        data_count = int(parts[2])
                        processing_time = float(parts[3])
                        task_times.append(processing_time)
                        total_data_points += data_count
                        
                        if len(parts) > 4 and parts[4] == 'ERROR':
                            failed_tasks += 1
                        else:
                            successful_tasks += 1
                    except (ValueError, IndexError):
                        failed_tasks += 1
                else:
                    failed_tasks += 1
            else:
                failed_tasks += 1
        
        total_tasks = successful_tasks + failed_tasks
        
        return TestMetrics(
            total_time=total_time,
            avg_task_time=statistics.mean(task_times) if task_times else 0.0,
            min_task_time=min(task_times) if task_times else 0.0,
            max_task_time=max(task_times) if task_times else 0.0,
            throughput=total_data_points / total_time if total_time > 0 else 0.0,
            success_rate=(successful_tasks / total_tasks) * 100 if total_tasks > 0 else 0.0,
            total_data_points=total_data_points,
            failed_tasks=failed_tasks,
            buffer_stats=buffer_stats
        )
    
    def start_consumer(self):
        """å¯åŠ¨Hueyæ¶ˆè´¹è€…"""
        if self.consumer is None:
            # å¯åŠ¨å¤šçº¿ç¨‹Consumerï¼Œæ”¯æŒçœŸæ­£çš„å¹¶å‘æ‰§è¡Œ
            self.consumer = Consumer(huey, workers=self.config.worker_count, worker_type="thread")
            self.consumer_thread = threading.Thread(target=self.consumer.run, daemon=True)
            self.consumer_thread.start()
            print(f"ğŸš€ Hueyæ¶ˆè´¹è€…å·²å¯åŠ¨ ({self.config.worker_count}ä¸ªå·¥ä½œçº¿ç¨‹)")
            time.sleep(1)  # ç­‰å¾…æ¶ˆè´¹è€…å¯åŠ¨
    
    def stop_consumer(self):
        """åœæ­¢Hueyæ¶ˆè´¹è€…"""
        if self.consumer:
            self.consumer.stop()
            if self.consumer_thread:
                self.consumer_thread.join(timeout=5)
            self.consumer = None
            self.consumer_thread = None
            print("Hueyæ¶ˆè´¹è€…å·²åœæ­¢")
    
    def test_sync_tasks(self) -> TestMetrics:
        """æµ‹è¯•åŒæ­¥ä»»åŠ¡"""
        print("å¼€å§‹HueyåŒæ­¥ä»»åŠ¡æµ‹è¯•...")
        
        # å¯åŠ¨æ¶ˆè´¹è€…
        self.start_consumer()
        
        symbols = [f"STOCK_{i:03d}" for i in range(self.config.stock_count)]
        
        start_time = time.time()
        
        # æäº¤ä»»åŠ¡
        tasks = self.sync_manager.submit_tasks(
            symbols, 
            self.config.data_points_per_stock, 
            self.config.batch_size
        )
        
        print(f"å·²æäº¤ {len(tasks)} ä¸ªæ‰¹æ¬¡ä»»åŠ¡")
        
        # ç­‰å¾…ç»“æœ
        results = self.sync_manager.wait_for_results(tasks)
        
        total_time = time.time() - start_time
        buffer_stats = self.sync_manager.get_buffer_stats()
        
        metrics = self._calculate_metrics(results, total_time, buffer_stats)
        
        print(f"åŒæ­¥ä»»åŠ¡å®Œæˆ: {len(results)}ä¸ªä»»åŠ¡, è€—æ—¶: {total_time:.2f}ç§’")
        print(f"æˆåŠŸç‡: {metrics.success_rate:.2%}, ååé‡: {metrics.throughput:.2f}ä»»åŠ¡/ç§’")
        print(f"ç¼“å†²å™¨ç»Ÿè®¡: {buffer_stats}")
        
        return metrics
    
    def test_async_tasks(self) -> TestMetrics:
        """æµ‹è¯•å¼‚æ­¥ä»»åŠ¡æ€§èƒ½"""
        print("\nå¼€å§‹å¼‚æ­¥ä»»åŠ¡æµ‹è¯•...")
        
        # å¯åŠ¨æ¶ˆè´¹è€…
        self.start_consumer()
        
        # ç”Ÿæˆæµ‹è¯•ç”¨è‚¡ç¥¨ç¬¦å·
        symbols = [f"STOCK_{i:03d}" for i in range(self.config.stock_count)]
        
        # é¢„çƒ­
        print(f"é¢„çƒ­è¿è¡Œ {self.config.warmup_iterations} æ¬¡...")
        for i in range(self.config.warmup_iterations):
            warmup_tasks = self.async_manager.submit_tasks(
                symbols[:5], 
                self.config.data_points_per_stock, 
                batch_size=2
            )
            self.async_manager.wait_for_results(warmup_tasks, timeout=30.0)
        
        # æ­£å¼æµ‹è¯•
        print(f"æ­£å¼æµ‹è¯•è¿è¡Œ {self.config.test_iterations} æ¬¡...")
        all_results = []
        total_start_time = time.time()
        
        for iteration in range(self.config.test_iterations):
            print(f"  ç¬¬ {iteration + 1}/{self.config.test_iterations} æ¬¡è¿­ä»£")
            
            # æäº¤ä»»åŠ¡
            tasks = self.async_manager.submit_tasks(
                symbols, 
                self.config.data_points_per_stock,
                batch_size=self.config.batch_size
            )
            
            # ç­‰å¾…ç»“æœ
            results = self.async_manager.wait_for_results(tasks, timeout=300.0)
            all_results.extend(results)
        
        total_end_time = time.time()
        total_time = total_end_time - total_start_time
        
        # è·å–ç¼“å†²å™¨ç»Ÿè®¡
        buffer_stats = self.async_manager.get_buffer_stats()
        
        # åœæ­¢æ¶ˆè´¹è€…
        self.stop_consumer()
        
        return self._calculate_metrics(all_results, total_time, buffer_stats)
    
    def _calculate_performance_improvement(self, sync_metrics: TestMetrics, async_metrics: TestMetrics) -> Dict[str, float]:
        """è®¡ç®—æ€§èƒ½æå‡"""
        improvements = {}
        
        if sync_metrics.total_time > 0:
            improvements['total_time'] = (sync_metrics.total_time - async_metrics.total_time) / sync_metrics.total_time * 100
        
        if sync_metrics.throughput > 0:
            improvements['throughput'] = (async_metrics.throughput - sync_metrics.throughput) / sync_metrics.throughput * 100
        
        if sync_metrics.avg_task_time > 0:
            improvements['avg_task_time'] = (sync_metrics.avg_task_time - async_metrics.avg_task_time) / sync_metrics.avg_task_time * 100
        
        return improvements
    
    def _get_huey_stats(self) -> Dict[str, Any]:
        """è·å–Hueyç»Ÿè®¡ä¿¡æ¯"""
        return {
            'huey_name': huey.name,
            'storage_type': type(huey.storage).__name__,
            'immediate_mode': getattr(huey, 'immediate', False),
            'pending_count': len(huey.pending()),
            'scheduled_count': len(huey.scheduled())
        }
    
    def run_comparison_test(self) -> HueyComparisonResult:
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        print(f"å¼€å§‹Hueyæ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
        print(f"æµ‹è¯•é…ç½®: {self.config.stock_count}åªè‚¡ç¥¨, æ¯åª{self.config.data_points_per_stock}ä¸ªæ•°æ®ç‚¹")
        print(f"æ‰¹æ¬¡å¤§å°: {self.config.batch_size}, æµ‹è¯•è¿­ä»£: {self.config.test_iterations}")
        print(f"Hueyé…ç½®: {type(huey.storage).__name__}")
        print("-" * 60)
        
        # æ‰§è¡Œå¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
        sync_metrics_list = []
        async_metrics_list = []
        
        for i in range(self.config.test_iterations):
            print(f"æ‰§è¡Œç¬¬ {i+1}/{self.config.test_iterations} è½®æµ‹è¯•")
            
            # æ¸…ç†Hueyé˜Ÿåˆ—
            huey.flush()
            
            # æµ‹è¯•åŒæ­¥ä»»åŠ¡
            sync_metrics = self.test_sync_tasks()
            sync_metrics_list.append(sync_metrics)
            
            # åœæ­¢æ¶ˆè´¹è€…
            self.stop_consumer()
            
            # ç­‰å¾…ä¸€ä¸‹å†æµ‹è¯•å¼‚æ­¥ä»»åŠ¡
            time.sleep(2)
            
            # æ¸…ç†Hueyé˜Ÿåˆ—
            huey.flush()
            
            # æµ‹è¯•å¼‚æ­¥ä»»åŠ¡
            async_metrics = self.test_async_tasks()
            async_metrics_list.append(async_metrics)
            
            # åœæ­¢æ¶ˆè´¹è€…
            self.stop_consumer()
            
            print(f"ç¬¬ {i+1} è½®å®Œæˆ")
            print("-" * 40)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        def average_metrics(metrics_list: List[TestMetrics]) -> TestMetrics:
            return TestMetrics(
                total_time=statistics.mean([m.total_time for m in metrics_list]),
                avg_task_time=statistics.mean([m.avg_task_time for m in metrics_list]),
                min_task_time=statistics.mean([m.min_task_time for m in metrics_list]),
                max_task_time=statistics.mean([m.max_task_time for m in metrics_list]),
                throughput=statistics.mean([m.throughput for m in metrics_list]),
                success_rate=statistics.mean([m.success_rate for m in metrics_list]),
                total_data_points=int(statistics.mean([m.total_data_points for m in metrics_list])),
                failed_tasks=int(statistics.mean([m.failed_tasks for m in metrics_list])),
                buffer_stats=metrics_list[-1].buffer_stats  # ä½¿ç”¨æœ€åä¸€æ¬¡çš„ç¼“å†²å™¨ç»Ÿè®¡
            )
        
        avg_sync_metrics = average_metrics(sync_metrics_list)
        avg_async_metrics = average_metrics(async_metrics_list)
        
        # è®¡ç®—æ€§èƒ½æå‡
        improvements = self._calculate_performance_improvement(avg_sync_metrics, avg_async_metrics)
        
        # è·å–Hueyç»Ÿè®¡ä¿¡æ¯
        huey_stats = self._get_huey_stats()
        
        return HueyComparisonResult(
            sync_metrics=avg_sync_metrics,
            async_metrics=avg_async_metrics,
            performance_improvement=improvements,
            test_config=asdict(self.config),
            huey_stats=huey_stats
        )
    
    def save_results(self, result: HueyComparisonResult, filename: str = None) -> str:
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if not filename:
            timestamp = int(time.time())
            filename = f"huey_performance_test_{timestamp}.json"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.config.output_dir, exist_ok=True)
        filepath = os.path.join(self.config.output_dir, filename)
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        result_dict = asdict(result)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result_dict, f, indent=2, ensure_ascii=False)
        
        return filepath
    
    def print_results(self, result: HueyComparisonResult) -> None:
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print("\n" + "=" * 80)
        print("Hueyæ€§èƒ½æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 80)
        
        print(f"\nğŸ”§ Hueyé…ç½®:")
        for key, value in result.huey_stats.items():
            print(f"  {key}: {value}")
        
        print("\nğŸ“Š åŒæ­¥ä»»åŠ¡æŒ‡æ ‡:")
        print(f"  æ€»è€—æ—¶: {result.sync_metrics.total_time:.2f}ç§’")
        print(f"  å¹³å‡ä»»åŠ¡æ—¶é—´: {result.sync_metrics.avg_task_time:.4f}ç§’")
        print(f"  ååé‡: {result.sync_metrics.throughput:.2f}ä»»åŠ¡/ç§’")
        print(f"  æˆåŠŸç‡: {result.sync_metrics.success_rate:.2%}")
        print(f"  å¤„ç†æ•°æ®ç‚¹: {result.sync_metrics.total_data_points:,}")
        print(f"  ç¼“å†²å™¨ç»Ÿè®¡: {result.sync_metrics.buffer_stats}")
        
        print("\nğŸš€ å¼‚æ­¥ä»»åŠ¡æŒ‡æ ‡:")
        print(f"  æ€»è€—æ—¶: {result.async_metrics.total_time:.2f}ç§’")
        print(f"  å¹³å‡ä»»åŠ¡æ—¶é—´: {result.async_metrics.avg_task_time:.4f}ç§’")
        print(f"  ååé‡: {result.async_metrics.throughput:.2f}ä»»åŠ¡/ç§’")
        print(f"  æˆåŠŸç‡: {result.async_metrics.success_rate:.2%}")
        print(f"  å¤„ç†æ•°æ®ç‚¹: {result.async_metrics.total_data_points:,}")
        print(f"  ç¼“å†²å™¨ç»Ÿè®¡: {result.async_metrics.buffer_stats}")
        
        print("\nğŸ“ˆ æ€§èƒ½æå‡:")
        for metric, improvement in result.performance_improvement.items():
            symbol = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰"
            print(f"  {symbol} {metric}: {improvement:+.1f}%")
        
        print("\n" + "=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æµ‹è¯•å‚æ•°
    config = HueyPerformanceConfig(
        stock_count=30,  # å‡å°‘è‚¡ç¥¨æ•°é‡ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
        data_points_per_stock=50,
        batch_size=5,
        test_iterations=2,
        worker_count=4    # ä½¿ç”¨4ä¸ªå·¥ä½œçº¿ç¨‹
    )
    
    runner = HueyPerformanceTestRunner(config)
    
    try:
        result = runner.run_comparison_test()
        runner.print_results(result)
        
        # ä¿å­˜ç»“æœ
        filepath = runner.save_results(result)
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        runner.stop_consumer()
    except Exception as e:
        print(f"\næµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        runner.stop_consumer()
    finally:
        # ç¡®ä¿æ¶ˆè´¹è€…è¢«åœæ­¢
        runner.stop_consumer()


if __name__ == "__main__":
    main()