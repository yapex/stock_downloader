#!/usr/bin/env python3
"""
ConsumerPool ä½¿ç”¨ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ¶ˆè´¹è€…ç®¡ç†å™¨ï¼ˆConsumerPoolï¼‰ï¼š
- é…ç½®æ¶ˆè´¹è€…æ± å‚æ•°
- æäº¤æ•°æ®æ‰¹æ¬¡
- ç›‘æ§å¤„ç†è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯
- ä¼˜é›…åœæ­¢å’Œèµ„æºæ¸…ç†
"""

import time
import pandas as pd
from pathlib import Path
from queue import Queue

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.downloader.consumer_pool import ConsumerPool
from src.downloader.models import DataBatch


def create_sample_data_batches():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®æ‰¹æ¬¡"""
    batches = []
    
    # è‚¡ç¥¨åˆ—è¡¨æ•°æ®
    stock_list_df = pd.DataFrame({
        'ts_code': ['000001.SZ', '000002.SZ', '600000.SH'],
        'symbol': ['000001', '000002', '600000'],
        'name': ['å¹³å®‰é“¶è¡Œ', 'ä¸‡ç§‘A', 'æµ¦å‘é“¶è¡Œ'],
        'area': ['æ·±åœ³', 'æ·±åœ³', 'ä¸Šæµ·'],
        'industry': ['é“¶è¡Œ', 'æˆ¿åœ°äº§', 'é“¶è¡Œ'],
        'market': ['ä¸»æ¿', 'ä¸»æ¿', 'ä¸»æ¿'],
        'list_date': ['19910403', '19910129', '19991110']
    })
    
    stock_list_batch = DataBatch(
        df=stock_list_df,
        meta={'task_type': 'stock_list'},
        task_id='stock_list_001',
        symbol='system'
    )
    batches.append(stock_list_batch)
    
    # æ—¥çº¿æ•°æ®
    for i, stock_code in enumerate(['000001.SZ', '000002.SZ', '600000.SH']):
        daily_df = pd.DataFrame({
            'ts_code': [stock_code] * 5,
            'trade_date': ['20241201', '20241202', '20241203', '20241204', '20241205'],
            'open': [10.0 + i, 10.1 + i, 9.9 + i, 10.2 + i, 10.0 + i],
            'high': [10.5 + i, 10.6 + i, 10.4 + i, 10.7 + i, 10.5 + i],
            'low': [9.8 + i, 9.9 + i, 9.7 + i, 10.0 + i, 9.8 + i],
            'close': [10.2 + i, 10.0 + i, 10.1 + i, 10.3 + i, 10.1 + i],
            'volume': [1000000, 1100000, 950000, 1200000, 1050000],
            'amount': [10200000, 11000000, 9595000, 12360000, 10605000]
        })
        
        daily_batch = DataBatch(
            df=daily_df,
            meta={'task_type': 'daily'},
            task_id=f'daily_{stock_code}_{i}',
            symbol=stock_code
        )
        batches.append(daily_batch)
    
    # åŸºç¡€æ•°æ®
    for stock_code in ['000001.SZ', '000002.SZ']:
        basic_df = pd.DataFrame({
            'ts_code': [stock_code] * 3,
            'trade_date': ['20241203', '20241204', '20241205'],
            'close': [10.1, 10.3, 10.1],
            'turnover_rate': [2.5, 3.1, 2.8],
            'volume_ratio': [1.2, 1.5, 1.1],
            'pe': [12.5, 12.8, 12.3],
            'pb': [1.8, 1.85, 1.82],
            'ps': [2.1, 2.15, 2.08],
            'dv_ratio': [3.2, 3.15, 3.18],
            'dv_ttm': [3.5, 3.48, 3.52],
            'total_share': [1000000000, 1000000000, 1000000000],
            'float_share': [800000000, 800000000, 800000000],
            'free_share': [750000000, 750000000, 750000000],
            'total_mv': [10100000000, 10300000000, 10100000000],
            'circ_mv': [8080000000, 8240000000, 8080000000]
        })
        
        basic_batch = DataBatch(
            df=basic_df,
            meta={'task_type': 'daily_basic'},
            task_id=f'basic_{stock_code}',
            symbol=stock_code
        )
        batches.append(basic_batch)
    
    return batches


def monitor_progress(pool: ConsumerPool, duration: int = 30):
    """ç›‘æ§å¤„ç†è¿›åº¦"""
    print("\\nğŸ”„ å¼€å§‹ç›‘æ§å¤„ç†è¿›åº¦...")
    
    start_time = time.time()
    last_stats = None
    
    while time.time() - start_time < duration:
        stats = pool.get_statistics()
        
        if stats != last_stats:
            print(f"\\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯ (è¿è¡Œæ—¶é—´: {stats['uptime_seconds']:.1f}s):")
            print(f"   - æ´»è·ƒå·¥ä½œçº¿ç¨‹: {stats['active_workers']}/{stats['max_consumers']}")
            print(f"   - æ•°æ®é˜Ÿåˆ—å¤§å°: {stats['data_queue_size']}")
            print(f"   - å·²å¤„ç†æ‰¹æ¬¡: {stats['total_batches_processed']}")
            print(f"   - å·²ç¼“å­˜æ‰¹æ¬¡: {stats['total_batches_cached']}")
            print(f"   - åˆ·æ–°æ“ä½œ: {stats['total_flush_operations']}")
            print(f"   - å¤±è´¥æ“ä½œ: {stats['total_failed_operations']}")
            print(f"   - ç¼“å­˜è®°å½•æ•°: {stats['total_cached_records']}")
            
            # æ˜¾ç¤ºæ¯ä¸ªå·¥ä½œçº¿ç¨‹çš„è¯¦ç»†ä¿¡æ¯
            for worker_stat in stats['worker_statistics']:
                if worker_stat['batches_processed'] > 0 or worker_stat['cached_records'] > 0:
                    print(f"   Worker {worker_stat['worker_id']}: "
                          f"processed={worker_stat['batches_processed']}, "
                          f"cached={worker_stat['cached_records']}, "
                          f"flushes={worker_stat['flush_operations']}")
            
            last_stats = stats
        
        time.sleep(1)
    
    print("\\nâœ… ç›‘æ§ç»“æŸ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ConsumerPool ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # é…ç½®å‚æ•°
    db_path = Path("data/consumer_pool_example.db")
    db_path.parent.mkdir(exist_ok=True)
    
    config = {
        'max_consumers': 2,
        'batch_size': 10,      # è¾ƒå°çš„æ‰¹é‡å¤§å°ï¼Œä¾¿äºè§‚å¯Ÿåˆ·æ–°è¡Œä¸º
        'flush_interval': 5.0,  # 5ç§’åˆ·æ–°é—´éš”
        'db_path': str(db_path),
        'max_retries': 3
    }
    
    print(f"ğŸ“ é…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"   - {key}: {value}")
    
    # åˆ›å»ºæ¶ˆè´¹è€…æ± 
    pool = ConsumerPool(**config)
    
    try:
        # å¯åŠ¨æ¶ˆè´¹è€…æ± 
        print("\\nğŸ”§ å¯åŠ¨æ¶ˆè´¹è€…æ± ...")
        pool.start()
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        print("\\nğŸ“¦ åˆ›å»ºç¤ºä¾‹æ•°æ®æ‰¹æ¬¡...")
        batches = create_sample_data_batches()
        print(f"   åˆ›å»ºäº† {len(batches)} ä¸ªæ•°æ®æ‰¹æ¬¡")
        
        # æäº¤æ•°æ®æ‰¹æ¬¡
        print("\\nğŸ“¤ æäº¤æ•°æ®æ‰¹æ¬¡...")
        submitted_count = 0
        for i, batch in enumerate(batches):
            success = pool.submit_data(batch, timeout=2.0)
            if success:
                submitted_count += 1
                print(f"   âœ… æ‰¹æ¬¡ {i+1}: {batch.meta.get('task_type')} - {batch.symbol} ({batch.size} æ¡è®°å½•)")
            else:
                print(f"   âŒ æ‰¹æ¬¡ {i+1}: æäº¤å¤±è´¥")
        
        print(f"\\nğŸ“Š æˆåŠŸæäº¤ {submitted_count}/{len(batches)} ä¸ªæ‰¹æ¬¡")
        
        # ç›‘æ§å¤„ç†è¿›åº¦
        monitor_progress(pool, duration=20)
        
        # ç­‰å¾…é˜Ÿåˆ—å¤„ç†å®Œæˆ
        print("\\nâ³ ç­‰å¾…é˜Ÿåˆ—å¤„ç†å®Œæˆ...")
        empty = pool.wait_for_empty_queue(timeout=10.0)
        if empty:
            print("   âœ… é˜Ÿåˆ—å¤„ç†å®Œæˆ")
        else:
            print("   âš ï¸  ç­‰å¾…è¶…æ—¶ï¼Œé˜Ÿåˆ—å¯èƒ½ä»æœ‰æœªå¤„ç†æ•°æ®")
        
        # å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
        print("\\nğŸ”„ å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰ç¼“å­˜...")
        pool.force_flush_all()
        
        # æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        final_stats = pool.get_statistics()
        print(f"\\nğŸ“‹ æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ€»å¤„ç†æ‰¹æ¬¡: {final_stats['total_batches_processed']}")
        print(f"   - æ€»ç¼“å­˜æ‰¹æ¬¡: {final_stats['total_batches_cached']}")
        print(f"   - æ€»åˆ·æ–°æ“ä½œ: {final_stats['total_flush_operations']}")
        print(f"   - æ€»å¤±è´¥æ“ä½œ: {final_stats['total_failed_operations']}")
        print(f"   - è¿è¡Œæ—¶é—´: {final_stats['uptime_seconds']:.1f}s")
        
        # éªŒè¯æ•°æ®å­˜å‚¨
        print("\\nğŸ” éªŒè¯æ•°æ®å­˜å‚¨...")
        # é€šè¿‡ç¬¬ä¸€ä¸ªå·¥ä½œçº¿ç¨‹è®¿é—®å­˜å‚¨
        worker = list(pool.workers.values())[0]
        storage = worker.storage
        
        # æ£€æŸ¥å„ç±»æ•°æ®è¡¨
        tables = storage.list_tables()
        print(f"   æ•°æ®åº“ä¸­çš„è¡¨: {tables}")
        
        for table in tables:
            if 'daily_' in table:
                data_type, entity_id = table.split('_', 1)
                df = storage.query(data_type, entity_id)
                print(f"   - {table}: {len(df)} æ¡è®°å½•")
        
        # æŸ¥çœ‹è‚¡ç¥¨åˆ—è¡¨æ•°æ®
        stock_list = storage.query('system', 'stock_list')  
        if not stock_list.empty:
            print(f"   - è‚¡ç¥¨åˆ—è¡¨: {len(stock_list)} æ¡è®°å½•")
            print(f"     è‚¡ç¥¨: {', '.join(stock_list['name'].tolist())}")
        
        print("\\nâœ… ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³: {db_path.resolve()}")
        
    except Exception as e:
        print(f"\\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # åœæ­¢æ¶ˆè´¹è€…æ± 
        print("\\nğŸ›‘ åœæ­¢æ¶ˆè´¹è€…æ± ...")
        pool.stop(timeout=10.0)
        print("   âœ… æ¶ˆè´¹è€…æ± å·²åœæ­¢")


if __name__ == "__main__":
    main()
