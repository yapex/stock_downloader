#!/usr/bin/env python3
"""
æ•°æ®å‹ç¼©å’Œæ’åºè„šæœ¬ (V2)

æœ¬è„šæœ¬ä¸ºæ–°ç‰ˆæ•°æ®æ¶æ„æœåŠ¡ï¼Œå…¶æ ¸å¿ƒèŒè´£æ˜¯ï¼š
1. æ ¡éªŒæ•°æ®ç›®å½•ç»“æ„æ˜¯å¦ç¬¦åˆè§„èŒƒã€‚
2. å°†åˆ†åŒºå†…é›¶æ•£çš„ Parquet æ–‡ä»¶åˆå¹¶ã€æ’åºã€é‡å†™ä¸ºå•ä¸ªå¤§æ–‡ä»¶ï¼Œä»¥æå‡æŸ¥è¯¢æ€§èƒ½ã€‚

ä½¿ç”¨æ–¹æ³•:
    uv run python scripts/compact_and_sort.py --help
"""

import os
import sys
import shutil
import tomllib
import uuid
from pathlib import Path
from typing import List, Dict, Any
import typer
from typing_extensions import Annotated

# --- åŠ¨æ€è·¯å¾„è®¾ç½® ---
PROJECT_ROOT = Path(__file__).parent.parent
SRC_PATH = PROJECT_ROOT / "src"
if str(SRC_PATH) not in sys.path:
    sys.path.insert(0, str(SRC_PATH))

try:
    import duckdb
except ImportError:
    print("é”™è¯¯: æ— æ³•å¯¼å…¥ duckdbã€‚è¯·è¿è¡Œ 'uv pip install duckdb'")
    sys.exit(1)

import pandas as pd

# --- æ ¸å¿ƒè·¯å¾„é…ç½® ---
DATA_DIR = PROJECT_ROOT / "data" / "parquet"
TEMP_DIR = PROJECT_ROOT / "data" / "parquet_temp"
SCHEMA_FILE = PROJECT_ROOT / "stock_schema.toml"

# --- ç³»ç»Ÿæ–‡ä»¶ç™½åå• ---
SYSTEM_FILES_WHITELIST = {
    ".DS_Store",        # macOS ç³»ç»Ÿæ–‡ä»¶
    "Thumbs.db",        # Windows ç³»ç»Ÿæ–‡ä»¶
    ".gitignore",       # Git é…ç½®æ–‡ä»¶
    "desktop.ini",      # Windows ç³»ç»Ÿæ–‡ä»¶
    ".localized",       # macOS æœ¬åœ°åŒ–æ–‡ä»¶
    "._.DS_Store",      # macOS ç½‘ç»œå·ä¸Šçš„åŒç‚¹æ–‡ä»¶
    "Icon\r",           # macOS è‡ªå®šä¹‰å›¾æ ‡æ–‡ä»¶
}

# --- Typer åº”ç”¨å®ä¾‹ ---
app = typer.Typer(
    help="æ•°æ®å‹ç¼©å’Œæ’åºå·¥å…· (V2) - ä¸ºæ–°æ¶æ„ä¸‹çš„ Parquet æ•°æ®è¿›è¡Œåˆå¹¶å’Œæ’åºã€‚",
    add_completion=False,
    no_args_is_help=True,
)


def ask_user_confirmation(message: str, default: bool = False) -> bool:
    """ç®€å•çš„ç”¨æˆ·ç¡®è®¤å‡½æ•°"""
    default_text = "[Y/n]" if default else "[y/N]"
    try:
        response = input(f"âš ï¸ {message} {default_text}: ").strip().lower()
        if not response:
            return default
        return response in ['y', 'yes']
    except (KeyboardInterrupt, EOFError):
        print("\nç”¨æˆ·ä¸­æ–­æ“ä½œã€‚")
        return False

def safe_remove_backup(backup_path: Path) -> bool:
    """å®‰å…¨åˆ é™¤å¤‡ä»½ç›®å½•ï¼Œå¤„ç†æƒé™é—®é¢˜"""
    try:
        if backup_path.exists():
            # å…ˆä¿®å¤æƒé™
            os.system(f"chmod -R u+w '{backup_path}'")
            shutil.rmtree(backup_path)
            print(f"  âœ… å¤‡ä»½å·²åˆ é™¤: {backup_path}")
            return True
    except Exception as e:
        print(f"  âŒ åˆ é™¤å¤‡ä»½å¤±è´¥ {backup_path}: {e}")
        return False
    return False

def load_schema_config() -> Dict[str, Any]:
    """åŠ è½½ stock_schema.toml é…ç½®æ–‡ä»¶"""
    if not SCHEMA_FILE.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {SCHEMA_FILE}")
    with open(SCHEMA_FILE, "rb") as f:
        return tomllib.load(f)

def validate_source_directory_structure(source_path: Path, is_partitioned: bool):
    """å‰ç½®æ ¡éªŒï¼šæ£€æŸ¥æºç›®å½•ç»“æ„æ˜¯å¦ç¬¦åˆæ–°æ¶æ„è§„èŒƒ"""
    print(f"  æ ¡éªŒç›®å½•ç»“æ„: {source_path}")
    if not source_path.exists() or not source_path.is_dir():
        raise FileNotFoundError(f"æºç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•: {source_path}")

    ignored_files_count = 0

    if is_partitioned:
        # åˆ†åŒºè¡¨ï¼šç›®å½•ä¸‹åªå…è®¸å­˜åœ¨ year=... çš„å­ç›®å½•
        for item in source_path.iterdir():
            # æ£€æŸ¥æ˜¯å¦ä¸ºç³»ç»Ÿæ–‡ä»¶ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡
            if item.name in SYSTEM_FILES_WHITELIST:
                ignored_files_count += 1
                continue
                
            if item.is_file():
                raise ValueError(f"æ ¡éªŒå¤±è´¥ï¼åˆ†åŒºè¡¨ {source_path.name} çš„æ ¹ç›®å½•ä¸åº”åŒ…å«ä»»ä½•æ–‡ä»¶ï¼Œå‘ç°: {item.name}")
            if not item.name.startswith("year="):
                raise ValueError(f"æ ¡éªŒå¤±è´¥ï¼åˆ†åŒºè¡¨ {source_path.name} çš„å­ç›®å½•å¿…é¡»ä»¥ 'year=' å¼€å¤´ï¼Œå‘ç°: {item.name}")
    else:
        # éåˆ†åŒºè¡¨ï¼šç›®å½•ä¸‹åªå…è®¸å­˜åœ¨ .parquet æ–‡ä»¶
        for item in source_path.iterdir():
            # æ£€æŸ¥æ˜¯å¦ä¸ºç³»ç»Ÿæ–‡ä»¶ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡
            if item.name in SYSTEM_FILES_WHITELIST:
                ignored_files_count += 1
                continue
                
            if item.is_dir():
                raise ValueError(f"æ ¡éªŒå¤±è´¥ï¼éåˆ†åŒºè¡¨ {source_path.name} çš„æ ¹ç›®å½•ä¸åº”åŒ…å«ä»»ä½•å­ç›®å½•ï¼Œå‘ç°: {item.name}")
            if item.suffix != ".parquet":
                raise ValueError(f"æ ¡éªŒå¤±è´¥ï¼éåˆ†åŒºè¡¨ {source_path.name} çš„æ ¹ç›®å½•åªåº”åŒ…å« .parquet æ–‡ä»¶ï¼Œå‘ç°: {item.name}")
    
    if ignored_files_count > 0:
        print(f"  ğŸ“‹ å·²å¿½ç•¥ {ignored_files_count} ä¸ªç³»ç»Ÿæ–‡ä»¶")
    print("  âœ… ç›®å½•ç»“æ„æ ¡éªŒé€šè¿‡ã€‚")

def get_sort_columns(table_config: Dict[str, Any]) -> str:
    """æ ¹æ®è¡¨é…ç½®è·å–æ’åºåˆ— (ä¸å†åŒ…å« year)"""
    primary_key = table_config.get("primary_key", [])
    if not primary_key:
        raise ValueError(f"è¡¨ {table_config.get('table_name')} æ²¡æœ‰å®šä¹‰ä¸»é”®")
    return ", ".join(primary_key)

def validate_data_consistency(con: duckdb.DuckDBPyConnection, table_name: str, source_path: Path, target_path: Path, is_partitioned: bool) -> tuple[bool, bool]:
    """éªŒè¯æºæ•°æ®å’Œä¼˜åŒ–åæ•°æ®çš„ä¸€è‡´æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰
    
    Returns:
        tuple[bool, bool]: (is_valid, needs_confirmation)
        - is_valid: è¡¨ç¤ºéªŒè¯æ˜¯å¦é€šè¿‡
        - needs_confirmation: è¡¨ç¤ºæ˜¯å¦éœ€è¦ç”¨æˆ·ç¡®è®¤ï¼ˆæœ‰è­¦å‘Šæˆ–é”™è¯¯ï¼‰
    """
    print("ã€€å¼€å§‹æ•°æ®ä¸€è‡´æ€§éªŒè¯...")
    needs_confirmation = False
    
    try:
        if is_partitioned:
            # åˆ†åŒºè¡¨ä½¿ç”¨ç›®å½•æ¨¡å¼
            source_pattern = f"'{source_path}/**/*.parquet'"
            target_pattern = f"'{target_path}/**/*.parquet'"
            source_count = con.execute(f"SELECT COUNT(*) FROM read_parquet({source_pattern}, hive_partitioning=1)").fetchone()[0]
            target_count = con.execute(f"SELECT COUNT(*) FROM read_parquet({target_pattern}, hive_partitioning=1)").fetchone()[0]
        else:
            # éåˆ†åŒºè¡¨å¯èƒ½æ˜¯å•ä¸ªæ–‡ä»¶æˆ–ç›®å½•
            if target_path.is_file():
                # å¦‚æœç›®æ ‡æ˜¯å•ä¸ªæ–‡ä»¶ï¼Œç›´æ¥è¯»å–
                source_pattern = f"'{source_path}/**/*.parquet'"
                target_pattern = f"'{target_path}'"
                source_count = con.execute(f"SELECT COUNT(*) FROM read_parquet({source_pattern})").fetchone()[0]
                target_count = con.execute(f"SELECT COUNT(*) FROM read_parquet({target_pattern})").fetchone()[0]
            else:
                # å¦‚æœç›®æ ‡æ˜¯ç›®å½•ï¼Œä½¿ç”¨ç›®å½•æ¨¡å¼
                source_pattern = f"'{source_path}/**/*.parquet'"
                target_pattern = f"'{target_path}/**/*.parquet'"
                source_count = con.execute(f"SELECT COUNT(*) FROM read_parquet({source_pattern})").fetchone()[0]
                target_count = con.execute(f"SELECT COUNT(*) FROM read_parquet('{target_path}/**/*.parquet')").fetchone()[0]

        print(f"ã€€ã€€è®°å½•æ•°éªŒè¯: æº={source_count:,}, ä¼˜åŒ–å={target_count:,}")
        
        if source_count != target_count:
            needs_confirmation = True
            if not ask_user_confirmation(f"è¡¨ {table_name} è®°å½•æ•°ä¸ä¸€è‡´ (æº: {source_count}, ç›®æ ‡: {target_count})ã€‚è¿™é€šå¸¸æ˜¯ç”±äºå»é‡å¯¼è‡´ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ", default=True):
                return False, True
            print("ã€€ã€€âœ… è®°å½•æ•°æ ¡éªŒé€šè¿‡ï¼ˆç”¨æˆ·ç¡®è®¤ï¼‰ã€‚")
            return True, True
        else:
            print("ã€€ã€€âœ… è®°å½•æ•°æ ¡éªŒé€šè¿‡ã€‚")
            return True, False
            
    except Exception as e:
        print(f"ã€€âŒ éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        needs_confirmation = True
        if not ask_user_confirmation("éªŒè¯å¤±è´¥ï¼Œæ˜¯å¦å¼ºè¡Œç»§ç»­ï¼Ÿ", default=False):
            return False, True
        print("ã€€ã€€âœ… ç”¨æˆ·é€‰æ‹©å¼ºè¡Œç»§ç»­ã€‚")
        return True, True

def optimize_table(con: duckdb.DuckDBPyConnection, table_name: str, table_config: Dict[str, Any]):
    """å¯¹å•ä¸ªè¡¨çš„æ•°æ®è¿›è¡Œæ’åºã€åˆ†åŒºå’Œé‡å†™"""
    source_path = DATA_DIR / table_name
    target_path = TEMP_DIR / table_name
    
    is_partitioned = "date_col" in table_config

    # 1. å‰ç½®æ ¡éªŒ
    validate_source_directory_structure(source_path, is_partitioned)

    # 2. å‡†å¤‡ SQL
    sort_columns = get_sort_columns(table_config)
    source_pattern = f"'{source_path}/**/*.parquet'"
    
    if is_partitioned:
        copy_statement = f"""
        COPY (
            SELECT * FROM read_parquet({source_pattern}, hive_partitioning=1)
            ORDER BY {sort_columns}
        )
        TO '{target_path}'
        (FORMAT PARQUET, PARTITION_BY (year), OVERWRITE_OR_IGNORE 1);
        """
    else:
        # éåˆ†åŒºè¡¨ï¼šéœ€è¦å…ˆåˆ›å»ºç›®å½•ï¼Œç„¶åæŒ‡å®šå…·ä½“æ–‡ä»¶è·¯å¾„ä»¥ä¿æŒç›®å½•ç»“æ„
        # ä½¿ç”¨UUIDç¡®ä¿æ–‡ä»¶åå”¯ä¸€ï¼Œé¿å…é‡å¤è¦†ç›–
        unique_id = str(uuid.uuid4())[:8]  # ä½¿ç”¨UUIDå‰8ä½ç¡®ä¿å”¯ä¸€æ€§
        target_file_path = target_path / f"{table_name}-{unique_id}.parquet"
        copy_statement = f"""
        COPY (
            SELECT * FROM read_parquet({source_pattern})
            ORDER BY {sort_columns}
        )
        TO '{target_file_path}'
        (FORMAT PARQUET, OVERWRITE_OR_IGNORE 1);
        """

    print(f"  å‡†å¤‡æ‰§è¡Œä¼˜åŒ– SQL...")
    print(copy_statement)

    # 3. æ‰§è¡Œä¼˜åŒ–
    if target_path.exists():
        if target_path.is_dir():
            shutil.rmtree(target_path)
        else:
            target_path.unlink()
    
    # ä¸ºéåˆ†åŒºè¡¨åˆ›å»ºç›®å½•
    if not is_partitioned:
        target_path.mkdir(parents=True, exist_ok=True)
    
    con.execute(copy_statement)
    print(f"  âœ… è¡¨ {table_name} å·²æˆåŠŸä¼˜åŒ–åˆ°ä¸´æ—¶ç›®å½•: {target_path}")

    # 4. æ•°æ®ä¸€è‡´æ€§æ ¡éªŒ
    is_valid, needs_confirmation = validate_data_consistency(con, table_name, source_path, target_path, is_partitioned)
    if not is_valid:
        print(f"âŒ {table_name} æ•°æ®éªŒè¯å¤±è´¥ï¼Œä¼˜åŒ–ä¸­æ–­ã€‚ä¸´æ—¶æ•°æ®ä¿ç•™åœ¨ {target_path}")
        raise typer.Exit(1)

    # 5. æ›¿æ¢æ—§æ•°æ®
    # ä½¿ç”¨UUIDç¡®ä¿å¤‡ä»½ç›®å½•åå”¯ä¸€
    timestamp = pd.Timestamp.now().strftime('%Y%m%d%H%M%S')
    unique_id = str(uuid.uuid4())[:8]
    backup_path = source_path.with_suffix(f".backup_{timestamp}_{unique_id}")
    print(f"  æ­£åœ¨ç”¨ä¼˜åŒ–åçš„æ•°æ®æ›¿æ¢æ—§æ•°æ®...å¤‡ä»½è‡³: {backup_path}")
    source_path.rename(backup_path)
    target_path.rename(source_path)
    print("  âœ… æ•°æ®æ›¿æ¢æˆåŠŸã€‚ à¦¸à¦¨")

    # 6. æ¸…ç†å¤‡ä»½
    if needs_confirmation:
        # æœ‰è­¦å‘Šæˆ–é”™è¯¯ï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤æ˜¯å¦åˆ é™¤å¤‡ä»½
        if ask_user_confirmation(f"æ˜¯å¦åˆ é™¤å¤‡ä»½ç›®å½• {backup_path}ï¼Ÿ", default=True):
            safe_remove_backup(backup_path)
        else:
            print(f"ã€€ğŸ“‹ å¤‡ä»½ç›®å½•ä¿ç•™åœ¨: {backup_path}")
    else:
        # éªŒè¯å®Œå…¨æ­£å¸¸ï¼Œè‡ªåŠ¨åˆ é™¤å¤‡ä»½
        print(f"ã€€âœ… éªŒè¯æ­£å¸¸ï¼Œè‡ªåŠ¨åˆ é™¤å¤‡ä»½ç›®å½•: {backup_path}")
        safe_remove_backup(backup_path)

@app.command(help="ä¼˜åŒ–æŒ‡å®šçš„è¡¨")
def optimize(
    table: Annotated[str, typer.Option("--table", "-t", help="è¦ä¼˜åŒ–çš„è¡¨å")],
):
    """ä¼˜åŒ–æŒ‡å®šçš„è¡¨"""
    try:
        schema_config = load_schema_config()
        if table not in schema_config:
            raise ValueError(f"è¡¨ '{table}' åœ¨é…ç½®æ–‡ä»¶ä¸­ä¸å­˜åœ¨")
        
        TEMP_DIR.mkdir(parents=True, exist_ok=True)
        with duckdb.connect(database=':memory:') as con:
            optimize_table(con, table, schema_config[table])
        print(f"\nğŸ‰ è¡¨ {table} ä¼˜åŒ–å®Œæˆï¼")

    except (ValueError, FileNotFoundError) as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        raise typer.Exit(1)
    finally:
        if TEMP_DIR.exists() and not any(TEMP_DIR.iterdir()):
            TEMP_DIR.rmdir()

@app.command(help="ä¼˜åŒ–æ‰€æœ‰è¡¨")
def optimize_all(
    exclude: Annotated[List[str], typer.Option("--exclude", "-e", help="è¦æ’é™¤çš„è¡¨å")] = None,
):
    """ä¼˜åŒ–æ‰€æœ‰è¡¨"""
    exclude = exclude or []
    schema_config = load_schema_config()
    tables_to_optimize = [name for name in schema_config.keys() if name not in exclude]
    
    TEMP_DIR.mkdir(parents=True, exist_ok=True)
    
    with duckdb.connect(database=':memory:') as con:
        for i, table_name in enumerate(tables_to_optimize):
            print(f"\n---\nå¤„ç†è¡¨ {i+1}/{len(tables_to_optimize)}: {table_name}")
            try:
                optimize_table(con, table_name, schema_config[table_name])
            except (ValueError, FileNotFoundError) as e:
                print(f"âŒ è·³è¿‡è¡¨ {table_name}ï¼ŒåŸå› : {e}")
                continue
    
    print("\nğŸ‰ æ‰€æœ‰è¡¨ä¼˜åŒ–å®Œæˆï¼")
    if TEMP_DIR.exists() and not any(TEMP_DIR.iterdir()):
        TEMP_DIR.rmdir()

@app.command(help="æ¸…ç†æ‰€æœ‰æ—§çš„å¤‡ä»½ç›®å½•")
def clean_backups(
    force: Annotated[bool, typer.Option("--force", "-f", help="å¼ºåˆ¶åˆ é™¤ï¼Œä¸è¯¢é—®ç”¨æˆ·")] = False,
):
    """æ¸…ç†æ‰€æœ‰æ—§çš„å¤‡ä»½ç›®å½•"""
    if not DATA_DIR.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
        raise typer.Exit(1)
    
    # æŸ¥æ‰¾æ‰€æœ‰å¤‡ä»½ç›®å½•
    backup_dirs = []
    for item in DATA_DIR.iterdir():
        if item.is_dir() and (".backup" in item.name or item.name.endswith(".backup")):
            backup_dirs.append(item)
    
    if not backup_dirs:
        print("âœ… æ²¡æœ‰å‘ç°éœ€è¦æ¸…ç†çš„å¤‡ä»½ç›®å½•ã€‚")
        return
    
    print(f"å‘ç° {len(backup_dirs)} ä¸ªå¤‡ä»½ç›®å½•:")
    for backup_dir in backup_dirs:
        print(f"  - {backup_dir.name}")
    
    if not force:
        if not ask_user_confirmation(f"æ˜¯å¦åˆ é™¤æ‰€æœ‰ {len(backup_dirs)} ä¸ªå¤‡ä»½ç›®å½•ï¼Ÿ", default=False):
            print("ç”¨æˆ·å–æ¶ˆæ“ä½œã€‚")
            return
    
    # åˆ é™¤å¤‡ä»½
    success_count = 0
    for backup_dir in backup_dirs:
        print(f"\næ­£åœ¨åˆ é™¤: {backup_dir}")
        if safe_remove_backup(backup_dir):
            success_count += 1
    
    print(f"\nğŸ‰ æ¸…ç†å®Œæˆï¼æˆåŠŸåˆ é™¤ {success_count}/{len(backup_dirs)} ä¸ªå¤‡ä»½ç›®å½•ã€‚")

if __name__ == "__main__":
    app()
