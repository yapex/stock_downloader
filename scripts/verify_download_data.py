#!/usr/bin/env python3
"""éªŒè¯ä¸‹è½½æ•°æ®çš„å®Œæ•´æ€§è„šæœ¬

æ­¤è„šæœ¬ç”¨äºï¼š
1. ä» check_and_redown æ—¥å¿—æ–‡ä»¶ä¸­è¯»å–éœ€è¦ä¸‹è½½çš„è‚¡ç¥¨åˆ—è¡¨
2. éªŒè¯è¿™äº›è‚¡ç¥¨åœ¨ metadata.db ä¸­æ˜¯å¦æœ‰æ•°æ®
3. æŒ‰æ¯”ä¾‹æŠ½æ ·æ£€æŸ¥ parquet æ–‡ä»¶æ˜¯å¦åŒ…å«ç›¸åº”æ•°æ®
4. ç”ŸæˆéªŒè¯æŠ¥å‘Š
"""

import logging
import sys
import json
import random
from pathlib import Path
from typing import List, Set, Dict, Tuple
import duckdb

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from neo.configs import get_config

logger = logging.getLogger(__name__)


def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler(sys.stdout)],
    )


def get_latest_check_log() -> Path:
    """è·å–æœ€æ–°çš„check_and_redownæ—¥å¿—æ–‡ä»¶"""
    logs_dir = project_root / "logs"
    if not logs_dir.exists():
        raise FileNotFoundError("logsç›®å½•ä¸å­˜åœ¨")
    
    log_files = list(logs_dir.glob("check_and_redown_*.json"))
    if not log_files:
        raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ°check_and_redownæ—¥å¿—æ–‡ä»¶")
    
    # è¿”å›æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶
    latest_log = max(log_files, key=lambda f: f.stat().st_mtime)
    logger.info(f"ä½¿ç”¨æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶: {latest_log}")
    return latest_log


def load_missing_stocks_from_log(log_file: Path) -> Dict[str, List[str]]:
    """ä»æ—¥å¿—æ–‡ä»¶ä¸­åŠ è½½ç¼ºå¤±è‚¡ç¥¨ä¿¡æ¯"""
    with open(log_file, 'r', encoding='utf-8') as f:
        log_data = json.load(f)
    
    missing_by_table = {}
    for table_name, table_data in log_data["missing_by_table"].items():
        if table_data["count"] > 0:
            missing_by_table[table_name] = table_data["stocks"]
    
    return missing_by_table


def verify_stocks_in_database(conn: duckdb.DuckDBPyConnection, 
                             table_name: str, 
                             stock_codes: List[str]) -> Tuple[List[str], List[str]]:
    """éªŒè¯è‚¡ç¥¨åœ¨æ•°æ®åº“ä¸­çš„å­˜åœ¨æƒ…å†µ
    
    Returns:
        Tuple[List[str], List[str]]: (å­˜åœ¨çš„è‚¡ç¥¨, ä»ç„¶ç¼ºå¤±çš„è‚¡ç¥¨)
    """
    if not stock_codes:
        return [], []
    
    try:
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        check_table_sql = f"SELECT COUNT(*) FROM information_schema.tables WHERE table_name = '{table_name}'"
        table_exists = conn.execute(check_table_sql).fetchone()[0] > 0
        
        if not table_exists:
            logger.warning(f"è¡¨ {table_name} ä¸å­˜åœ¨")
            return [], stock_codes
        
        # æ„å»º IN æŸ¥è¯¢æ¡ä»¶
        codes_str = "', '".join(stock_codes)
        sql = f"""
            SELECT DISTINCT ts_code 
            FROM {table_name} 
            WHERE ts_code IN ('{codes_str}')
        """
        
        result = conn.execute(sql).fetchall()
        existing_stocks = [row[0] for row in result]
        missing_stocks = [code for code in stock_codes if code not in existing_stocks]
        
        return existing_stocks, missing_stocks
        
    except Exception as e:
        logger.error(f"éªŒè¯è¡¨ {table_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return [], stock_codes


def sample_verify_parquet_files(table_name: str, 
                               existing_stocks: List[str], 
                               sample_ratio: float = 0.1) -> Dict[str, any]:
    """æŠ½æ ·éªŒè¯parquetæ–‡ä»¶ä¸­çš„æ•°æ®
    
    Args:
        table_name: è¡¨å
        existing_stocks: åœ¨æ•°æ®åº“ä¸­å­˜åœ¨çš„è‚¡ç¥¨åˆ—è¡¨
        sample_ratio: æŠ½æ ·æ¯”ä¾‹ï¼Œé»˜è®¤10%
        
    Returns:
        Dict: éªŒè¯ç»“æœ
    """
    if not existing_stocks:
        return {"sampled_count": 0, "verified_count": 0, "success_rate": 0.0}
    
    # è®¡ç®—æŠ½æ ·æ•°é‡ï¼ˆè‡³å°‘1ä¸ªï¼Œæœ€å¤š10ä¸ªï¼‰
    sample_count = max(1, min(10, int(len(existing_stocks) * sample_ratio)))
    sampled_stocks = random.sample(existing_stocks, sample_count)
    
    parquet_dir = project_root / "data/parquet" / table_name
    if not parquet_dir.exists():
        logger.warning(f"Parquetç›®å½•ä¸å­˜åœ¨: {parquet_dir}")
        return {"sampled_count": sample_count, "verified_count": 0, "success_rate": 0.0}
    
    verified_count = 0
    
    for stock_code in sampled_stocks:
        # æŸ¥æ‰¾åŒ…å«è¯¥è‚¡ç¥¨çš„parquetæ–‡ä»¶
        found_files = list(parquet_dir.rglob("*.parquet"))
        stock_found = False
        
        for parquet_file in found_files[:5]:  # æœ€å¤šæ£€æŸ¥5ä¸ªæ–‡ä»¶
            try:
                # ä½¿ç”¨DuckDBç›´æ¥æŸ¥è¯¢parquetæ–‡ä»¶
                with duckdb.connect() as temp_conn:
                    check_sql = f"""
                        SELECT COUNT(*) 
                        FROM read_parquet('{parquet_file}') 
                        WHERE ts_code = '{stock_code}'
                        LIMIT 1
                    """
                    result = temp_conn.execute(check_sql).fetchone()
                    if result and result[0] > 0:
                        stock_found = True
                        break
            except Exception as e:
                # å¿½ç•¥å•ä¸ªæ–‡ä»¶çš„è¯»å–é”™è¯¯
                continue
        
        if stock_found:
            verified_count += 1
    
    success_rate = verified_count / sample_count if sample_count > 0 else 0.0
    
    return {
        "sampled_count": sample_count,
        "verified_count": verified_count,
        "success_rate": success_rate,
        "sampled_stocks": sampled_stocks
    }


def generate_verification_report(verification_results: Dict[str, any]) -> Path:
    """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
    from datetime import datetime
    
    logs_dir = project_root / "logs"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = logs_dir / f"verification_report_{timestamp}.json"
    
    # æ·»åŠ æ±‡æ€»ä¿¡æ¯
    total_missing = sum(len(table_data["original_missing"]) for table_data in verification_results["tables"].values())
    total_found = sum(len(table_data["now_existing"]) for table_data in verification_results["tables"].values())
    total_still_missing = sum(len(table_data["still_missing"]) for table_data in verification_results["tables"].values())
    
    verification_results["summary"] = {
        "total_originally_missing": total_missing,
        "total_now_found": total_found,
        "total_still_missing": total_still_missing,
        "overall_success_rate": total_found / total_missing if total_missing > 0 else 0.0
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(verification_results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ“Š éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    return report_file


def main():
    """ä¸»å‡½æ•°"""
    setup_logging()
    logger.info("å¼€å§‹éªŒè¯ä¸‹è½½æ•°æ®çš„å®Œæ•´æ€§...")
    
    try:
        # 1. è·å–æœ€æ–°çš„check_and_redownæ—¥å¿—æ–‡ä»¶
        log_file = get_latest_check_log()
        
        # 2. ä»æ—¥å¿—æ–‡ä»¶ä¸­åŠ è½½ç¼ºå¤±è‚¡ç¥¨ä¿¡æ¯
        missing_by_table = load_missing_stocks_from_log(log_file)
        if not missing_by_table:
            logger.info("âœ… æ²¡æœ‰éœ€è¦éªŒè¯çš„ç¼ºå¤±è‚¡ç¥¨")
            return
        
        logger.info(f"éœ€è¦éªŒè¯ {len(missing_by_table)} ä¸ªè¡¨çš„æ•°æ®")
        
        # 3. è¿æ¥æ•°æ®åº“å¹¶éªŒè¯
        config = get_config()
        db_path = Path(config.database.metadata_path)
        if not db_path.is_absolute():
            db_path = project_root / db_path
        
        verification_results = {
            "timestamp": log_file.stem.replace("check_and_redown_", ""),
            "tables": {}
        }
        
        with duckdb.connect(str(db_path), read_only=True) as conn:
            for table_name, missing_stocks in missing_by_table.items():
                logger.info(f"éªŒè¯è¡¨ {table_name} (åŸç¼ºå¤± {len(missing_stocks)} ä¸ªè‚¡ç¥¨)...")
                
                # éªŒè¯æ•°æ®åº“ä¸­çš„å­˜åœ¨æƒ…å†µ
                existing_stocks, still_missing = verify_stocks_in_database(
                    conn, table_name, missing_stocks
                )
                
                logger.info(f"  æ•°æ®åº“éªŒè¯: {len(existing_stocks)} å·²æ‰¾åˆ°, {len(still_missing)} ä»ç¼ºå¤±")
                
                # æŠ½æ ·éªŒè¯parquetæ–‡ä»¶
                parquet_verification = sample_verify_parquet_files(table_name, existing_stocks)
                
                logger.info(f"  ParquetæŠ½æ ·éªŒè¯: {parquet_verification['verified_count']}/{parquet_verification['sampled_count']} "
                           f"({parquet_verification['success_rate']:.1%} æˆåŠŸç‡)")
                
                verification_results["tables"][table_name] = {
                    "original_missing": missing_stocks,
                    "now_existing": existing_stocks,
                    "still_missing": still_missing,
                    "parquet_verification": parquet_verification
                }
        
        # 4. ç”ŸæˆéªŒè¯æŠ¥å‘Š
        report_file = generate_verification_report(verification_results)
        
        # 5. è¾“å‡ºæ±‡æ€»ä¿¡æ¯
        summary = verification_results["summary"]
        logger.info(f"\nğŸ“ˆ éªŒè¯æ±‡æ€»:")
        logger.info(f"  åŸç¼ºå¤±è‚¡ç¥¨æ€»æ•°: {summary['total_originally_missing']}")
        logger.info(f"  ç°å·²æ‰¾åˆ°è‚¡ç¥¨æ•°: {summary['total_now_found']}")
        logger.info(f"  ä»ç„¶ç¼ºå¤±è‚¡ç¥¨æ•°: {summary['total_still_missing']}")
        logger.info(f"  æ•´ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.1%}")
        
        if summary['total_still_missing'] > 0:
            logger.warning(f"âš ï¸  ä»æœ‰ {summary['total_still_missing']} ä¸ªè‚¡ç¥¨æ•°æ®ç¼ºå¤±ï¼Œå»ºè®®é‡æ–°è¿è¡Œ check_and_redown.py")
        else:
            logger.info("ğŸ‰ æ‰€æœ‰è‚¡ç¥¨æ•°æ®éªŒè¯é€šè¿‡ï¼")
            
        logger.info("éªŒè¯å®Œæˆ")
        
    except Exception as e:
        logger.error(f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
