好的，我已经将您提供的两份文档内容进行合并与重构。

新的文档结构保留了您原有文档的完整性和易用性，同时将我们讨论的**核心存储架构理念（两级火箭优化）**提升到了一个非常突出和核心的位置（新的第 3 节），并增加了关于**数据重整**的关键维护步骤。

这使得任何开发者不仅知道“怎么做”，更能深刻理解“为什么必须这么做”。

---

# **数据存储开发指南 (V2.0)**

## 1. 技术核心

本项目的数据存储采用现代化的本地数据湖架构，其核心技术如下：

- **查询引擎 (Query Engine)**: **DuckDB**

  - 一个高性能的嵌入式分析数据库，无需启动任何数据库服务（Serverless）。

- **存储格式 (Storage Format)**: **Apache Parquet**

  - 高效的列式存储格式，提供高压缩比和快速的分析查询性能。所有数据文件存储在 `data/parquet/` 文件夹下。

- **查询入口 (Query Entry Point)**: `data/metadata.db`

  - 这是**唯一**的查询入口文件。它是一个 DuckDB 数据库，仅包含指向 Parquet 数据文件的**视图 (VIEWs)**，本身不存储任何业务数据。

- **物理布局 (Physical Layout)**: **Hive 分区与主键排序**
  - 为了实现极致的查询性能，数据在物理层面经过精心设计：
    1.  **按 `year` 字段进行目录分区**，用于粗粒度数据裁剪。
    2.  **在分区文件内部，数据严格按主键（如 `ts_code`, `trade_date`）排序**，用于实现微观数据块的跳过。

## 2. 快速上手

您可以通过任何支持 DuckDB 的工具连接 `data/metadata.db` 文件进行查询。

### 2.1. 环境准备

```bash
pip install duckdb pandas
```

### 2.2. Python 查询示例

```python
import duckdb
import pandas

DB_FILE = "data/metadata.db"

try:
    with duckdb.connect(database=DB_FILE, read_only=True) as con:
        print("Successfully connected to DuckDB.")

        # 示例1：查看所有可用的表
        print("\n--- Available Tables ---")
        tables = con.execute("SHOW TABLES;").fetchdf()
        print(tables)

        # 示例2：高效查询贵州茅台2023年后的利润表数据
        print("\n--- Querying income_statement for 600519.SH (year >= 2023) ---")
        income_statement_df = con.execute(
            """
            SELECT end_date, n_income, total_revenue
            FROM income_statement
            WHERE
                ts_code = '600519.SH'
                AND year >= 2023  -- 关键性能点
            ORDER BY end_date DESC;
            """
        ).fetchdf()
        print(income_statement_df)

except duckdb.Error as e:
    print(f"An error occurred: {e}")
```

---

## 3. 核心架构与查询最佳实践

> [!IMPORTANT] > **理解本章节是写出高性能查询并维护数据湖的关键。** 本项目的性能基石源于一个“两级火箭式”的查询优化策略。

### 3.1. L1 优化：按 `year` 分区 -> 实现“分区裁剪”

这是第一级，也是最粗粒度的优化，旨在**剔除不相关年份的全部数据**。

- **原理**：我们将数据在物理上按 `year` 分割成不同的目录 (`/year=2022/`, `/year=2023/` ...)。当查询的 `WHERE` 子句包含 `year` 条件时，DuckDB 会执行**分区裁剪 (Partition Pruning)**，直接忽略所有不相关年份的目录，从而将扫描数据量减少几个数量级。
- **规则**：**任何对时间序列表的查询，都必须在 `WHERE` 子句中包含对 `year` 的过滤。**

✅ **高效查询**:

```sql
-- 查询2023年的数据
SELECT * FROM stock_daily WHERE year = 2023;
```

❌ **低效查询（严禁）**:

```sql
-- 错误！这将导致全表扫描！
SELECT * FROM stock_daily WHERE trade_date LIKE '2023%';
```

### 3.2. L2 优化：按主键排序 -> 实现“谓词下推”

这是第二级，在文件内部进行的微观优化，旨在**跳过文件内不相关的数据块**。

- **原理**：在每个分区目录的 Parquet 文件内部，数据**严格按主键（如 `ts_code`, `trade_date`）排序**。Parquet 文件格式允许 DuckDB 读取每个数据块的元数据（包含列的最大/最小值）。当处理 `WHERE ts_code = '600519.SH'` 这样的条件时，DuckDB 会执行**谓词下推 (Predicate Pushdown)**，直接跳过那些 `ts_code` 范围不包含 `'600519.SH'` 的数据块，实现类似“索引”的快速定位效果。
- **规则**：**数据在写入时必须经过排序。**（详见第 5.1 节）

### 3.3. 组合查询的最佳实践

当 L1 和 L2 优化组合使用时，性能最高。

```sql
-- 查询 600519.SH 的2022年年度现金流量表
SELECT * FROM cash_flow
WHERE
  ts_code = '600519.SH' -- 2. 触发 L2 谓词下推，快速定位股票
  AND year = 2022 -- 1. 触发 L1 分区裁剪，快速定位年份
  AND end_date LIKE '%1231'; -- 3. 在最小数据范围内进行精确匹配
```

---

## 4. 可用数据表及结构

> [!IMPORTANT]
> 除 `stock_basic` 表外，所有时间序列表都包含 `year` 字段用于分区。`stock_basic` 为静态维度表，不含 `year`。

完整的表结构定义在项目根目录的 `stock_schema.toml` 文件中。

- **`stock_basic`**: 股票基本信息表
- **`stock_daily`**: 股票日线数据
- **`daily_basic`**: 每日重要的基本面指标
- **`income_statement`**: 利润表
- **`balance_sheet`**: 资产负债表
- **`cash_flow`**: 现金流量表
- **`trade_cal`**: 交易日历
- ... 等

---

## 5. 数据与元数据维护

### 5.1. 关键维护：数据重整与排序 (Compaction & Sorting)

新下载的数据是零散、乱序的，其查询性能低下。为了使其符合我们 L2 优化的要求，**必须定期运行数据优化脚本**。

- **目的**：将零散、无序的 Parquet 小文件，合并重写为内部按主键（如 `year, ts_code, trade_date`）严格排序的大文件。
- **操作**：
  ```bash
  # 此脚本负责读取原始数据，进行高效的核外排序，并按分区写回
  uv run python scripts/compact_and_sort.py
  ```
- **时机**：建议在每日数据下载任务完成后，或在需要执行高性能分析前运行此脚本。

### 5.2. 重建元数据 (`metadata.db`)

`metadata.db` 文件是无状态的，它只存储指向 `data/parquet/` 目录下数据文件的视图。如果数据文件发生了物理变化（例如，经过了 5.1 节的重整），或者 `metadata.db` 损坏，你需要重建它。

- **前提**：`data/parquet/` 目录中已经存在优化好的数据。
- **步骤 1：删除旧文件**
  ```bash
  rm -f data/metadata.db data/metadata.db.wal
  ```
- **步骤 2：运行同步脚本**
  ```bash
  # 此脚本会扫描 parquet 目录，并为每个表重新创建视图
  uv run python scripts/manual_sync_metadata.py
  ```

执行完毕后，`metadata.db` 将被更新，指向优化后的数据文件，从而使所有查询都能享受到完整的性能优势。
