# 数据存储开发指南

本文档为开发者提供访问和使用 `stock_downloader` 项目所存储数据的详细指南。

## 1. 技术核心

本项目的数据存储采用现代化的本地数据湖架构，其核心技术如下：

- **查询引擎 (Query Engine)**: **DuckDB**
  - 一个高性能的嵌入式分析数据库，无需启动任何数据库服务（Serverless）。
  - 通过标准 SQL 即可查询。

- **存储格式 (Storage Format)**: **Apache Parquet**
  - 高效的列式存储格式，提供高压缩比和快速的分析查询性能。
  - 所有数据文件存储在项目根目录的 `data/parquet/` 文件夹下。

- **查询入口 (Query Entry Point)**: `data/metadata.db`
  - 这是**唯一**的查询入口文件。它是一个 DuckDB 数据库，本身不存储任何庞大的业务数据，仅包含所有数据表的**模式 (Schema)**和指向 Parquet 数据文件的**元数据索引**。
  - 任何兼容 DuckDB 的客户端或库都可以通过连接此文件来查询整个数据湖。

- **数据分区 (Data Partitioning)**
  - 所有按日期记录的数据表都额外增加了一个 `year` 列作为**分区键**。在查询时利用此字段作为过滤条件，可以极大提升查询性能。

## 2. 快速上手

您可以通过任何支持 DuckDB 的工具连接 `data/metadata.db` 文件进行查询。以下是一个 Python 示例。

### 2.1. 环境准备

首先，请确保您已安装 `duckdb` 和 `pandas` 库。

```bash
pip install duckdb pandas
```

### 2.2. Python 查询示例

下面的代码演示了如何连接数据库，并查询“每日基本面指标” (`daily_basic`) 表的少量数据。

```python
import duckdb
import pandas

# 定义元数据数据库的路径
DB_FILE = "data/metadata.db"

# 使用 with 语句确保连接自动关闭
try:
    with duckdb.connect(database=DB_FILE, read_only=True) as con:
        print("Successfully connected to DuckDB.")

        # 示例1：查看所有可用的表
        print("\n--- Available Tables ---")
        tables = con.execute("SHOW TABLES;").fetchdf()
        print(tables)

        # 示例2：查询 'daily_basic' 表的前 5 条数据
        print("\n--- Querying daily_basic (first 5 rows) ---")
        daily_basic_df = con.execute("SELECT * FROM daily_basic LIMIT 5;").fetchdf()
        print(daily_basic_df)

        # 示例3：利用分区键 'year' 高效查询贵州茅台2023年后的利润表数据
        print("\n--- Querying income_statement for 600519.SH (year >= 2023) ---")
        income_statement_df = con.execute(
            """
            SELECT end_date, n_income, total_revenue
            FROM income_statement
            WHERE
                ts_code = '600519.SH'
                AND year >= 2023
            ORDER BY end_date DESC;
            """
        ).fetchdf()
        print(income_statement_df)

except duckdb.Error as e:
    print(f"An error occurred: {e}")

```

## 3. 数据查询最佳实践

### 3.1. 必须利用 `year` 分区键提升性能

数据湖中的核心设计之一就是按 `year` 字段对时间序列数据进行了**分区（Partitioning）**。这个字段并非由原始数据源（如Tushare）提供，而是项目在数据处理阶段为了优化性能特意添加的。

在查询时，**必须在 `WHERE` 子句中加入对 `year` 的过滤条件**，例如 `WHERE year = 2023`。这样做可以触发 DuckDB 的分区裁剪（Partition Pruning）功能，使其仅扫描对应年份的 Parquet 文件，从而将查询性能提升数个数量级。

这远比使用 `LIKE` 对日期字符串（如 `end_date LIKE '2023%'`）进行过滤要高效得多。

### 3.2. 精确查询年度报告的最佳实践

当需要查询特定年份的**年度报告**时，最高效且最准确的方式是**组合使用 `year` 和 `end_date` 字段**：

```sql
-- 查询 600519.SH 的2022年年度现金流量表
SELECT * FROM cash_flow
WHERE
  ts_code = '600519.SH'
  AND year = 2022 -- 1. 利用分区键快速定位到2022年数据
  AND end_date LIKE '%1231'; -- 2. 在分区内部精确匹配年度报告
```
这种方式兼具了分区查询的性能和业务逻辑的准确性。

### 3.3. 使用内置查询脚本

本项目在 `scripts/` 目录下提供了一个命令行查询工具 `query_parquet_data.py`，方便快速执行 SQL 查询。

**查询示例：**

查询贵州茅台（600519.SH）自2015年以来的净利润：

```bash
uv run python scripts/query_parquet_data.py --sql "SELECT end_date, n_income FROM income_statement WHERE ts_code = '600519.SH' AND year >= '2015' ORDER BY end_date DESC"
```

## 4. 可用数据表及结构

以下是当前数据湖中所有可用的数据表。完整的表结构（包括所有字段名、类型、主键等）定义在项目根目录的 `stock_schema.toml` 文件中，请直接参考该文件以获取最准确的 schema 信息。

> [!IMPORTANT]
> 除 `stock_basic` 表外，所有其他记录时间序列数据的表都包含一个由系统自动添加的 `year` 字段作为分区键，用于提升查询性能。`stock_basic` 为静态维度表，因此不包含 `year` 字段。

---

- **`stock_basic`**: 股票基本信息表字段
- **`stock_daily`**: 股票日线数据字段
- **`stock_adj_qfq`**: 复权行情数据字段
- **`daily_basic`**: 获取全部股票每日重要的基本面指标
- **`income_statement`**: 利润表字段
- **`balance_sheet`**: 资产负债表字段
- **`cash_flow`**: 现金流量表字段
- **`trade_cal`**: 交易日历

---

## 5. 维护指南：重建元数据 (`metadata.db`)

本文档详细说明了 `stock_downloader` 项目中元数据文件 `data/metadata.db` 的作用，以及如何从零开始安全地重建它。

### 5.1. `metadata.db` 的核心作用

`metadata.db` 是一个 DuckDB 数据库文件，但它**不存储任何实际的股票数据**。它的核心作用是充当一个**“元数据目录”**或**“查询入口”**。

它内部只包含了一系列的 **视图（VIEWs）**，每个视图对应一张数据表（如 `daily_basic`）。这些视图的定义指向了 `data/parquet/` 目录下对应的 Parquet 数据文件。

当你通过 `metadata.db` 查询一张表时，DuckDB 会：
1.  查找对应的视图定义。
2.  根据视图定义，实时地从 `data/parquet/` 目录下的一个或多个 Parquet 文件中读取数据并返回结果。

**结论：`metadata.db` 是无状态的，可以随时被删除和重建，而不会丢失任何业务数据。**

### 5.2. 重建 `metadata.db` 的先决条件

在开始之前，请确保满足以下条件：
1.  **项目环境已配置**：能够通过 `uv run ...` 成功执行项目中的脚本。
2.  **数据已存在**：`data/parquet/` 目录下已经有下载好的数据。这些数据应该按表名分成了不同的子目录，例如 `data/parquet/daily_basic/`、`data/parquet/income_statement/` 等。

### 5.3. 重建步骤

#### 第 1 步：删除旧的元数据文件（确保全新重建）

为了避免任何旧状态的干扰，请先删除 `data/` 目录下的 `metadata.db` 和 `metadata.db.wal`（如果存在）。

```bash
# 在项目根目录下执行
rm -f data/metadata.db data/metadata.db.wal
```

#### 第 2 步：运行元数据同步脚本

执行以下命令来触发元数据同步过程：

```bash
uv run python scripts/manual_sync_metadata.py
```

执行完毕后，你会看到 `data/metadata.db` 文件被重新创建。此时，你就可以通过任何 DuckDB 客户端连接此文件进行数据查询了。

### 5.4. 深入解析：`manual_sync_metadata.py` 背后发生了什么？

当你运行该脚本时，`MetadataSyncManager` 类会执行以下核心操作：

1.  **连接/创建数据库**：
    *   它连接到 `data/metadata.db`。如果文件不存在，DuckDB 会自动创建它。

2.  **扫描数据目录**：
    *   脚本会扫描 `data/parquet/` 目录下的所有子目录，每个子目录名被视为一个**表名**。例如，它会找到 `daily_basic`, `income_statement` 等目录。

3.  **为每个表创建视图（VIEW）**：
    *   对于扫描到的每一个表（例如 `daily_basic`），脚本会执行一个核心的 SQL 命令来创建或替换一个同名视图。
    *   这个 SQL 命令的模板如下：
        ```sql
        CREATE OR REPLACE VIEW daily_basic AS
        SELECT * FROM read_parquet(
            ['/path/to/project/data/parquet/daily_basic/file1.parquet', '/path/to/project/data/parquet/daily_basic/file2.parquet', ...],
            hive_partitioning=1,
            union_by_name=True
        );
        ```

    *   **关键参数解释**：
        *   `CREATE OR REPLACE VIEW`：这个命令非常关键。它创建的是一个**视图**，而不是**表（TABLE）**。视图只存储查询语句，不存储数据本身，因此 `metadata.db` 文件会非常小。`OR REPLACE` 确保了每次运行都能覆盖旧的定义。
        *   `read_parquet([...])`：这是 DuckDB 的一个强大函数，它可以直接从一个或多个 Parquet 文件中读取数据。脚本会自动查找表目录下所有有效的 `.parquet` 文件，并将它们的绝对路径列表传入。
        *   `hive_partitioning=1`：这个参数让 DuckDB 能够自动识别并利用 Hive 风格的分区（例如 `year=2023/month=01/*.parquet`），极大地提升查询性能。
        *   `union_by_name=True`：当从多个 Parquet 文件合并数据时，即使它们的列顺序不同，只要列名能对上，就能正确合并。这增强了数据处理的鲁棒性。