# 架构改造方案：多队列性能优化

## 1. 问题背景

### 1.1 性能瓶颈

在对应用进行压力测试时，我们发现当前架构下的任务处理速率约为 **630次/分钟**，这远低于各个下载任务类型合计的理论API速率上限（约1200次/分钟）。

### 1.2 根本原因分析

经过分析，我们定位到性能瓶颈的根本原因在于**所有任务（无论快慢）都在争抢同一个消费者worker池的资源**。

当前的实现中，一个worker在完成一个快速的下载任务（`download_task`）后，会立即被分配去执行一个慢速的、阻塞的数据保存任务（`process_data_task`）。由于数据保存任务需要等待数据库写入，速度很慢，这导致了整个worker池被迅速占满并普遍处于等待状态。最终，没有空闲的worker来处理新的下载任务，使得系统的整体吞吐能力被数据库I/O严重拖累。

## 2. 解决方案：隔离任务队列

为了解决此问题，我们提出采用**多队列、多消费者**的架构，将不同性质的任务进行隔离。

1.  **快速任务队列 (`huey_fast`)**: 用于处理耗时短、I/O密集的任务，例如 `download_task`。为其分配一个拥有大量worker（如50个）的专属消费者进程，以实现高并发。

2.  **慢速任务队列 (`huey_slow`)**: 用于处理耗时较长、有阻塞的CPU或磁盘密集型任务，例如 `process_data_task`。为其分配一个只有少量worker的专属消费者进程。

通过这种方式，慢速任务的执行将不再占用快速任务的处理资源，从而将瓶颈隔离，使下载任务的处理速度可以达到其理论上限。

## 3. 原型验证

我们已经通过一个原型成功验证了该方案的可行性。原型测试结果表明，在使用独立队列和独立消费者进程后，快速任务队列的吞吐量完全不受慢速队列的影响，达到了其理论上的最大值。

> [!IMPORTANT]
> **该原型的完整实现可作为本次架构改造的直接参考。**
> **源码路径: `examples/huey_multi_instance_prototype/`**

## 4. 具体改造步骤

将原型中验证过的模式应用到主应用中，需要修改以下部分：

### 4.1 修改配置文件 (`config.toml`)

为两个独立的队列提供各自的配置。

**修改后:**
```toml
[huey_fast]
max_workers = 50
sqlite_path = "data/tasks_fast.db"

[huey_slow]
max_workers = 1   # 最终确定为1，以解决数据库写入冲突
sqlite_path = "data/tasks_slow.db"
```

### 4.2 创建多Huey实例 (`src/neo/configs/huey_config.py`)

根据新的配置，创建两个独立的Huey实例 (`huey_fast`, `huey_slow`)。

### 4.3 任务分类与链接 (`src/neo/tasks/huey_tasks.py`)

1.  `download_task` 使用 `@huey_fast.task()` 装饰，并移除内部的数据库查询逻辑。
2.  `process_data_task` 使用 `@huey_slow.task()` 装饰。
3.  在 `download_task` 的末尾，手动调用 `process_data_task` 并传递数据。

### 4.4 修改依赖注入 (`src/neo/containers.py`)

在容器配置中，移除向 `SimpleDownloader` 注入 `db_operator` 的逻辑，因为下载器不再需要访问数据库。

### 4.5 更新消费者启动逻辑

-   **`src/neo/main.py`**: 修改 `dp` 命令，使其接受一个必需的 `queue_name` 参数 (`fast` 或 `slow`)。
-   **`src/neo/helpers/app_service.py`**: 修改 `run_data_processor` 方法，根据传入的 `queue_name` 启动对应的消费者实例。
-   **`run_dp_and_monitor.sh`**: 修改脚本，使其可以同时启动 `fast` 和 `slow` 两个队列的消费者进程。
-   **`src/neo/helpers/utils.py`**: 修改 `setup_logging` 函数，使其能够根据 `log_type` 参数生成不同的日志文件（如 `consumer_fast.log`），方便独立调试。

## 5. 实施与调试中的关键发现

在实施过程中，我们遇到了由 `duckdb` 数据库的并发特性引发的关键问题，并通过以下决策得以解决：

1.  **问题：快速队列中的数据库锁竞争**
    -   **现象**：在改造初期，`consumer_fast.log` 中出现大量的 `IO Error: Could not set lock on file` 错误。
    -   **原因**：`download_task` 在下载前会查询数据库以获取最新日期，50个并发的worker同时访问单个数据库文件，导致了锁竞争。
    -   **解决方案**：将数据库访问逻辑从 `download_task` 中彻底移除。下载任务只负责获取全部数据，增量更新的判断完全交给下游的 `process_data_task` 和数据库的 `upsert` 逻辑处理。

2.  **问题：慢速队列中的数据库锁竞争**
    -   **现象**：即使在解决了快速队列的问题后，当慢速队列的worker数量大于1时，`consumer_slow.log` 中依然出现数据库锁竞争错误。
    -   **原因**：`duckdb` 作为嵌入式数据库，其默认配置不支持多个线程或进程同时进行写操作。
    -   **解决方案**：将慢速队列的 `max_workers` 设置为 **1**。这确保了在任何时候只有一个进程在执行数据写入操作，从而根除了并发写入的问题，保证了数据处理的稳定性。

## 6. 最终结论

本次重构成功达成了预定目标。

通过引入**双队列架构**，并将**数据库访问与高并发的下载任务解耦**，我们彻底解决了原架构的性能瓶颈。同时，通过将**慢速队列的消费者设置为单线程**，我们保证了数据写入的稳定性和一致性，避免了并发冲突。

最终的系统架构稳定、清晰，且性能表现符合预期。所有相关代码修改已在 `feature/multi-queue-refactor` 分支中提交。
